<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 9 Markov Chain Monte Carlo | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-06-08" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 9 Markov Chain Monte Carlo | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>Chapter 9 Markov Chain Monte Carlo | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index">Index</a></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2-1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2-2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2-3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li><a href="2-4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a></li>
</ul></li>
<li><a href="3-sampling-from-the-imaginary.html#sampling-from-the-imaginary"><span class="toc-section-number">3</span> Sampling from the Imaginary</a>
<ul>
<li><a href="3-1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> Sampling from a grid-approximate posterior</a></li>
<li><a href="3-2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> Sampling to summarize</a></li>
<li><a href="3-3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> Sampling to simulate prediction</a></li>
<li><a href="practice.html#practice">Practice</a></li>
<li><a href="homework-week-1.html#homework-week-1">Homework: week 1</a></li>
</ul></li>
<li><a href="4-geocentric-models.html#geocentric-models"><span class="toc-section-number">4</span> Geocentric Models</a>
<ul>
<li><a href="4-1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">4.1</span> Why normal distributions are normal</a></li>
<li><a href="4-2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">4.2</span> A language for describing models</a></li>
<li><a href="4-3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">4.3</span> Gaussian model of height</a></li>
<li><a href="4-4-linear-prediction.html#linear-prediction"><span class="toc-section-number">4.4</span> Linear prediction</a></li>
<li><a href="4-5-curves-from-lines.html#curves-from-lines"><span class="toc-section-number">4.5</span> Curves from lines</a></li>
<li><a href="4-6-practice-1.html#practice-1"><span class="toc-section-number">4.6</span> Practice</a></li>
</ul></li>
<li><a href="5-the-many-variables-the-spurious-waffles.html#the-many-variables-the-spurious-waffles"><span class="toc-section-number">5</span> The Many Variables &amp; The Spurious Waffles</a>
<ul>
<li><a href="5-1-spurious-association.html#spurious-association"><span class="toc-section-number">5.1</span> Spurious association</a></li>
</ul></li>
<li><a href="6-the-haunted-dag-the-causal-terror.html#the-haunted-dag-the-causal-terror"><span class="toc-section-number">6</span> The Haunted DAG &amp; The Causal Terror</a></li>
<li><a href="7-ulysses-compass.html#ulysses-compass"><span class="toc-section-number">7</span> Ulysses’ Compass</a></li>
<li><a href="8-conditional-manatees.html#conditional-manatees"><span class="toc-section-number">8</span> Conditional Manatees</a></li>
<li><a href="9-markov-chain-monte-carlo.html#markov-chain-monte-carlo"><span class="toc-section-number">9</span> Markov Chain Monte Carlo</a></li>
<li><a href="10-big-entropy-and-the-generalized-linear-model.html#big-entropy-and-the-generalized-linear-model"><span class="toc-section-number">10</span> Big Entropy and the Generalized Linear Model</a></li>
<li><a href="11-god-spiked-the-integers.html#god-spiked-the-integers"><span class="toc-section-number">11</span> God Spiked the Integers</a></li>
<li><a href="12-monsters-and-mixtures.html#monsters-and-mixtures"><span class="toc-section-number">12</span> Monsters and Mixtures</a></li>
<li><a href="13-models-with-memory.html#models-with-memory"><span class="toc-section-number">13</span> Models With Memory</a></li>
<li><a href="14-adventures-in-covariance.html#adventures-in-covariance"><span class="toc-section-number">14</span> Adventures in Covariance</a></li>
<li><a href="15-missing-data-and-other-opportunities.html#missing-data-and-other-opportunities"><span class="toc-section-number">15</span> Missing Data and Other Opportunities</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="markov-chain-monte-carlo" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Markov Chain Monte Carlo</h1>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="9-markov-chain-monte-carlo.html#cb230-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb230-2"><a href="9-markov-chain-monte-carlo.html#cb230-2" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;code/scripts/source.R&quot;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="9-markov-chain-monte-carlo.html#cb231-1" aria-hidden="true" tabindex="-1"></a>slides_dir <span class="ot">=</span> here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;docs/slides/L10&quot;</span>)</span></code></pre></div>
<div class="figure">
<img src="slides/L10/01.png" alt="Major transition point where we switch over algorithms for estimating the posterior." width="80%" />
<p class="caption marginnote shownote">
Major transition point where we switch over algorithms for estimating the posterior.
</p>
</div>
<div class="figure">
<img src="slides/L10/02.png" alt="As a reminder, Bayesian inference is not about how to get the posterior, it's just about the posterior distribution. There are a lot of ways to get it. In this case, you can calculate it a huge number of ways and they're all valid. In biology, Bayesian is thought of as synonymous with Markov Chains. But you can use Markov chains for lots of things. There's a bunch of additional machinery you need to fool around with when you play with Markov chains." width="80%" />
<p class="caption marginnote shownote">
As a reminder, Bayesian inference is not about how to get the posterior, it’s just about the posterior distribution. There are a lot of ways to get it. In this case, you can calculate it a huge number of ways and they’re all valid. In biology, Bayesian is thought of as synonymous with Markov Chains. But you can use Markov chains for lots of things. There’s a bunch of additional machinery you need to fool around with when you play with Markov chains.
</p>
</div>
<div class="figure">
<img src="slides/L10/03.png" alt="We'll discuss for ways to compute the posterior. The analytical approach was used to create the previous slide. Interesting models, but almost always impossible. So you need another way to do the numerical differentiation. Grid I showed you earlier. You've been using `quap` for months now. Unreasonably effective for lots of models. If you're doing maximum likelihood estimation, you're doing the same steps. There's a connection there between lots of standard tools. But now we're going to get into things that make the connection a lot blurrier." width="80%" />
<p class="caption marginnote shownote">
We’ll discuss for ways to compute the posterior. The analytical approach was used to create the previous slide. Interesting models, but almost always impossible. So you need another way to do the numerical differentiation. Grid I showed you earlier. You’ve been using <code>quap</code> for months now. Unreasonably effective for lots of models. If you’re doing maximum likelihood estimation, you’re doing the same steps. There’s a connection there between lots of standard tools. But now we’re going to get into things that make the connection a lot blurrier.
</p>
</div>
<div class="figure">
<img src="slides/L10/04.png" alt="King Markov rules a bunch of islands." width="80%" />
<p class="caption marginnote shownote">
King Markov rules a bunch of islands.
</p>
</div>
<div class="figure">
<img src="slides/L10/05.png" alt="There are 10, with different population sizes and densities." width="80%" />
<p class="caption marginnote shownote">
There are 10, with different population sizes and densities.
</p>
</div>
<div class="figure">
<img src="slides/L10/06.png" alt="They'll let you tax them as long as you visit them. You must visit them in proportion to their population density. So you need some simple algorithm." width="80%" />
<p class="caption marginnote shownote">
They’ll let you tax them as long as you visit them. You must visit them in proportion to their population density. So you need some simple algorithm.
</p>
</div>
<div class="figure">
<img src="slides/L10/07.png" alt="You start on a particular island. Now you're ready to move to another one. You flip a coin to decide on which island." width="80%" />
<p class="caption marginnote shownote">
You start on a particular island. Now you’re ready to move to another one. You flip a coin to decide on which island.
</p>
</div>
<div class="figure">
<img src="slides/L10/08.png" alt="Then you send a servant across to the proposed island and they take a survey of how many people are on the island. You do the same for the island you're on. We'll call that $p_5$, and the current island $p_4$." width="80%" />
<p class="caption marginnote shownote">
Then you send a servant across to the proposed island and they take a survey of how many people are on the island. You do the same for the island you’re on. We’ll call that <span class="math inline">\(p_5\)</span>, and the current island <span class="math inline">\(p_4\)</span>.
</p>
</div>
<p><img src="slides/L10/09.png" width="80%" /></p>
<p><img src="slides/L10/10.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/11.png" alt="You want to compare these two numbers in a particular way." width="80%" />
<p class="caption marginnote shownote">
You want to compare these two numbers in a particular way.
</p>
</div>
<div class="figure">
<img src="slides/L10/12.png" alt="You want to take the ratio of them, and that will be the probability of accepting the proposal of moving from island 4 to 5. If it's greater than 1, you'll move. " width="80%" />
<p class="caption marginnote shownote">
You want to take the ratio of them, and that will be the probability of accepting the proposal of moving from island 4 to 5. If it’s greater than 1, you’ll move.
</p>
</div>
<div class="figure">
<img src="slides/L10/13.png" alt="So you move to the proposal island with that probability." width="80%" />
<p class="caption marginnote shownote">
So you move to the proposal island with that probability.
</p>
</div>
<div class="figure">
<img src="slides/L10/14.png" alt="This is a valid way to fill the contract. It guarantees that in the long run, you will visit each island in proportion to its relative populations size. This is an example of Markov Chain Monte Carlo." width="80%" />
<p class="caption marginnote shownote">
This is a valid way to fill the contract. It guarantees that in the long run, you will visit each island in proportion to its relative populations size. This is an example of Markov Chain Monte Carlo.
</p>
</div>
<div class="figure">
<img src="slides/L10/15.png" alt="It's the most famous, and the most primitive. The huge advantage is if you don't know the distribution of population sizes, you actually don't need to actually visit each of the islands in proportion to their population sizes. We don't know the posterior distribution, but we can visit each part of it in proportion to its relative probability. That's the magic: we can sample from the distribution that we don't know. Also going to introduce you to Stan. `ulam` is a simplified input that will make a custom Markov chain for you." width="80%" />
<p class="caption marginnote shownote">
It’s the most famous, and the most primitive. The huge advantage is if you don’t know the distribution of population sizes, you actually don’t need to actually visit each of the islands in proportion to their population sizes. We don’t know the posterior distribution, but we can visit each part of it in proportion to its relative probability. That’s the magic: we can sample from the distribution that we don’t know. Also going to introduce you to Stan. <code>ulam</code> is a simplified input that will make a custom Markov chain for you.
</p>
</div>
<div class="figure">
<img src="slides/L10/16.png" alt="Here is the R script version of King Monty's royal tour. Metropolis was also a person. Let's walk through each line to get a sense of how simple it is. Positions is an empty vector. We'll store the islands the King is on here. Then we'll just put him on island 10 (`current`). Then we loop over the weeks, and record where the king is now. Then we'll flip a coin to generate the proposal. Finally there's the action. We've got the ratio, where we're asserting the relative popn size is the same as their number. " width="80%" />
<p class="caption marginnote shownote">
Here is the R script version of King Monty’s royal tour. Metropolis was also a person. Let’s walk through each line to get a sense of how simple it is. Positions is an empty vector. We’ll store the islands the King is on here. Then we’ll just put him on island 10 (<code>current</code>). Then we loop over the weeks, and record where the king is now. Then we’ll flip a coin to generate the proposal. Finally there’s the action. We’ve got the ratio, where we’re asserting the relative popn size is the same as their number.
</p>
</div>
<div class="figure">
<img src="slides/L10/17.png" alt="Run that code and plot it out, and you'll get this. You can see the King zig-zagging around. You can see he gets stuck on densely populated islands. In the long run, it's in the right proportions." width="80%" />
<p class="caption marginnote shownote">
Run that code and plot it out, and you’ll get this. You can see the King zig-zagging around. You can see he gets stuck on densely populated islands. In the long run, it’s in the right proportions.
</p>
</div>
<div class="figure">
<img src="slides/L10/18.png" alt="Chain is more obvious here." width="80%" />
<p class="caption marginnote shownote">
Chain is more obvious here.
</p>
</div>
<div class="figure">
<img src="slides/L10/19.png" alt="Starting to emerge that he's visiting 10 more. After 2000 weeks, we're almost there. Guaranteed to work in the long run. What the 'long run' means is controversial." width="80%" />
<p class="caption marginnote shownote">
Starting to emerge that he’s visiting 10 more. After 2000 weeks, we’re almost there. Guaranteed to work in the long run. What the ‘long run’ means is controversial.
</p>
</div>
<div class="figure">
<img src="slides/L10/20.png" alt="Summary slide. Not sensitive to initial conditions. In this algorithm you need symmetric proposals... there are other algorithms without this condition, which improves them. " width="80%" />
<p class="caption marginnote shownote">
Summary slide. Not sensitive to initial conditions. In this algorithm you need symmetric proposals… there are other algorithms without this condition, which improves them.
</p>
</div>
<div class="figure">
<img src="slides/L10/21.png" alt="The population size is the posterior probability. This works no matter how many parameters you have, in the long run. The long run is really long if you have a lot of parameters. That's the snag." width="80%" />
<p class="caption marginnote shownote">
The population size is the posterior probability. This works no matter how many parameters you have, in the long run. The long run is really long if you have a lot of parameters. That’s the snag.
</p>
</div>
<div class="figure">
<img src="slides/L10/22.png" alt="Here's the famous paper where they first implemented it. Some famous people here. Rosenbluth did most of the programming, and Teller figured out the memory. The fed the tape in with a bicycle wheel. This was all part of the Manhattan project - for making fusion bombs post-war. " width="80%" />
<p class="caption marginnote shownote">
Here’s the famous paper where they first implemented it. Some famous people here. Rosenbluth did most of the programming, and Teller figured out the memory. The fed the tape in with a bicycle wheel. This was all part of the Manhattan project - for making fusion bombs post-war.
</p>
</div>
<div class="figure">
<img src="slides/L10/23.png" alt="Metropolis in the foreground. MANIAC in the background. Currently a laptop can do billions of multiplications per second." width="80%" />
<p class="caption marginnote shownote">
Metropolis in the foreground. MANIAC in the background. Currently a laptop can do billions of multiplications per second.
</p>
</div>
<div class="figure">
<img src="slides/L10/24.png" alt="What are Markov Chains? The Metropolis is the simplest verison. Named after Markov. What makes something a Markov chain only depends on where you are now, not where you've been. What matters is the current state. Great for computing because you don't need to store a bunch of numbers." width="80%" />
<p class="caption marginnote shownote">
What are Markov Chains? The Metropolis is the simplest verison. Named after Markov. What makes something a Markov chain only depends on where you are now, not where you’ve been. What matters is the current state. Great for computing because you don’t need to store a bunch of numbers.
</p>
</div>
<div class="figure">
<img src="slides/L10/25.png" alt="Sometimes you get a model that is to hard to integrate. Often with integrals you're guessing. Not like derivatives. Going in the other direction is much harder. Integrating is sometimes not practical. MCMC is just one of the methods of estimating it. Optimisation (like quap) often targets the wrong area of the distribution ('concentration of measure'). This is why everyone gives up on optimisation once you have more than 100 parameters. This is bread and butter sort of stuff in applied statistics. " width="80%" />
<p class="caption marginnote shownote">
Sometimes you get a model that is to hard to integrate. Often with integrals you’re guessing. Not like derivatives. Going in the other direction is much harder. Integrating is sometimes not practical. MCMC is just one of the methods of estimating it. Optimisation (like quap) often targets the wrong area of the distribution (‘concentration of measure’). This is why everyone gives up on optimisation once you have more than 100 parameters. This is bread and butter sort of stuff in applied statistics.
</p>
</div>
<div class="figure">
<img src="slides/L10/26.png" alt="There are a bunch of different MCMC strategies. Hastings showed that you don't need symmetric proposals, which made them more efficient. Gibbs sampling is extremely efficient, but basically Metropolis. What they have in common is they guess and check. They make a proposal to move somewhere, check the posterior probability at that location, then decide to move. If you make dumb proposals, you won't move. The goal is to constantly move by making really smart proposals. Guess and checking gives you dumb proposals, so you need a completely different strategy: Hamiltonian. Really efficient and can make models with tens of thousands of parameters. Hamiltonians use a gradient. " width="80%" />
<p class="caption marginnote shownote">
There are a bunch of different MCMC strategies. Hastings showed that you don’t need symmetric proposals, which made them more efficient. Gibbs sampling is extremely efficient, but basically Metropolis. What they have in common is they guess and check. They make a proposal to move somewhere, check the posterior probability at that location, then decide to move. If you make dumb proposals, you won’t move. The goal is to constantly move by making really smart proposals. Guess and checking gives you dumb proposals, so you need a completely different strategy: Hamiltonian. Really efficient and can make models with tens of thousands of parameters. Hamiltonians use a gradient.
</p>
</div>
<p><img src="slides/L10/27.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/28.png" alt="Simulation of the MC run. Red it stays, green is moves. It stacks the distribution as it moves. Metropolis works really well. The problem comes when the distribution is not as nice as a Gaussian hill." width="80%" />
<p class="caption marginnote shownote">
Simulation of the MC run. Red it stays, green is moves. It stacks the distribution as it moves. Metropolis works really well. The problem comes when the distribution is not as nice as a Gaussian hill.
</p>
</div>
<p><img src="slides/L10/29.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/30.png" alt="... like a donut. In high-dimensional spaces, it gets concentrated into thin shells. Picture a hyper-donut. We must sample from it. And the Metropolialogrithm is really bad like that, because it makes a lot of proposals into dead space. You can see it's a donut, but it doesn't. Now the long run is very long indeed. It gets stuck in narrow regions. The basic problem is that the proposals don't know the shape of the distribution." width="80%" />
<p class="caption marginnote shownote">
… like a donut. In high-dimensional spaces, it gets concentrated into thin shells. Picture a hyper-donut. We must sample from it. And the Metropolialogrithm is really bad like that, because it makes a lot of proposals into dead space. You can see it’s a donut, but it doesn’t. Now the long run is very long indeed. It gets stuck in narrow regions. The basic problem is that the proposals don’t know the shape of the distribution.
</p>
</div>
<div class="figure">
<img src="slides/L10/31.png" alt="You can tune it, but there's a really tight trade-off. On the left we start with the Markov chain. Filled circles are accepted. If you only consider points near you, you'll get more valid proposals, but you'll move really slow. If we lengthen, then we reject more. This is the fundamental trade-off. Really you can't win. Gibbs can do a little better. Butas soon as the dimensions increase, the problem eventually appears. The issue is guess and check." width="80%" />
<p class="caption marginnote shownote">
You can tune it, but there’s a really tight trade-off. On the left we start with the Markov chain. Filled circles are accepted. If you only consider points near you, you’ll get more valid proposals, but you’ll move really slow. If we lengthen, then we reject more. This is the fundamental trade-off. Really you can’t win. Gibbs can do a little better. Butas soon as the dimensions increase, the problem eventually appears. The issue is guess and check.
</p>
</div>
<div class="figure">
<img src="slides/L10/32.png" alt="So what do we do instead? This is a different process entirely. No guess and check. Instead, it runs a physics simulation. We'll represent our parameter state as a coordinate in some high-dimensional space. In more dimensions, you have some hyperspace. You're a particle in this space in some position. Then we'll flick you, and it'll cruise on some surface - the posterior distribution - and record where you stop, then do it again. Because it follows the curvature, it always makes good proposals because it doesn't go into bad areas. So there's no more guessing and checking, all proposals are good proposals." width="80%" />
<p class="caption marginnote shownote">
So what do we do instead? This is a different process entirely. No guess and check. Instead, it runs a physics simulation. We’ll represent our parameter state as a coordinate in some high-dimensional space. In more dimensions, you have some hyperspace. You’re a particle in this space in some position. Then we’ll flick you, and it’ll cruise on some surface - the posterior distribution - and record where you stop, then do it again. Because it follows the curvature, it always makes good proposals because it doesn’t go into bad areas. So there’s no more guessing and checking, all proposals are good proposals.
</p>
</div>
<div class="figure">
<img src="slides/L10/33.png" alt="Continuous urban smear. More living in the bottom. How to use this? Hamiltonian." width="80%" />
<p class="caption marginnote shownote">
Continuous urban smear. More living in the bottom. How to use this? Hamiltonian.
</p>
</div>
<p><img src="slides/L10/34.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/35.png" alt="In the book, here's a simulation. Time on the horizontal. It's Gaussian. You take the log of a Gaussian and it's a parabola. So you start in the middle. But you need to know the contour." width="80%" />
<p class="caption marginnote shownote">
In the book, here’s a simulation. Time on the horizontal. It’s Gaussian. You take the log of a Gaussian and it’s a parabola. So you start in the middle. But you need to know the contour.
</p>
</div>
<div class="figure">
<img src="slides/L10/36.png" alt="Roll the marble and periodically stop and record the position. Over time you get position samples that are proportional to the shape, as if there is more probability in the bottom." width="80%" />
<p class="caption marginnote shownote">
Roll the marble and periodically stop and record the position. Over time you get position samples that are proportional to the shape, as if there is more probability in the bottom.
</p>
</div>
<p><img src="slides/L10/37.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/38.png" alt="Again, 2D Gaussian hill. It's a bowl now. Flick the simulation and do the pass. Always ends up inside the bowl. What stops you from getting into the silly spots. Better living through physics." width="80%" />
<p class="caption marginnote shownote">
Again, 2D Gaussian hill. It’s a bowl now. Flick the simulation and do the pass. Always ends up inside the bowl. What stops you from getting into the silly spots. Better living through physics.
</p>
</div>
<div class="figure">
<img src="slides/L10/39.png" alt="Because each proposal is accepted, you need many many fewer. So a lot of efficiency. Using the code in the chapter, we get tours. Start with x on the left, and it rolls down the valley. Eventually we stop, then flick it again. High acceptance rate, but the sequential auto-correlation is very low. " width="80%" />
<p class="caption marginnote shownote">
Because each proposal is accepted, you need many many fewer. So a lot of efficiency. Using the code in the chapter, we get tours. Start with x on the left, and it rolls down the valley. Eventually we stop, then flick it again. High acceptance rate, but the sequential auto-correlation is very low.
</p>
</div>
<p><img src="slides/L10/40.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/41.png" alt="Hamiltonian MC does really well with the donut. It knows the curvature. Tours the whole thing, and you don't get stuck. If you have 27K parameters, that's really handy. " width="80%" />
<p class="caption marginnote shownote">
Hamiltonian MC does really well with the donut. It knows the curvature. Tours the whole thing, and you don’t get stuck. If you have 27K parameters, that’s really handy.
</p>
</div>
<div class="figure">
<img src="slides/L10/42.png" alt="You need to put in momentum variables now. You need to choose the mass. The system has energy, which is how to check that it's working. If energy is not conserved, the simulation stops working. With Metropolis, you won't know. But this breaks dramatically. Also need gradients - the log posterior at a particular point." width="80%" />
<p class="caption marginnote shownote">
You need to put in momentum variables now. You need to choose the mass. The system has energy, which is how to check that it’s working. If energy is not conserved, the simulation stops working. With Metropolis, you won’t know. But this breaks dramatically. Also need gradients - the log posterior at a particular point.
</p>
</div>
<div class="figure">
<img src="slides/L10/43.png" alt="Not that complicated, so tour through it. " width="80%" />
<p class="caption marginnote shownote">
Not that complicated, so tour through it.
</p>
</div>
<div class="figure">
<img src="slides/L10/44.png" alt="Problem is that there's stuff to pick, namely the step size (the length of time we run a segment). You want basically the biggest step size, but if you make it too big, you can overshoot the shape. THen you need to choose the number of steps you'll take in each trajectory. If you choose bad values for those, you'll have a bad time. In general it's not as bad as it gets in a 2D Gaussian. Since it's a parabolic bowl, you can get these parabolic loops. In the long run it'll still work, but it's super inefficient. How do you fix it? Choose good values for the tuning parameters, which is annoying." width="80%" />
<p class="caption marginnote shownote">
Problem is that there’s stuff to pick, namely the step size (the length of time we run a segment). You want basically the biggest step size, but if you make it too big, you can overshoot the shape. THen you need to choose the number of steps you’ll take in each trajectory. If you choose bad values for those, you’ll have a bad time. In general it’s not as bad as it gets in a 2D Gaussian. Since it’s a parabolic bowl, you can get these parabolic loops. In the long run it’ll still work, but it’s super inefficient. How do you fix it? Choose good values for the tuning parameters, which is annoying.
</p>
</div>
<p><img src="slides/L10/45.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/46.png" alt="Here's the simulation where it resembles an Ouroboros. Since you don't know the distribution, it's hard to say what the best tuning parameters are." width="80%" />
<p class="caption marginnote shownote">
Here’s the simulation where it resembles an Ouroboros. Since you don’t know the distribution, it’s hard to say what the best tuning parameters are.
</p>
</div>
<div class="figure">
<img src="slides/L10/47.png" alt="Stan does two things: 1) Warm up phase, then maximises the step size. 2) Runs the NUTS2 algorithm. " width="80%" />
<p class="caption marginnote shownote">
Stan does two things: 1) Warm up phase, then maximises the step size. 2) Runs the NUTS2 algorithm.
</p>
</div>
<p><img src="slides/L10/48.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/49.png" alt="Here's NUTS. It runs the simulation in both directions in time. It imagines a simulation that loops back on itself, and runs it backwards from teh starting point, and goes backdwards at the same time, but when it sees itself turning around, it stops. This means it figures out a good number of leapfrog steps for each trajectory. So you don't need to make a bunch of decisions." width="80%" />
<p class="caption marginnote shownote">
Here’s NUTS. It runs the simulation in both directions in time. It imagines a simulation that loops back on itself, and runs it backwards from teh starting point, and goes backdwards at the same time, but when it sees itself turning around, it stops. This means it figures out a good number of leapfrog steps for each trajectory. So you don’t need to make a bunch of decisions.
</p>
</div>
<div class="figure">
<img src="slides/L10/50.png" alt="One of the mathematicians working with Metropolis. Built mechnical MCMC simulators. Also did important work in biology. Could run Stan on anything you like. " width="80%" />
<p class="caption marginnote shownote">
One of the mathematicians working with Metropolis. Built mechnical MCMC simulators. Also did important work in biology. Could run Stan on anything you like.
</p>
</div>
<p><img src="slides/L10/51.png" width="80%" /></p>
<p><img src="slides/L10/52.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/53.png" alt="There are still problems. If you have multiple separated hills, like in factored analytical models, the wait between hills can be long indeed. In my experience you handle this by changing the geometry of the model. We get these with item-response theory models." width="80%" />
<p class="caption marginnote shownote">
There are still problems. If you have multiple separated hills, like in factored analytical models, the wait between hills can be long indeed. In my experience you handle this by changing the geometry of the model. We get these with item-response theory models.
</p>
</div>
<div class="figure">
<img src="slides/L10/54.png" alt="SWant you to recognise a bad chain when you see it." width="80%" />
<p class="caption marginnote shownote">
SWant you to recognise a bad chain when you see it.
</p>
</div>
<div class="figure">
<img src="slides/L10/55.png" alt="Run quap as before. Now let's do this with a MC." width="80%" />
<p class="caption marginnote shownote">
Run quap as before. Now let’s do this with a MC.
</p>
</div>
<div class="figure">
<img src="slides/L10/56.png" alt="Same formula, but slim dataset with just the variables of interest. 4 chains on separate cores." width="80%" />
<p class="caption marginnote shownote">
Same formula, but slim dataset with just the variables of interest. 4 chains on separate cores.
</p>
</div>
<div class="figure">
<img src="slides/L10/57.png" alt="`ulam` translates this into raw Stan code. A bunch of formal variable definitions. " width="80%" />
<p class="caption marginnote shownote">
<code>ulam</code> translates this into raw Stan code. A bunch of formal variable definitions.
</p>
</div>
<div class="figure">
<img src="slides/L10/58.png" alt="What happens here is it reports each chain. Warmup figures out the step size. Total samples is the length of each chain minus the warmup. You won't need more than a couple of thousand samples to get a good estimate." width="80%" />
<p class="caption marginnote shownote">
What happens here is it reports each chain. Warmup figures out the step size. Total samples is the length of each chain minus the warmup. You won’t need more than a couple of thousand samples to get a good estimate.
</p>
</div>
<div class="figure">
<img src="slides/L10/59.png" alt=" `n_eff` is the number of effective samples. Note that it's greater than the number of samples from the MC. True because it takes dispersed samples. The number of samples you would get if there was no auto-correlation between sequential samples. `Rhat` is the Gelman-Ruben diagnostic. You want it to converge across chains. They should all look the same and be exchangeable." width="80%" />
<p class="caption marginnote shownote">
<code>n_eff</code> is the number of effective samples. Note that it’s greater than the number of samples from the MC. True because it takes dispersed samples. The number of samples you would get if there was no auto-correlation between sequential samples. <code>Rhat</code> is the Gelman-Ruben diagnostic. You want it to converge across chains. They should all look the same and be exchangeable.
</p>
</div>
<p><img src="slides/L10/60.png" width="80%" /></p>
<p><img src="slides/L10/61.png" width="80%" /></p>
<p><img src="slides/L10/62.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/63.png" alt="What you want to see are these hairy caterpillars." width="80%" />
<p class="caption marginnote shownote">
What you want to see are these hairy caterpillars.
</p>
</div>
<p><img src="slides/L10/64.png" width="80%" /></p>
<p><img src="slides/L10/65.png" width="80%" /></p>
<p><img src="slides/L10/66.png" width="80%" /></p>
<p><img src="slides/L10/67.png" width="80%" /></p>
<p><img src="slides/L10/68.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/69.png" alt="You might get something like this instead. Not good." width="80%" />
<p class="caption marginnote shownote">
You might get something like this instead. Not good.
</p>
</div>
<p><img src="slides/L10/70.png" width="80%" /></p>
<p><img src="slides/L10/71.png" width="80%" /></p>
<p><img src="slides/L10/72.png" width="80%" /></p>
<p><img src="slides/L10/73.png" width="80%" /></p>
<div class="figure">
<img src="slides/L10/74.png" alt="Typically when you're having trouble getting your chain to work, it's because there's something wrong with your model definition. So first check the model. " width="80%" />
<p class="caption marginnote shownote">
Typically when you’re having trouble getting your chain to work, it’s because there’s something wrong with your model definition. So first check the model.
</p>
</div>
<p><img src="slides/L10/75.png" width="80%" /></p>
<p><img src="slides/L10/76.png" width="80%" /></p>
<p><img src="slides/L10/77.png" width="80%" /></p>
<p><img src="slides/L10/78.png" width="80%" /></p>
<p><img src="slides/L10/79.png" width="80%" /></p>

</div>
<p style="text-align: center;">
<a href="8-conditional-manatees.html"><button class="btn btn-default">Previous</button></a>
<a href="10-big-entropy-and-the-generalized-linear-model.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
