---
title: "Notes for Statistical Rethinking 2nd ed. by Richard McElreath"
date: '`r format(Sys.Date())`'
#output: html_notebook
editor_options: 
  chunk_output_type: inline
#output:
#  bookdown::tufte_html_book:
#    toc: yes
#    css: toc.css
#    pandoc_args: --lua-filter=color-text.lua
#    highlight: pygments
#    link-citations: yes
---

# Missing Data and Other Opportunities

```{r, message = F, warning=F}
library(here)
source(here::here("code/scripts/source.R"))
```

```{r}
slides_dir = here::here("docs/slides/L20")
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '01.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Pancakes. Hatching indicates that that side is burnt. "}
knitr::include_graphics(file.path(slides_dir, '02.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Now I serve you a burnt pancake. The probability of the other side being burnt is not half, but rather 2/3."}
knitr::include_graphics(file.path(slides_dir, '03.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Famous logic problem. Point of logic puzzles is to correct your intuitions and teach you methods for solving them. You don't have to be clever, just ruthlessley apply the rules of conditioning. Don't trust your intuitions. Theways we figure things out in probabilty theory, we condition on what we know, and seee if that updates. "}
knitr::include_graphics(file.path(slides_dir, '04.png'))
```

```{r, echo = F, out.width='80%', fig.cap="We want to know the probabilty that the other side is burnt conditional on what we know, that one side is burnt. "}
knitr::include_graphics(file.path(slides_dir, '05.png'))
```

```{r, echo = F, out.width='80%', fig.cap="We have all the information to compute this. There are three pancakes. BB is burnt-burnt. Probabillity that you would see a burnt side if it's BB is 1. "}
knitr::include_graphics(file.path(slides_dir, '06.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '07.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '08.png'))
```

```{r, echo = F, out.width='80%', fig.cap="In the text there's a simulation. The mistake is focusign on pancakes. You want to focus on sides. There are three burnt sides. Of the other sides of those, how many of those are burnt? 2, so 2/3. "}
knitr::include_graphics(file.path(slides_dir, '09.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Everything we've done is underlain by being ruthless. Express our information by constraints in distributions. Take this approach and show you how it produces automatic solutions. Avoid being clever and you can get useful solutions. Missing data is the extreme version of measurement error."}
knitr::include_graphics(file.path(slides_dir, '10.png'))
```

```{r, echo = F, out.width='80%', fig.cap="There's always some error in measurement, and ther'e always this sigma error. But what if there's also error on the predictors, and it's not consistent. Let's think about avoiding thying to be clever, and just codntiiong on what we know."}
knitr::include_graphics(file.path(slides_dir, '11.png'))
```

```{r, echo = F, out.width='80%', fig.cap="The columns were standard errors on a coulpl eof the variables. The error of measurement has been quantified. We have an esimate of the divors=ce rate, and the stnadared error stells us the error rate of that estiamte. SOome of the standard errors are big. "}
knitr::include_graphics(file.path(slides_dir, '12.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Right now has the log population on the horizontal. And California is on the right. It's so big that the error rate is small."}
knitr::include_graphics(file.path(slides_dir, '13.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Let's think about this in terms of a causal model. We want to see the divorce rate we observe as a function of the true divorce rate `D`, and the population size `N`. "}
knitr::include_graphics(file.path(slides_dir, '14.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Let's not be clever, just apply ruthless probability theory. There's some true divorce rate, and we'd like to use that as our outcome variable. Generatively thinking, our observed data is generated from a normal disttribution. The mean of this normal distribution will be the true rate. THen there's a standard deviation. In the long run, there's some D true. But in any finite period, ther's error, and that will be inversely proportional to population size."}
knitr::include_graphics(file.path(slides_dir, '15.png'))
```

```{r, echo = F, out.width='80%', fig.cap="This was our DAG before. The thing on the top is D TRUE. We're going to put a line on top of it."}
knitr::include_graphics(file.path(slides_dir, '16.png'))
```

```{r, echo = F, out.width='80%', fig.cap="... The observation process. Now D TRUE is's a vector of unkonwn parameters, then the line at the top estimates them for us. We also have the whole regression relationship that is going to pin down values from other states. Shrinkage is going to happen. If you were going to simulate pmeaurement error, this would be the model. Then it runs backwards. **Bayesian models are generative, and you can run them in both directions. If you run them forwards you simulate fake data, and if you run them in the reverse they spit out a posterior distribution. You feed in a distirbution and the spit out data, you feed in data they spit out a distribution.** So if you were going to simulate measurement error, you use the top line.  "}
knitr::include_graphics(file.path(slides_dir, '17.png'))
```

```{r, echo = F, out.width='80%', fig.cap="How do we do this in a model? For every dstate, there's a D true, and that's what the vector[N] is. "}
knitr::include_graphics(file.path(slides_dir, '18.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Remember the distinction between likelihoods and priors in a Baeysian model is coginitive. Probabilityt theory don't care. When something in your dataset becomes unobserved, the model doesn't change. The model exists before you know the sample. The fact that you haven't observed some data doesn't mean the model chagnes. It cjust means you have paraemters there, because parameters are unobserved variables."}
knitr::include_graphics(file.path(slides_dir, '19.png'))
```

```{r, echo = F, out.width='80%', fig.cap="There's shrinkage. Plotting the relationship between median age of marriage and the divorce rate. Most of these are standardised variables. The blue points are the values that were observed. The open circles are the values fromt eh posterior distribution. The line segments connect them for each state. There's shrinkage. Some have moved more than others. You can explain this pattern. Why have some moved way more than others? Thye've moved tothe regression line because that's the expectation. How much it shrinks is also a function of standard error. For ID it says given this relationship and these variables, that measred rate is too edteme to be beileveable, party due to sampling error, so it shirnks it. Wyoming is interesitn gbecause it'snot so far but so uncertain that it gets shrunk directly to the line. Maine gets shrunk a lot too. "}
knitr::include_graphics(file.path(slides_dir, '20.png'))
```

```{r, echo = F, out.width='80%', fig.cap="You can compare it to the plot with teh standard devations. Ont ehleft we've taken the differnce between the estimated and the observed. Horizontal is the standard error. Any state taht has adifference of 0 means there's no shrinkage. "}
knitr::include_graphics(file.path(slides_dir, '21.png'))
```

```{r, echo = F, out.width='80%', fig.cap="That's error on the outcome. YOu can also have error on predictor variables. Imagine sampling a predictor now with error. On the plot on this slide we have the marriage rate. On the horizontal against log population. "}
knitr::include_graphics(file.path(slides_dir, '22.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Top part is the observation process on divorce rate. Then the regression of the true divorce rate on age of marriage and marriage rate. But now inside the regression, we have..."}
knitr::include_graphics(file.path(slides_dir, '23.png'))
```

```{r, echo = F, out.width='80%', fig.cap="M true, not the observed M, and that's a paramter. There's a parameter for each state and it goes in. It's a parameter times a parameter. Every state will have one of this M trues. Same model because it's the same generative process. Then we have the likelihood for the observed rate, the M observed for each state comes from this sampling process again.  "}
knitr::include_graphics(file.path(slides_dir, '24.png'))
```

```{r, echo = F, out.width='80%', fig.cap="You ahve to put in a prior for the MTrues. What hapens as a consquence of setting it as Normal(0, 1), because it's standaredised. Not terrible. Butyou're ignoring oinfoamtion inthe data because if i tele you the age of marrige in each state, you get information about themarriage rate. If you believe the DAG, age of marrige influence marriage rate. So we can get a better prior if we put the whole DAG into the model. If we do it all ato once, there's even more infomration. "}
knitr::include_graphics(file.path(slides_dir, '25.png'))
```

```{r, echo = F, out.width='80%', fig.cap="We've got two variables now, which are observed with error. Divorce rate on vertical; marigage rate on horitzontal. Open points are the corresponding pairs of posterior means for the esitmate true means. So we have shrinkage in two directions now towards some regression line (undrawn). Some shirnk a lot more than others. If you're really far from the line you shrink more. But also there's more shrinkage for the diveroce rate than the marriage rate. Top left is extreme in both. Comes downa  lot more on divorce rate. WHy?"}
knitr::include_graphics(file.path(slides_dir, '26.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Because the regression says marriage rate isn't strongly associtated. It doesn 't know where to move it, so it doesn't.  "}
knitr::include_graphics(file.path(slides_dir, '27.png'))
```

```{r, echo = F, out.width='80%', fig.cap="It comes in many disguises. Simplet is when there's variable that's called 'error'. But there are many more subtle forms. Pre-averaging removes the fact tht you have a finite sample to estimate the mean from. THat takes variation out of the dataset. If you're doing that constantly, that's cheating. Otherwise you could just use a MLM. Then the means are varying effects - parameters - and you do the averaging within the model. And all the uncertainty to do with different sample sizes is taken care of. Parentage analysis is a fun case. Say you have a popuation of wild rodents and you're tyring to figure out who was whose parents. So you get their genotypes and figure out how they're related. Phylogenetics - the example last week used a single tree. Phylogenies are rarely very scertain. Trend to plot them like ont he right, so you're showing the whole posterior distribution. You can do the analysis over the whole distribution of trees. In archaology, measurement error is the norm, e.g. radio carbon dating. People take this very seriously now. Very sdifficult to sex a fossil. Or studying a place where they don't keep track of birthdays."}
knitr::include_graphics(file.path(slides_dir, '28.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Grown up measurement error. Mechanically similar but feels really different. You want to do something about missing data. Most of the standard regression tools will autoamtically remove missing cases. So all the variables are removed. This squanders information, but can also create confounds. There are ways to deal with this. So how to deal with it? Worst appraoch is to replace teh missing values with the mean of the column. Really bad idea because they model will intepret it as if you knew the value. Procedure called multiple imputation, whcih works really well. Frequentist way of doing what we're going to do. Unreasoanbly effective. Basically do the modle multiple times on samples on some stochastic model of the dataset. We're just going to go full-flavour Bayesian."}
knitr::include_graphics(file.path(slides_dir, '29.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Let's talk about DAGs again. Deeply confusing, because the terminology is awful. Let's think about the primate milk data again. Interested in understanding why the energy content of milk varies so much. Is is related to the proportion of brain neocortex. U is the strong postive correlation between M and B, but we don't know what it is. Something going on here but we don't know what it is."}
knitr::include_graphics(file.path(slides_dir, '30.png'))
```

```{r, echo = F, out.width='80%', fig.cap="This taxonomy tells us what to do. Confusingly, MCAR is totally different to MAR. "}
knitr::include_graphics(file.path(slides_dir, '31.png'))
```

```{r, echo = F, out.width='80%', fig.cap="MCAR. We're not going to get to see B, because it has missing values in it. Lot's of primates where they didn't measure percent neocortex. To get the gaps, we know it's partly caused by B, but it's also caused by the missingness process R. `R_B` creates missing values in `B`. "}
knitr::include_graphics(file.path(slides_dir, '32.png'))
```

```{r, echo = F, out.width='80%', fig.cap="We're going to condition on `B_obs`. Are there backdoors? The answer is no, but there are two paths. Direct and indirect. But the total causal effect can be estimated by simple regerssion with just `B_obs`. There's an indirect effect through M. But there's no back door. "}
knitr::include_graphics(file.path(slides_dir, '33.png'))
```

```{r, echo = F, out.width='80%', fig.cap="There's no path that takes you through `R_B`. This means the missingness mechanism is ignorable, because it doesn't create any backdoor confound. So you don't need to know it. This is the benign case."}
knitr::include_graphics(file.path(slides_dir, '34.png'))
```

```{r, echo = F, out.width='80%', fig.cap="You don't have to condition on anything to keep your infnerence about K independent fromt he missingness mechanism.  "}
knitr::include_graphics(file.path(slides_dir, '35.png'))
```

```{r, echo = F, out.width='80%', fig.cap="The only way this could happen is if you had a random number generator deletes values from your dataset. I assert this is hihgly implausible in more research situations. "}
knitr::include_graphics(file.path(slides_dir, '36.png'))
```

```{r, echo = F, out.width='80%', fig.cap="This is something else that could be going on. This will give us MAR. M is now entering/influencing R_B. Now the missingness mechanism depends on the body mass values. Large or small body masses are more likely to have missing body mass values. Different species are more or less attractive to study. That generates a pattern where some features are associated causally with the missingness. "}
knitr::include_graphics(file.path(slides_dir, '37.png'))
```

```{r, echo = F, out.width='80%', fig.cap="As before, is there a backdoor path. Now because there's an arrow entering R_B.."}
knitr::include_graphics(file.path(slides_dir, '38.png'))
```

```{r, echo = F, out.width='80%', fig.cap="You have a complete path. How to close the backdoor? Condition on M. Here you don't have to knwo the missingness mechanism, but do need to do imputation."}
knitr::include_graphics(file.path(slides_dir, '39.png'))
```

```{r, echo = F, out.width='80%', fig.cap="There's some variable in the graph we can condition on, and separate the two. This is a nice situation to be in, and probably the most common. Why need to impute? Because you'd be polluting the other variables with this missngness pattern, whcihc an create really strong biases."}
knitr::include_graphics(file.path(slides_dir, '40.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Worst case. In this case, the most obvious way to get iit si the variable itself causes the missngieesss. Cerain values of neoxortex percetn are more likely to go missing. How? Maybe species with low neocortex weren't measured."}
knitr::include_graphics(file.path(slides_dir, '41.png'))
```

```{r, echo = F, out.width='80%', fig.cap="This is nasty because you get a backdoor you can't close. Your only hope is to model the missngness mechanism and thereby condition on it."}
knitr::include_graphics(file.path(slides_dir, '42.png'))
```

```{r, echo = F, out.width='80%', fig.cap="The other way to get it would be to have a missingness variable. Here there's fork, like phylogeny. We like to study animals closer to us. That will influence neoxotrex percent, and also influence missingness."}
knitr::include_graphics(file.path(slides_dir, '43.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '44.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Imagine a DAG iwth four variables. R is nowD, a dog. In the first, the dog will eat any homework. In the middle, teh dog eats particular students' homework. The attribute could be attention span, as in they turn away and the dog eats it. Finally, the dog only eats bad homework. Or, more liekly, the stuent feeds it to the dog. But it depends ont he score ofthe homework. "}
knitr::include_graphics(file.path(slides_dir, '45.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Let's go through the mechanics of this. "}
knitr::include_graphics(file.path(slides_dir, '46.png'))
```

```{r, echo = F, out.width='80%', fig.cap="We'll replace the NAs with a parameter, and get posteriro distributions for each of the missing values. "}
knitr::include_graphics(file.path(slides_dir, '47.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Now they get assigned a parameter. They'll be imputed by the modeo."}
knitr::include_graphics(file.path(slides_dir, '48.png'))
```

```{r, echo = F, out.width='80%', fig.cap="B is now a vector in which some values are observed, and toehrs are paratmets. We'll stick them in an ordianry regression modeo. But now we have a prior. When B is observed, it infomrs the parameters in side it. Those will be estimated from teh observed values."}
knitr::include_graphics(file.path(slides_dir, '49.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '50.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '51.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Looks exavtly the same, but we add this prior. `ulam` automates this."}
knitr::include_graphics(file.path(slides_dir, '52.png'))
```

```{r, echo = F, out.width='80%', fig.cap=" You can see 12 imputes. What does this do to the slopes in the model?"}
knitr::include_graphics(file.path(slides_dir, '53.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Let's compare the same model. Now we can compare the slopes. Two predictors assocaited with the outcome variable in opposite directions. NOtice that the esimates have got more precise. They've also moved closer to the mean. "}
knitr::include_graphics(file.path(slides_dir, '54.png'))
```

```{r, echo = F, out.width='80%', fig.cap="We can plot the values up, but they'll have standard errors on them. Open circles is imputed. Posterior means follow the regression line. "}
knitr::include_graphics(file.path(slides_dir, '55.png'))
```

```{r, echo = F, out.width='80%', fig.cap="The disappointing thing about this midel is that the relationship between the imputed values adn the predictor is 0, which is wrong. THat's because we dodn't tell it they're assocaited."}
knitr::include_graphics(file.path(slides_dir, '56.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '57.png'))
```

```{r, echo = F, out.width='80%', fig.cap=" We fix this by making it a MVNormal. "}
knitr::include_graphics(file.path(slides_dir, '58.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Need to manually construct it. "}
knitr::include_graphics(file.path(slides_dir, '59.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '60.png'))
```

```{r, echo = F, out.width='80%', fig.cap="Then happy days, you get even more precision."}
knitr::include_graphics(file.path(slides_dir, '61.png'))
```

```{r, echo = F, out.width='80%', fig.cap="This is a really big topic. One ofthe areas that is most important is with occupancy. Really missing data problems. There's a true occupancy but 0s are not trustworthy. They ahve a special preocess that comes from the detection process that you model."}
knitr::include_graphics(file.path(slides_dir, '62.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '63.png'))
```

```{r, echo = F, out.width='80%', fig.cap=""}
knitr::include_graphics(file.path(slides_dir, '64.png'))
```

