<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="10.1 Maximum entropy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-12-04" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="10.1 Maximum entropy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>10.1 Maximum entropy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index">Index</a></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li class="has-sub"><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2.1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2.2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2.3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li class="has-sub"><a href="2.4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a>
<ul>
<li><a href="2.4-making-the-model-go.html#markov-chain-monte-carlo"><span class="toc-section-number">2.4.1</span> Markov chain Monte Carlo</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="3-sampling-from-the-imaginary.html#sampling-from-the-imaginary"><span class="toc-section-number">3</span> Sampling from the Imaginary</a>
<ul>
<li><a href="3.1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> Sampling from a grid-approximate posterior</a></li>
<li><a href="3.2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> Sampling to summarize</a></li>
<li><a href="3.3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> Sampling to simulate prediction</a></li>
<li><a href="3.4-lets-practice-with-brms.html#lets-practice-with-brms"><span class="toc-section-number">3.4</span> Let’s practice with brms</a></li>
<li><a href="practice.html#practice">Practice</a></li>
<li><a href="homework-week-1.html#homework-week-1">Homework: week 1</a></li>
</ul></li>
<li class="has-sub"><a href="4-geocentric-models.html#geocentric-models"><span class="toc-section-number">4</span> Geocentric Models</a>
<ul>
<li><a href="4.1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">4.1</span> Why normal distributions are normal</a></li>
<li><a href="4.2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">4.2</span> A language for describing models</a></li>
<li><a href="4.3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">4.3</span> Gaussian model of height</a></li>
<li><a href="4.4-linear-prediction.html#linear-prediction"><span class="toc-section-number">4.4</span> Linear prediction</a></li>
<li><a href="4.5-curves-from-lines.html#curves-from-lines"><span class="toc-section-number">4.5</span> Curves from lines</a></li>
<li><a href="4.6-practice-1.html#practice-1"><span class="toc-section-number">4.6</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="5-the-many-variables-the-spurious-waffles.html#the-many-variables-the-spurious-waffles"><span class="toc-section-number">5</span> The Many Variables &amp; The Spurious Waffles</a>
<ul>
<li><a href="5.1-spurious-association.html#spurious-association"><span class="toc-section-number">5.1</span> Spurious association</a></li>
<li><a href="5.2-masked-relationship.html#masked-relationship"><span class="toc-section-number">5.2</span> Masked relationship</a></li>
<li><a href="5.3-categorical-variables.html#categorical-variables"><span class="toc-section-number">5.3</span> Categorical variables</a></li>
<li><a href="5.4-practice-2.html#practice-2"><span class="toc-section-number">5.4</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="6-the-haunted-dag-the-causal-terror.html#the-haunted-dag-the-causal-terror"><span class="toc-section-number">6</span> The Haunted DAG &amp; The Causal Terror</a>
<ul>
<li><a href="6.1-multicollinearity.html#multicollinearity"><span class="toc-section-number">6.1</span> Multicollinearity</a></li>
<li><a href="6.2-post-treatment-bias.html#post-treatment-bias"><span class="toc-section-number">6.2</span> Post-treatment bias</a></li>
<li><a href="6.3-collider-bias.html#collider-bias"><span class="toc-section-number">6.3</span> Collider bias</a></li>
<li><a href="6.4-confronting-confounding.html#confronting-confounding"><span class="toc-section-number">6.4</span> Confronting confounding</a></li>
<li><a href="6.5-summary.html#summary"><span class="toc-section-number">6.5</span> Summary</a></li>
<li><a href="6.6-practice-3.html#practice-3"><span class="toc-section-number">6.6</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="7-ulysses-compass.html#ulysses-compass"><span class="toc-section-number">7</span> Ulysses’ Compass</a>
<ul>
<li><a href="7.1-the-problem-with-parameters.html#the-problem-with-parameters"><span class="toc-section-number">7.1</span> The problem with parameters</a></li>
<li><a href="7.2-entropy-and-accuracy.html#entropy-and-accuracy"><span class="toc-section-number">7.2</span> Entropy and accuracy</a></li>
<li><a href="7.3-golem-taming-regularization.html#golem-taming-regularization"><span class="toc-section-number">7.3</span> Golem taming: regularization</a></li>
<li><a href="7.4-predicting-predictive-accuracy.html#predicting-predictive-accuracy"><span class="toc-section-number">7.4</span> Predicting predictive accuracy</a></li>
<li><a href="7.5-model-comparison.html#model-comparison"><span class="toc-section-number">7.5</span> Model comparison</a></li>
<li><a href="7.6-practice-4.html#practice-4"><span class="toc-section-number">7.6</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="8-conditional-manatees.html#conditional-manatees"><span class="toc-section-number">8</span> Conditional Manatees</a>
<ul>
<li><a href="8.1-building-an-interaction.html#building-an-interaction"><span class="toc-section-number">8.1</span> Building an interaction</a></li>
<li><a href="8.2-symmetry-of-interactions.html#symmetry-of-interactions"><span class="toc-section-number">8.2</span> Symmetry of interactions</a></li>
<li><a href="8.3-continuous-interactions.html#continuous-interactions"><span class="toc-section-number">8.3</span> Continuous interactions</a></li>
</ul></li>
<li class="has-sub"><a href="9-markov-chain-monte-carlo-1.html#markov-chain-monte-carlo-1"><span class="toc-section-number">9</span> Markov Chain Monte Carlo</a>
<ul>
<li><a href="9.1-good-king-markov-and-his-island-kingdom.html#good-king-markov-and-his-island-kingdom"><span class="toc-section-number">9.1</span> Good King Markov and his island kingdom</a></li>
<li><a href="9.2-metropolis-algorithm.html#metropolis-algorithm"><span class="toc-section-number">9.2</span> Metropolis algorithm</a></li>
<li><a href="9.3-hamiltonian-monte-carlo.html#hamiltonian-monte-carlo"><span class="toc-section-number">9.3</span> Hamiltonian Monte Carlo</a></li>
<li><a href="9.4-easy-hmc-ulam.html#easy-hmc-ulam"><span class="toc-section-number">9.4</span> Easy HMC: <code>ulam</code></a></li>
<li><a href="9.5-care-and-feeding-of-your-markov-chain.html#care-and-feeding-of-your-markov-chain"><span class="toc-section-number">9.5</span> Care and feeding of your Markov chain</a></li>
</ul></li>
<li class="has-sub"><a href="10-big-entropy-and-the-generalized-linear-model.html#big-entropy-and-the-generalized-linear-model"><span class="toc-section-number">10</span> Big Entropy and the Generalized Linear Model</a>
<ul>
<li><a href="10.1-maximum-entropy.html#maximum-entropy"><span class="toc-section-number">10.1</span> Maximum entropy</a></li>
<li><a href="10.2-generalized-linear-models.html#generalized-linear-models"><span class="toc-section-number">10.2</span> Generalized linear models</a></li>
</ul></li>
<li class="has-sub"><a href="11-god-spiked-the-integers.html#god-spiked-the-integers"><span class="toc-section-number">11</span> God Spiked the Integers</a>
<ul>
<li><a href="11.1-binomial-regression.html#binomial-regression"><span class="toc-section-number">11.1</span> Binomial regression</a></li>
<li><a href="11.2-poisson-regression.html#poisson-regression"><span class="toc-section-number">11.2</span> Poisson regression</a></li>
<li><a href="11.3-multinomial-and-categorical-models.html#multinomial-and-categorical-models"><span class="toc-section-number">11.3</span> Multinomial and categorical models</a></li>
</ul></li>
<li class="has-sub"><a href="12-models-with-memory.html#models-with-memory"><span class="toc-section-number">12</span> Models With Memory</a>
<ul>
<li><a href="12.1-example-multilevel-tadpoles.html#example-multilevel-tadpoles"><span class="toc-section-number">12.1</span> Example: Multilevel tadpoles</a></li>
<li><a href="12.2-varying-effects-and-the-underfittingoverfitting-trade-off.html#varying-effects-and-the-underfittingoverfitting-trade-off"><span class="toc-section-number">12.2</span> Varying effects and the underfitting/overfitting trade-off</a></li>
<li><a href="12.3-more-than-one-type-of-cluster.html#more-than-one-type-of-cluster"><span class="toc-section-number">12.3</span> More than one type of cluster</a></li>
<li><a href="12.4-divergent-transitions-and-non-centered-priors.html#divergent-transitions-and-non-centered-priors"><span class="toc-section-number">12.4</span> Divergent transitions and non-centered priors</a></li>
<li><a href="12.5-multilevel-posterior-predictions.html#multilevel-posterior-predictions"><span class="toc-section-number">12.5</span> Multilevel posterior predictions</a></li>
</ul></li>
<li class="has-sub"><a href="13-adventures-in-covariance.html#adventures-in-covariance"><span class="toc-section-number">13</span> Adventures in Covariance</a>
<ul>
<li><a href="13.1-varying-slopes-by-construction.html#varying-slopes-by-construction"><span class="toc-section-number">13.1</span> Varying slopes by construction</a></li>
<li><a href="13.2-advanced-varying-slopes.html#advanced-varying-slopes"><span class="toc-section-number">13.2</span> Advanced varying slopes</a></li>
<li><a href="13.3-instruments-and-causal-designs.html#instruments-and-causal-designs"><span class="toc-section-number">13.3</span> Instruments and causal designs</a></li>
<li><a href="13.4-social-relations-as-correlated-varying-effects.html#social-relations-as-correlated-varying-effects"><span class="toc-section-number">13.4</span> Social relations as correlated varying effects</a></li>
<li><a href="13.5-continuous-categories-and-the-gaussian-process.html#continuous-categories-and-the-gaussian-process"><span class="toc-section-number">13.5</span> Continuous categories and the Gaussian process</a></li>
<li><a href="13.6-bonus-multilevel-growth-models-and-the-melsm.html#bonus-multilevel-growth-models-and-the-melsm"><span class="toc-section-number">13.6</span> Bonus: Multilevel growth models and the MELSM</a></li>
</ul></li>
<li class="has-sub"><a href="14-missing-data-and-other-opportunities.html#missing-data-and-other-opportunities"><span class="toc-section-number">14</span> Missing Data and Other Opportunities</a>
<ul>
<li><a href="14.1-measurement-error.html#measurement-error"><span class="toc-section-number">14.1</span> Measurement error</a></li>
<li><a href="14.2-missing-data.html#missing-data"><span class="toc-section-number">14.2</span> Missing data</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="maximum-entropy" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Maximum entropy</h2>
<p>Maximum entropy principle:</p>
<blockquote>
<p>The distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints.</p>
</blockquote>
<div class="figure">
<img src="slides/L11/02.png" alt="Imagine you have buckets equidistant from you. At your feet you have 100 pebbles, each painted with a number. Unique pebbles. " width="80%" />
<p class="caption marginnote shownote">
Imagine you have buckets equidistant from you. At your feet you have 100 pebbles, each painted with a number. Unique pebbles.
</p>
</div>
<div class="figure">
<img src="slides/L11/03.png" alt="What happens when we toss pebbles one at a time into the buckets at random. Eventually all 100 pebbles end up in the buckets, and you count them, and you get a distribution of pebbles. What types of distributions are really common, and what types are really rare?" width="80%" />
<p class="caption marginnote shownote">
What happens when we toss pebbles one at a time into the buckets at random. Eventually all 100 pebbles end up in the buckets, and you count them, and you get a distribution of pebbles. What types of distributions are really common, and what types are really rare?
</p>
</div>
<div class="figure">
<img src="slides/L11/04.png" alt="Think about extreme distributions first. There's only 1 way to get all 100 pebbles in bucket 1. " width="80%" />
<p class="caption marginnote shownote">
Think about extreme distributions first. There’s only 1 way to get all 100 pebbles in bucket 1.
</p>
</div>
<div class="figure">
<img src="slides/L11/05.png" alt="Same with bucket 5. So there are 5 unique distributions with all pebbles in a single bucket." width="80%" />
<p class="caption marginnote shownote">
Same with bucket 5. So there are 5 unique distributions with all pebbles in a single bucket.
</p>
</div>
<div class="figure">
<img src="slides/L11/06.png" alt="There are a bunch of distributions that will happen in a bunch of different ways. We could take a pebble from bucket 2 and swap it with one from bucket 3. How many ways could you get the same distribution. This very problem is the basis of Bayesian inference. Some distributions can arise in many more ways. It's a principle called Maximum Entropy, and it justifies Bayesian inference." width="80%" />
<p class="caption marginnote shownote">
There are a bunch of distributions that will happen in a bunch of different ways. We could take a pebble from bucket 2 and swap it with one from bucket 3. How many ways could you get the same distribution. This very problem is the basis of Bayesian inference. Some distributions can arise in many more ways. It’s a principle called Maximum Entropy, and it justifies Bayesian inference.
</p>
</div>
<div class="figure">
<img src="slides/L11/07.png" alt="We can replace the integers with $n$s. In some point, you learned that there's a formula for the number of arrangements of the pebbles." width="80%" />
<p class="caption marginnote shownote">
We can replace the integers with <span class="math inline">\(n\)</span>s. In some point, you learned that there’s a formula for the number of arrangements of the pebbles.
</p>
</div>
<div class="figure">
<img src="slides/L11/08.png" alt="This is called the multiplicity. It's the foundation of statistical inference. It gets big really fast when the Ns get equal. " width="80%" />
<p class="caption marginnote shownote">
This is called the multiplicity. It’s the foundation of statistical inference. It gets big really fast when the Ns get equal.
</p>
</div>
<p>Let’s put each distribution of pebbles in a list:</p>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="10.1-maximum-entropy.html#cb710-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span></span>
<span id="cb710-2"><a href="10.1-maximum-entropy.html#cb710-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">a =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb710-3"><a href="10.1-maximum-entropy.html#cb710-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">b =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb710-4"><a href="10.1-maximum-entropy.html#cb710-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">c =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">0</span>),</span>
<span id="cb710-5"><a href="10.1-maximum-entropy.html#cb710-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">d =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb710-6"><a href="10.1-maximum-entropy.html#cb710-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">e =</span> <span class="dv">2</span>) </span></code></pre></div>
<p>And let’s normalize each such that it is a probability distribution.</p>
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="10.1-maximum-entropy.html#cb711-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is our analogue to McElreath&#39;s `lapply()` code</span></span>
<span id="cb711-2"><a href="10.1-maximum-entropy.html#cb711-2" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb711-3"><a href="10.1-maximum-entropy.html#cb711-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_all</span>(<span class="sc">~</span> . <span class="sc">/</span> <span class="fu">sum</span>(.)) <span class="sc">%&gt;%</span> </span>
<span id="cb711-4"><a href="10.1-maximum-entropy.html#cb711-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the next few lines constitute our analogue to his `sapply()` code</span></span>
<span id="cb711-5"><a href="10.1-maximum-entropy.html#cb711-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">&quot;plot&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb711-6"><a href="10.1-maximum-entropy.html#cb711-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(plot) <span class="sc">%&gt;%</span> </span>
<span id="cb711-7"><a href="10.1-maximum-entropy.html#cb711-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">h =</span> <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">ifelse</span>(value <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">0</span>, value <span class="sc">*</span> <span class="fu">log</span>(value))))</span></code></pre></div>
<pre><code>## # A tibble: 5 × 2
##   plot      h
##   &lt;chr&gt; &lt;dbl&gt;
## 1 a     0    
## 2 b     0.639
## 3 c     0.950
## 4 d     1.47 
## 5 e     1.61</code></pre>
<p>Since these are now probability distributions, we can compute the information entropy of each as above.</p>
<p>So distribution E, which can realized by far the greatest number of ways, also has the biggest entropy.</p>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="10.1-maximum-entropy.html#cb713-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ghibli)</span>
<span id="cb713-2"><a href="10.1-maximum-entropy.html#cb713-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ghibli_palette</span>(<span class="st">&quot;MarnieMedium1&quot;</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]</span></code></pre></div>
<pre><code>## [1] &quot;#28231DFF&quot; &quot;#5E2D30FF&quot; &quot;#008E90FF&quot; &quot;#1C77A3FF&quot; &quot;#C5A387FF&quot; &quot;#67B8D6FF&quot;
## [7] &quot;#E9D097FF&quot;</code></pre>
<div class="figure">
<img src="slides/L11/09.png" alt="Only one way to get all the pebbles in bucket 3. " width="80%" />
<p class="caption marginnote shownote">
Only one way to get all the pebbles in bucket 3.
</p>
</div>
<div class="figure">
<img src="slides/L11/10.png" alt="How many ways to get the second distribution?" width="80%" />
<p class="caption marginnote shownote">
How many ways to get the second distribution?
</p>
</div>
<div class="figure">
<img src="slides/L11/11.png" alt="It's massively bigger. This will accelerate. People have really bad intuitions regarding combinatorics." width="80%" />
<p class="caption marginnote shownote">
It’s massively bigger. This will accelerate. People have really bad intuitions regarding combinatorics.
</p>
</div>
<div class="figure">
<img src="slides/L11/12.png" alt="Now we've got two in bucket 2. Now we're getting an order of magnitude increase." width="80%" />
<p class="caption marginnote shownote">
Now we’ve got two in bucket 2. Now we’re getting an order of magnitude increase.
</p>
</div>
<p><img src="slides/L11/13.png" width="80%" /></p>
<p><img src="slides/L11/14.png" width="80%" /></p>
<p><img src="slides/L11/15.png" width="80%" /></p>
<div class="figure">
<img src="slides/L11/16.png" alt="General principle: Distributions that are flat can happen in many many more ways. And this is why we bet on them. They have high entropy. Flat distributions are closer, less surprised when the distribution turns out to be different. Then become really good foundations for statistical inference, because they distribute the possibilities as widely as possible." width="80%" />
<p class="caption marginnote shownote">
General principle: Distributions that are flat can happen in many many more ways. And this is why we bet on them. They have high entropy. Flat distributions are closer, less surprised when the distribution turns out to be different. Then become really good foundations for statistical inference, because they distribute the possibilities as widely as possible.
</p>
</div>
<div class="figure">
<img src="slides/L11/17.png" alt="This is a unique way to derive the formula. It's nothing more than the multiplicity. W is the multiplicity (number of ways to get the N). Then we've normalised it across the number of the pebbles. And that turns out to be a good approximation. Information entropy is just the logarithm of the number of ways to realise a distribution. And it's maximised when the distribution is flat. And flatter distributions have higher entropy." width="80%" />
<p class="caption marginnote shownote">
This is a unique way to derive the formula. It’s nothing more than the multiplicity. W is the multiplicity (number of ways to get the N). Then we’ve normalised it across the number of the pebbles. And that turns out to be a good approximation. Information entropy is just the logarithm of the number of ways to realise a distribution. And it’s maximised when the distribution is flat. And flatter distributions have higher entropy.
</p>
</div>
<p><img src="slides/L11/18.png" width="80%" /></p>
<div class="figure">
<img src="slides/L11/19.png" alt="Most centrally associated with Jaynes. If you choose any other distribution to characterise your state of knowledge, you will be implicitly adding other information into your distribution. So if you lay out all the constraints, then solve for the distribution that's as flat as possible under those constraints, you do the best you possibly can. You're honestly characterising your ignorance." width="80%" />
<p class="caption marginnote shownote">
Most centrally associated with Jaynes. If you choose any other distribution to characterise your state of knowledge, you will be implicitly adding other information into your distribution. So if you lay out all the constraints, then solve for the distribution that’s as flat as possible under those constraints, you do the best you possibly can. You’re honestly characterising your ignorance.
</p>
</div>
<div class="figure">
<img src="slides/L11/20.png" alt="Lots of conceptual examples for. What is the information content of a prior distribution? It turns out that Bayesian updating is a special case of this principle. You can input the data as constraints, and you get the posterior distribution by solving the maximum entropy problem. High entropy is good because the distance from the truth is smaller. One way to thing about it is it's deflationary. No matter what happens, and even distrubtion is bound to arise. We put in a tiny sliver of scientific information in our model, and the rest we just bet on entropy." width="80%" />
<p class="caption marginnote shownote">
Lots of conceptual examples for. What is the information content of a prior distribution? It turns out that Bayesian updating is a special case of this principle. You can input the data as constraints, and you get the posterior distribution by solving the maximum entropy problem. High entropy is good because the distance from the truth is smaller. One way to thing about it is it’s deflationary. No matter what happens, and even distrubtion is bound to arise. We put in a tiny sliver of scientific information in our model, and the rest we just bet on entropy.
</p>
</div>
<div class="figure">
<img src="slides/L11/21.png" alt="Motivates forward to other distributions. If we're going to maximise this function, if all the $p$s are equal, they're highest. Sometimes there are constrants that prevent us from making the $p$s equal. What kind of constraints? Known mean or variance." width="80%" />
<p class="caption marginnote shownote">
Motivates forward to other distributions. If we’re going to maximise this function, if all the <span class="math inline">\(p\)</span>s are equal, they’re highest. Sometimes there are constrants that prevent us from making the <span class="math inline">\(p\)</span>s equal. What kind of constraints? Known mean or variance.
</p>
</div>
<div class="figure">
<img src="slides/L11/22.png" alt="This is actually what we did in Week 1. Shows that it's just counting. " width="80%" />
<p class="caption marginnote shownote">
This is actually what we did in Week 1. Shows that it’s just counting.
</p>
</div>
<div class="figure">
<img src="slides/L11/23.png" alt="Under some set of constraints, the distributions we use are maximum entropy distributions. Exponential distributions used for scale. They have a very clear maxent constraint. If a parameter is non-negative real, and has some mean value, then the exponential contains only that information." width="80%" />
<p class="caption marginnote shownote">
Under some set of constraints, the distributions we use are maximum entropy distributions. Exponential distributions used for scale. They have a very clear maxent constraint. If a parameter is non-negative real, and has some mean value, then the exponential contains only that information.
</p>
</div>
<p><strong><em>10.1.1. Gaussian</em></strong></p>
<p>To appreciate why the Gaussian shape has the biggest entropy for any continuous distribution with this variance, consider that entropy increases as we make a distribution flatter. So we could easily make up a probability distribution with larger entropy than the blue distribution in Figure 10.2: Just take probability from the center and put it in the tails. The more uniform the distribution looks, the higher its entropy will be. But there are limits on how much of this we can do and maintain the same variance, <span class="math inline">\(\sigma^2 = 1\)</span>.</p>
<p><em>Then the Gaussian distribution gets its shape by being as spread out as possible for a distribution with fixed variance.</em></p>
<p><strong><em>10.1.2. Binomial</em></strong></p>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="10.1-maximum-entropy.html#cb715-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb715-2"><a href="10.1-maximum-entropy.html#cb715-2" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span></span>
<span id="cb715-3"><a href="10.1-maximum-entropy.html#cb715-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">distribution =</span> letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>],</span>
<span id="cb715-4"><a href="10.1-maximum-entropy.html#cb715-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">ww =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>, <span class="dv">2</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">8</span>),</span>
<span id="cb715-5"><a href="10.1-maximum-entropy.html#cb715-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">bw =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">2</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">4</span><span class="sc">/</span><span class="dv">8</span>),</span>
<span id="cb715-6"><a href="10.1-maximum-entropy.html#cb715-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">wb =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">2</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">2</span><span class="sc">/</span><span class="dv">8</span>),</span>
<span id="cb715-7"><a href="10.1-maximum-entropy.html#cb715-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">bb =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>, <span class="dv">2</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">8</span>))</span>
<span id="cb715-8"><a href="10.1-maximum-entropy.html#cb715-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb715-9"><a href="10.1-maximum-entropy.html#cb715-9" aria-hidden="true" tabindex="-1"></a><span class="co"># table</span></span>
<span id="cb715-10"><a href="10.1-maximum-entropy.html#cb715-10" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb715-11"><a href="10.1-maximum-entropy.html#cb715-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.numeric, <span class="sc">~</span>MASS<span class="sc">::</span><span class="fu">fractions</span>(.) <span class="sc">%&gt;%</span> <span class="fu">as.character</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb715-12"><a href="10.1-maximum-entropy.html#cb715-12" aria-hidden="true" tabindex="-1"></a>  flextable<span class="sc">::</span><span class="fu">flextable</span>()</span></code></pre></div>
<template id="9b0bf497-d963-4772-bea4-3f000e078a75"><style>
.tabwid table{
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-spacing: 0;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style><div class="tabwid"><style>.cl-4eda109e{border-collapse:collapse;}.cl-4ed2a746{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4ed2ce60{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4ed3146a{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ed31492{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ed3149c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-4eda109e'><thead><tr style="overflow-wrap:break-word;"><td class="cl-4ed3149c"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">distribution</span></p></td><td class="cl-4ed3149c"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">ww</span></p></td><td class="cl-4ed3149c"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">bw</span></p></td><td class="cl-4ed3149c"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">wb</span></p></td><td class="cl-4ed3149c"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">bb</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">a</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/4</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/4</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/4</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/4</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">b</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/3</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/6</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/6</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/3</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">c</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/6</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/3</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/3</span></p></td><td class="cl-4ed3146a"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/6</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-4ed31492"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">d</span></p></td><td class="cl-4ed31492"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/8</span></p></td><td class="cl-4ed31492"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/2</span></p></td><td class="cl-4ed31492"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/4</span></p></td><td class="cl-4ed31492"><p class="cl-4ed2ce60"><span class="cl-4ed2a746">1/8</span></p></td></tr></tbody></table></div></template>
<div class="flextable-shadow-host" id="0f38f866-276e-44e5-b588-e5142c6d363b"></div>
<script>
var dest = document.getElementById("0f38f866-276e-44e5-b588-e5142c6d363b");
var template = document.getElementById("9b0bf497-d963-4772-bea4-3f000e078a75");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script>

<p>Compute the entropy of each distribution:</p>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="10.1-maximum-entropy.html#cb716-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> </span>
<span id="cb716-2"><a href="10.1-maximum-entropy.html#cb716-2" aria-hidden="true" tabindex="-1"></a>  d <span class="sc">%&gt;%</span> </span>
<span id="cb716-3"><a href="10.1-maximum-entropy.html#cb716-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>distribution,</span>
<span id="cb716-4"><a href="10.1-maximum-entropy.html#cb716-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;sequence&quot;</span>, </span>
<span id="cb716-5"><a href="10.1-maximum-entropy.html#cb716-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&quot;probability&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb716-6"><a href="10.1-maximum-entropy.html#cb716-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sequence =</span> <span class="fu">factor</span>(sequence, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;ww&quot;</span>, <span class="st">&quot;bw&quot;</span>, <span class="st">&quot;wb&quot;</span>, <span class="st">&quot;bb&quot;</span>)))</span>
<span id="cb716-7"><a href="10.1-maximum-entropy.html#cb716-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb716-8"><a href="10.1-maximum-entropy.html#cb716-8" aria-hidden="true" tabindex="-1"></a>d  <span class="sc">%&gt;%</span> </span>
<span id="cb716-9"><a href="10.1-maximum-entropy.html#cb716-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sequence, <span class="at">y =</span> probability, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb716-10"><a href="10.1-maximum-entropy.html#cb716-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>, <span class="at">color =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;PonyoMedium&quot;</span>)[<span class="dv">4</span>]) <span class="sc">+</span></span>
<span id="cb716-11"><a href="10.1-maximum-entropy.html#cb716-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;PonyoMedium&quot;</span>)[<span class="dv">5</span>]) <span class="sc">+</span></span>
<span id="cb716-12"><a href="10.1-maximum-entropy.html#cb716-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>, <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb716-13"><a href="10.1-maximum-entropy.html#cb716-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb716-14"><a href="10.1-maximum-entropy.html#cb716-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.ticks.x =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb716-15"><a href="10.1-maximum-entropy.html#cb716-15" aria-hidden="true" tabindex="-1"></a>        <span class="at">panel.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;PonyoMedium&quot;</span>)[<span class="dv">2</span>]),</span>
<span id="cb716-16"><a href="10.1-maximum-entropy.html#cb716-16" aria-hidden="true" tabindex="-1"></a>        <span class="at">panel.grid =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb716-17"><a href="10.1-maximum-entropy.html#cb716-17" aria-hidden="true" tabindex="-1"></a>        <span class="at">strip.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;PonyoMedium&quot;</span>)[<span class="dv">6</span>])) <span class="sc">+</span></span>
<span id="cb716-18"><a href="10.1-maximum-entropy.html#cb716-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> distribution)</span></code></pre></div>
<div class="figure">
<img src="10_big_entropy_and_the_generalized_linear_model_files/figure-html/unnamed-chunk-27-1.png" alt="Figure 10.3" width="672" />
<p class="caption marginnote shownote">
Figure 10.3
</p>
</div>
<div class="sourceCode" id="cb717"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb717-1"><a href="10.1-maximum-entropy.html#cb717-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb717-2"><a href="10.1-maximum-entropy.html#cb717-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># `str_count()` will count the number of times &quot;b&quot; occurs within a given row of `sequence`</span></span>
<span id="cb717-3"><a href="10.1-maximum-entropy.html#cb717-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n_b =</span> <span class="fu">str_count</span>(sequence, <span class="st">&quot;b&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb717-4"><a href="10.1-maximum-entropy.html#cb717-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">product =</span> probability <span class="sc">*</span> n_b) <span class="sc">%&gt;%</span> </span>
<span id="cb717-5"><a href="10.1-maximum-entropy.html#cb717-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(distribution) <span class="sc">%&gt;%</span> </span>
<span id="cb717-6"><a href="10.1-maximum-entropy.html#cb717-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">expected_value =</span> <span class="fu">sum</span>(product))</span></code></pre></div>
<pre><code>## # A tibble: 4 × 2
##   distribution expected_value
##   &lt;chr&gt;                 &lt;dbl&gt;
## 1 a                         1
## 2 b                         1
## 3 c                         1
## 4 d                         1</code></pre>
<div class="sourceCode" id="cb719"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb719-1"><a href="10.1-maximum-entropy.html#cb719-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb719-2"><a href="10.1-maximum-entropy.html#cb719-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(distribution) <span class="sc">%&gt;%</span> </span>
<span id="cb719-3"><a href="10.1-maximum-entropy.html#cb719-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">entropy =</span> <span class="sc">-</span><span class="fu">sum</span>(probability <span class="sc">*</span> <span class="fu">log</span>(probability)))</span></code></pre></div>
<pre><code>## # A tibble: 4 × 2
##   distribution entropy
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 a               1.39
## 2 b               1.33
## 3 c               1.33
## 4 d               1.21</code></pre>
<p>The binomial with this expected value is:</p>
<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb721-1"><a href="10.1-maximum-entropy.html#cb721-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb721-2"><a href="10.1-maximum-entropy.html#cb721-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb721-3"><a href="10.1-maximum-entropy.html#cb721-3" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb721-4"><a href="10.1-maximum-entropy.html#cb721-4" aria-hidden="true" tabindex="-1"></a>  a <span class="ot">&lt;-</span> </span>
<span id="cb721-5"><a href="10.1-maximum-entropy.html#cb721-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>((<span class="dv">1</span> <span class="sc">-</span> p)<span class="sc">^</span><span class="dv">2</span>, </span>
<span id="cb721-6"><a href="10.1-maximum-entropy.html#cb721-6" aria-hidden="true" tabindex="-1"></a>    p <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p), </span>
<span id="cb721-7"><a href="10.1-maximum-entropy.html#cb721-7" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span> <span class="sc">-</span> p) <span class="sc">*</span> p, </span>
<span id="cb721-8"><a href="10.1-maximum-entropy.html#cb721-8" aria-hidden="true" tabindex="-1"></a>    p<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb721-9"><a href="10.1-maximum-entropy.html#cb721-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## [1] 0.09 0.21 0.21 0.49</code></pre>
<p>This distribution is definitely not flat. So to appreciate how this distribution has maximum entropy—is the flattest distribution with expected value 1.4—we’ll simulate a bunch of distributions with the same expected value and then compare entropies. The entropy of the distribution above is just:</p>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb723-1"><a href="10.1-maximum-entropy.html#cb723-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">sum</span>(a <span class="sc">*</span> <span class="fu">log</span>(a))</span></code></pre></div>
<pre><code>## [1] 1.221729</code></pre>
<p>So if we randomly generate thousands of distributions with expected value 1.4, we expect that none will have a larger entropy than this.</p>
<p>We can use a short R function to simulate random probability distributions that have any specified expected value. The code below will do the job. Don’t worry about how it works (unless you want to).</p>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="10.1-maximum-entropy.html#cb725-1" aria-hidden="true" tabindex="-1"></a>sim_p <span class="ot">&lt;-</span> <span class="cf">function</span>(seed, <span class="at">g =</span> <span class="fl">1.4</span>) {</span>
<span id="cb725-2"><a href="10.1-maximum-entropy.html#cb725-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb725-3"><a href="10.1-maximum-entropy.html#cb725-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(seed)</span>
<span id="cb725-4"><a href="10.1-maximum-entropy.html#cb725-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb725-5"><a href="10.1-maximum-entropy.html#cb725-5" aria-hidden="true" tabindex="-1"></a>  x_123 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">3</span>)</span>
<span id="cb725-6"><a href="10.1-maximum-entropy.html#cb725-6" aria-hidden="true" tabindex="-1"></a>  x_4   <span class="ot">&lt;-</span> ((g) <span class="sc">*</span> <span class="fu">sum</span>(x_123) <span class="sc">-</span> x_123[<span class="dv">2</span>] <span class="sc">-</span> x_123[<span class="dv">3</span>]) <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">-</span> g)</span>
<span id="cb725-7"><a href="10.1-maximum-entropy.html#cb725-7" aria-hidden="true" tabindex="-1"></a>  z     <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">c</span>(x_123, x_4))</span>
<span id="cb725-8"><a href="10.1-maximum-entropy.html#cb725-8" aria-hidden="true" tabindex="-1"></a>  p     <span class="ot">&lt;-</span> <span class="fu">c</span>(x_123, x_4) <span class="sc">/</span> z</span>
<span id="cb725-9"><a href="10.1-maximum-entropy.html#cb725-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb725-10"><a href="10.1-maximum-entropy.html#cb725-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">h   =</span> <span class="sc">-</span><span class="fu">sum</span>(p <span class="sc">*</span> <span class="fu">log</span>(p)), </span>
<span id="cb725-11"><a href="10.1-maximum-entropy.html#cb725-11" aria-hidden="true" tabindex="-1"></a>         <span class="at">p   =</span> p,</span>
<span id="cb725-12"><a href="10.1-maximum-entropy.html#cb725-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">key =</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="st">&quot;ww&quot;</span>, <span class="st">&quot;bw&quot;</span>, <span class="st">&quot;wb&quot;</span>, <span class="st">&quot;bb&quot;</span>), <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;ww&quot;</span>, <span class="st">&quot;bw&quot;</span>, <span class="st">&quot;wb&quot;</span>, <span class="st">&quot;bb&quot;</span>)))</span>
<span id="cb725-13"><a href="10.1-maximum-entropy.html#cb725-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb725-14"><a href="10.1-maximum-entropy.html#cb725-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb726-1"><a href="10.1-maximum-entropy.html#cb726-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sim_p</span>(<span class="at">seed =</span> <span class="fl">9.9</span>, <span class="at">g =</span> <span class="fl">1.4</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 3
##       h      p key  
##   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;
## 1  1.02 0.197  ww   
## 2  1.02 0.0216 bw   
## 3  1.02 0.184  wb   
## 4  1.02 0.597  bb</code></pre>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb728-1"><a href="10.1-maximum-entropy.html#cb728-1" aria-hidden="true" tabindex="-1"></a><span class="co"># how many replications would you like?</span></span>
<span id="cb728-2"><a href="10.1-maximum-entropy.html#cb728-2" aria-hidden="true" tabindex="-1"></a>n_rep <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb728-3"><a href="10.1-maximum-entropy.html#cb728-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb728-4"><a href="10.1-maximum-entropy.html#cb728-4" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span></span>
<span id="cb728-5"><a href="10.1-maximum-entropy.html#cb728-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">seed =</span> <span class="dv">1</span><span class="sc">:</span>n_rep) <span class="sc">%&gt;%</span> </span>
<span id="cb728-6"><a href="10.1-maximum-entropy.html#cb728-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sim =</span> <span class="fu">map2</span>(seed, <span class="fl">1.4</span>, sim_p)) <span class="sc">%&gt;%</span> </span>
<span id="cb728-7"><a href="10.1-maximum-entropy.html#cb728-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(sim)</span>
<span id="cb728-8"><a href="10.1-maximum-entropy.html#cb728-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb728-9"><a href="10.1-maximum-entropy.html#cb728-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(d)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
##    seed     h      p key  
##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;
## 1     1  1.21 0.108  ww   
## 2     1  1.21 0.151  bw   
## 3     1  1.21 0.233  wb   
## 4     1  1.21 0.508  bb   
## 5     2  1.21 0.0674 ww   
## 6     2  1.21 0.256  bw</code></pre>
<p>Let’s split out the entropies and distributions, so that it’s easier to work with them.</p>
<p>Now we can ask what the largest observed entropy was:</p>
<div class="sourceCode" id="cb730"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb730-1"><a href="10.1-maximum-entropy.html#cb730-1" aria-hidden="true" tabindex="-1"></a>ranked_d <span class="ot">&lt;-</span></span>
<span id="cb730-2"><a href="10.1-maximum-entropy.html#cb730-2" aria-hidden="true" tabindex="-1"></a>  d <span class="sc">%&gt;%</span> </span>
<span id="cb730-3"><a href="10.1-maximum-entropy.html#cb730-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(seed) <span class="sc">%&gt;%</span> </span>
<span id="cb730-4"><a href="10.1-maximum-entropy.html#cb730-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(h)) <span class="sc">%&gt;%</span> </span>
<span id="cb730-5"><a href="10.1-maximum-entropy.html#cb730-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb730-6"><a href="10.1-maximum-entropy.html#cb730-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># here&#39;s the rank order step</span></span>
<span id="cb730-7"><a href="10.1-maximum-entropy.html#cb730-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">rank =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_rep, <span class="at">each =</span> <span class="dv">4</span>))</span>
<span id="cb730-8"><a href="10.1-maximum-entropy.html#cb730-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb730-9"><a href="10.1-maximum-entropy.html#cb730-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(ranked_d)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 5
##    seed     h      p key    rank
##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;
## 1 55665  1.22 0.0903 ww        1
## 2 55665  1.22 0.209  bw        1
## 3 55665  1.22 0.210  wb        1
## 4 55665  1.22 0.490  bb        1
## 5 71132  1.22 0.0902 ww        2
## 6 71132  1.22 0.210  bw        2</code></pre>
<div class="sourceCode" id="cb732"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb732-1"><a href="10.1-maximum-entropy.html#cb732-1" aria-hidden="true" tabindex="-1"></a>subset_d <span class="ot">&lt;-</span></span>
<span id="cb732-2"><a href="10.1-maximum-entropy.html#cb732-2" aria-hidden="true" tabindex="-1"></a>  ranked_d <span class="sc">%&gt;%</span></span>
<span id="cb732-3"><a href="10.1-maximum-entropy.html#cb732-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># I arrived at these `rank` values by trial and error</span></span>
<span id="cb732-4"><a href="10.1-maximum-entropy.html#cb732-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(rank <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">87373</span>, n_rep <span class="sc">-</span> <span class="dv">1500</span>, n_rep <span class="sc">-</span> <span class="dv">10</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb732-5"><a href="10.1-maximum-entropy.html#cb732-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># I arrived at the `height` values by trial and error, too</span></span>
<span id="cb732-6"><a href="10.1-maximum-entropy.html#cb732-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">height       =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">8</span>, <span class="fl">2.25</span>, .<span class="dv">75</span>, .<span class="dv">5</span>), <span class="at">each =</span> <span class="dv">4</span>),</span>
<span id="cb732-7"><a href="10.1-maximum-entropy.html#cb732-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">distribution =</span> <span class="fu">rep</span>(letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">each =</span> <span class="dv">4</span>))</span>
<span id="cb732-8"><a href="10.1-maximum-entropy.html#cb732-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(subset_d)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 7
##    seed     h      p key    rank height distribution
##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;       
## 1 55665  1.22 0.0903 ww        1   8    a           
## 2 55665  1.22 0.209  bw        1   8    a           
## 3 55665  1.22 0.210  wb        1   8    a           
## 4 55665  1.22 0.490  bb        1   8    a           
## 5 50981  1.00 0.0459 ww    87373   2.25 b           
## 6 50981  1.00 0.0459 bw    87373   2.25 b</code></pre>
<div class="sourceCode" id="cb734"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb734-1"><a href="10.1-maximum-entropy.html#cb734-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span></span>
<span id="cb734-2"><a href="10.1-maximum-entropy.html#cb734-2" aria-hidden="true" tabindex="-1"></a>  d <span class="sc">%&gt;%</span> </span>
<span id="cb734-3"><a href="10.1-maximum-entropy.html#cb734-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> h)) <span class="sc">+</span></span>
<span id="cb734-4"><a href="10.1-maximum-entropy.html#cb734-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">size =</span> <span class="dv">0</span>, <span class="at">fill =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">3</span>],</span>
<span id="cb734-5"><a href="10.1-maximum-entropy.html#cb734-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">adjust =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb734-6"><a href="10.1-maximum-entropy.html#cb734-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note the data statements for the next two geoms</span></span>
<span id="cb734-7"><a href="10.1-maximum-entropy.html#cb734-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="at">data =</span> subset_d <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(seed) <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="dv">1</span>),</span>
<span id="cb734-8"><a href="10.1-maximum-entropy.html#cb734-8" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">aes</span>(<span class="at">ymin =</span> <span class="dv">0</span>, <span class="at">ymax =</span> height),</span>
<span id="cb734-9"><a href="10.1-maximum-entropy.html#cb734-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">color =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">5</span>]) <span class="sc">+</span></span>
<span id="cb734-10"><a href="10.1-maximum-entropy.html#cb734-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> subset_d <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(seed) <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="dv">1</span>),</span>
<span id="cb734-11"><a href="10.1-maximum-entropy.html#cb734-11" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">y =</span> height <span class="sc">+</span> .<span class="dv">5</span>, <span class="at">label =</span> distribution)) <span class="sc">+</span></span>
<span id="cb734-12"><a href="10.1-maximum-entropy.html#cb734-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Entropy&quot;</span>, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="at">from =</span> .<span class="dv">7</span>, <span class="at">to =</span> <span class="fl">1.2</span>, <span class="at">by =</span> .<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb734-13"><a href="10.1-maximum-entropy.html#cb734-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">7</span>]),</span>
<span id="cb734-14"><a href="10.1-maximum-entropy.html#cb734-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">panel.grid =</span> <span class="fu">element_blank</span>())</span></code></pre></div>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="10.1-maximum-entropy.html#cb735-1" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span></span>
<span id="cb735-2"><a href="10.1-maximum-entropy.html#cb735-2" aria-hidden="true" tabindex="-1"></a>  ranked_d <span class="sc">%&gt;%</span></span>
<span id="cb735-3"><a href="10.1-maximum-entropy.html#cb735-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(rank <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">87373</span>, n_rep <span class="sc">-</span> <span class="dv">1500</span>, n_rep <span class="sc">-</span> <span class="dv">10</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb735-4"><a href="10.1-maximum-entropy.html#cb735-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">distribution =</span> <span class="fu">rep</span>(letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">each =</span> <span class="dv">4</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb735-5"><a href="10.1-maximum-entropy.html#cb735-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb735-6"><a href="10.1-maximum-entropy.html#cb735-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> key, <span class="at">y =</span> p, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb735-7"><a href="10.1-maximum-entropy.html#cb735-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">5</span>]) <span class="sc">+</span></span>
<span id="cb735-8"><a href="10.1-maximum-entropy.html#cb735-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>, <span class="at">color =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">4</span>]) <span class="sc">+</span></span>
<span id="cb735-9"><a href="10.1-maximum-entropy.html#cb735-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>, <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">75</span>)) <span class="sc">+</span></span>
<span id="cb735-10"><a href="10.1-maximum-entropy.html#cb735-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb735-11"><a href="10.1-maximum-entropy.html#cb735-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.ticks.x =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb735-12"><a href="10.1-maximum-entropy.html#cb735-12" aria-hidden="true" tabindex="-1"></a>        <span class="at">panel.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">7</span>]),</span>
<span id="cb735-13"><a href="10.1-maximum-entropy.html#cb735-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">panel.grid =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb735-14"><a href="10.1-maximum-entropy.html#cb735-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">strip.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> <span class="fu">ghibli_palette</span>(<span class="st">&quot;LaputaMedium&quot;</span>)[<span class="dv">6</span>])) <span class="sc">+</span></span>
<span id="cb735-15"><a href="10.1-maximum-entropy.html#cb735-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> distribution)</span>
<span id="cb735-16"><a href="10.1-maximum-entropy.html#cb735-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb735-17"><a href="10.1-maximum-entropy.html#cb735-17" aria-hidden="true" tabindex="-1"></a><span class="co"># combine and plot</span></span>
<span id="cb735-18"><a href="10.1-maximum-entropy.html#cb735-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb735-19"><a href="10.1-maximum-entropy.html#cb735-19" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">|</span> p2</span></code></pre></div>
<div class="figure">
<img src="10_big_entropy_and_the_generalized_linear_model_files/figure-html/unnamed-chunk-31-1.png" alt="Figure 10.4" width="672" />
<p class="caption marginnote shownote">
Figure 10.4
</p>
</div>
<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb736-1"><a href="10.1-maximum-entropy.html#cb736-1" aria-hidden="true" tabindex="-1"></a>ranked_d <span class="sc">%&gt;%</span> </span>
<span id="cb736-2"><a href="10.1-maximum-entropy.html#cb736-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(key) <span class="sc">%&gt;%</span> </span>
<span id="cb736-3"><a href="10.1-maximum-entropy.html#cb736-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(h)) <span class="sc">%&gt;%</span> </span>
<span id="cb736-4"><a href="10.1-maximum-entropy.html#cb736-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 5
## # Groups:   key [4]
##    seed     h      p key    rank
##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;
## 1 55665  1.22 0.0903 ww        1
## 2 55665  1.22 0.209  bw        1
## 3 55665  1.22 0.210  wb        1
## 4 55665  1.22 0.490  bb        1</code></pre>
<p>And that’s almost exactly <span class="math inline">\({0.09, 0.21, 0.21, 0.49}\)</span>, the distribution we calculated earlier.
The other distributions in Figure 10.4—B, C, and D—are all less even than A.</p>
<p>There is no guarantee that the maximum entropy distribution is the best probability distribution for the real problem you are analyzing. But there is a guarantee that no other distribution more conservatively reflects your assumptions.</p>
<p>That’s not everything, but nor is it nothing. Any other distribution implies hidden constraints that are unknown to us, reflecting phantom assumptions. A full and honest accounting of assumptions is helpful, because it aids in understanding how a model misbehaves. And since all models misbehave sometimes, it’s good to be able to anticipate those times before they happen, as well as to learn from those times when they inevitably do.</p>
<p><strong>Rethinking: Conditional independence</strong></p>
<p>What is usually meant by “independence” in a probability distribution is just that each observation is uncorrelated with the others, once we know the corresponding predictor values.</p>
</div>
<p style="text-align: center;">
<a href="10-big-entropy-and-the-generalized-linear-model.html"><button class="btn btn-default">Previous</button></a>
<a href="10.2-generalized-linear-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
