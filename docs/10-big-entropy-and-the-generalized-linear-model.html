<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Big Entropy and the Generalized Linear Model | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-06-22" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 10 Big Entropy and the Generalized Linear Model | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>Chapter 10 Big Entropy and the Generalized Linear Model | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index">Index</a></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2-1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2-2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2-3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li><a href="2-4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a></li>
</ul></li>
<li><a href="3-sampling-from-the-imaginary.html#sampling-from-the-imaginary"><span class="toc-section-number">3</span> Sampling from the Imaginary</a>
<ul>
<li><a href="3-1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> Sampling from a grid-approximate posterior</a></li>
<li><a href="3-2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> Sampling to summarize</a></li>
<li><a href="3-3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> Sampling to simulate prediction</a></li>
<li><a href="practice.html#practice">Practice</a></li>
<li><a href="homework-week-1.html#homework-week-1">Homework: week 1</a></li>
</ul></li>
<li><a href="4-geocentric-models.html#geocentric-models"><span class="toc-section-number">4</span> Geocentric Models</a>
<ul>
<li><a href="4-1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">4.1</span> Why normal distributions are normal</a></li>
<li><a href="4-2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">4.2</span> A language for describing models</a></li>
<li><a href="4-3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">4.3</span> Gaussian model of height</a></li>
<li><a href="4-4-linear-prediction.html#linear-prediction"><span class="toc-section-number">4.4</span> Linear prediction</a></li>
<li><a href="4-5-curves-from-lines.html#curves-from-lines"><span class="toc-section-number">4.5</span> Curves from lines</a></li>
<li><a href="4-6-practice-1.html#practice-1"><span class="toc-section-number">4.6</span> Practice</a></li>
</ul></li>
<li><a href="5-the-many-variables-the-spurious-waffles.html#the-many-variables-the-spurious-waffles"><span class="toc-section-number">5</span> The Many Variables &amp; The Spurious Waffles</a>
<ul>
<li><a href="5-1-spurious-association.html#spurious-association"><span class="toc-section-number">5.1</span> Spurious association</a></li>
<li><a href="5-2-masked-relationship.html#masked-relationship"><span class="toc-section-number">5.2</span> Masked relationship</a></li>
<li><a href="5-3-categorical-variables.html#categorical-variables"><span class="toc-section-number">5.3</span> Categorical variables</a></li>
<li><a href="5-4-practice-2.html#practice-2"><span class="toc-section-number">5.4</span> Practice</a></li>
</ul></li>
<li><a href="6-the-haunted-dag-the-causal-terror.html#the-haunted-dag-the-causal-terror"><span class="toc-section-number">6</span> The Haunted DAG &amp; The Causal Terror</a>
<ul>
<li><a href="6-1-multicollinearity.html#multicollinearity"><span class="toc-section-number">6.1</span> Multicollinearity</a></li>
<li><a href="6-2-post-treatment-bias.html#post-treatment-bias"><span class="toc-section-number">6.2</span> Post-treatment bias</a></li>
<li><a href="6-3-collider-bias.html#collider-bias"><span class="toc-section-number">6.3</span> Collider bias</a></li>
<li><a href="6-4-confronting-confounding.html#confronting-confounding"><span class="toc-section-number">6.4</span> Confronting confounding</a></li>
<li><a href="6-5-summary.html#summary"><span class="toc-section-number">6.5</span> Summary</a></li>
<li><a href="6-6-practice-3.html#practice-3"><span class="toc-section-number">6.6</span> Practice</a></li>
</ul></li>
<li><a href="7-ulysses-compass.html#ulysses-compass"><span class="toc-section-number">7</span> Ulysses’ Compass</a>
<ul>
<li><a href="7-1-the-problem-with-parameters.html#the-problem-with-parameters"><span class="toc-section-number">7.1</span> The problem with parameters</a></li>
<li><a href="7-2-entropy-and-accuracy.html#entropy-and-accuracy"><span class="toc-section-number">7.2</span> Entropy and accuracy</a></li>
<li><a href="7-3-golem-taming-regularization.html#golem-taming-regularization"><span class="toc-section-number">7.3</span> Golem taming: regularization</a></li>
<li><a href="7-4-predicting-predictive-accuracy.html#predicting-predictive-accuracy"><span class="toc-section-number">7.4</span> Predicting predictive accuracy</a></li>
<li><a href="7-5-model-comparison.html#model-comparison"><span class="toc-section-number">7.5</span> Model comparison</a></li>
<li><a href="7-6-practice-4.html#practice-4"><span class="toc-section-number">7.6</span> Practice</a></li>
</ul></li>
<li><a href="8-conditional-manatees.html#conditional-manatees"><span class="toc-section-number">8</span> Conditional Manatees</a></li>
<li><a href="9-markov-chain-monte-carlo.html#markov-chain-monte-carlo"><span class="toc-section-number">9</span> Markov Chain Monte Carlo</a></li>
<li><a href="10-big-entropy-and-the-generalized-linear-model.html#big-entropy-and-the-generalized-linear-model"><span class="toc-section-number">10</span> Big Entropy and the Generalized Linear Model</a></li>
<li><a href="11-god-spiked-the-integers.html#god-spiked-the-integers"><span class="toc-section-number">11</span> God Spiked the Integers</a></li>
<li><a href="12-monsters-and-mixtures.html#monsters-and-mixtures"><span class="toc-section-number">12</span> Monsters and Mixtures</a></li>
<li><a href="13-models-with-memory.html#models-with-memory"><span class="toc-section-number">13</span> Models With Memory</a></li>
<li><a href="14-adventures-in-covariance.html#adventures-in-covariance"><span class="toc-section-number">14</span> Adventures in Covariance</a></li>
<li><a href="15-missing-data-and-other-opportunities.html#missing-data-and-other-opportunities"><span class="toc-section-number">15</span> Missing Data and Other Opportunities</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="big-entropy-and-the-generalized-linear-model" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Big Entropy and the Generalized Linear Model</h1>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="10-big-entropy-and-the-generalized-linear-model.html#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb404-2"><a href="10-big-entropy-and-the-generalized-linear-model.html#cb404-2" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;code/scripts/source.R&quot;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="10-big-entropy-and-the-generalized-linear-model.html#cb405-1" aria-hidden="true" tabindex="-1"></a>slides_dir <span class="ot">=</span> here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;docs/slides/L11&quot;</span>)</span></code></pre></div>
<div class="figure">
<img src="slides/L11/01.png" alt="We'll move conceptually at a slow rate, which will set up a bunch of different models for this week and next." width="80%" />
<p class="caption marginnote shownote">
We’ll move conceptually at a slow rate, which will set up a bunch of different models for this week and next.
</p>
</div>
<div class="figure">
<img src="slides/L11/02.png" alt="Imagine you have buckets equidistant from you. At your feet you have 100 pebbles, each painted with a number. Unique pebbles. " width="80%" />
<p class="caption marginnote shownote">
Imagine you have buckets equidistant from you. At your feet you have 100 pebbles, each painted with a number. Unique pebbles.
</p>
</div>
<div class="figure">
<img src="slides/L11/03.png" alt="What happens when we toss pebbles one at a time into the buckets at random. Eventually all 100 pebbles end up in the buckets, and you count them, and you get a distribution of pebbles. What types of distributions are really common, and what types are really rare?" width="80%" />
<p class="caption marginnote shownote">
What happens when we toss pebbles one at a time into the buckets at random. Eventually all 100 pebbles end up in the buckets, and you count them, and you get a distribution of pebbles. What types of distributions are really common, and what types are really rare?
</p>
</div>
<div class="figure">
<img src="slides/L11/04.png" alt="Think about extreme distributions first. There's only 1 way to get all 100 pebbles in bucket 1. " width="80%" />
<p class="caption marginnote shownote">
Think about extreme distributions first. There’s only 1 way to get all 100 pebbles in bucket 1.
</p>
</div>
<div class="figure">
<img src="slides/L11/05.png" alt="Same with bucket 5. So there are 5 unique distributions with all pebbles in a single bucket." width="80%" />
<p class="caption marginnote shownote">
Same with bucket 5. So there are 5 unique distributions with all pebbles in a single bucket.
</p>
</div>
<div class="figure">
<img src="slides/L11/06.png" alt="There are a bunch of distributions that will happen in a bunch of different ways. We could take a pebble from bucket 2 and swap it with one from bucket 3. How many ways could you get the same distribution. This very problem is the basis of Bayesian inference. Some distributions can arise in many more ways. It's a principle called Maximum Entropy, and it justifies Bayesian inference." width="80%" />
<p class="caption marginnote shownote">
There are a bunch of distributions that will happen in a bunch of different ways. We could take a pebble from bucket 2 and swap it with one from bucket 3. How many ways could you get the same distribution. This very problem is the basis of Bayesian inference. Some distributions can arise in many more ways. It’s a principle called Maximum Entropy, and it justifies Bayesian inference.
</p>
</div>
<div class="figure">
<img src="slides/L11/07.png" alt="We can replace the integers with $n$s. In some point, you learned that there's a formula for the number of arrangements of the pebbles." width="80%" />
<p class="caption marginnote shownote">
We can replace the integers with <span class="math inline">\(n\)</span>s. In some point, you learned that there’s a formula for the number of arrangements of the pebbles.
</p>
</div>
<div class="figure">
<img src="slides/L11/08.png" alt="This is called the multiplicity. It's the foundation of statistical inference. It gets big really fast when the Ns get equal. " width="80%" />
<p class="caption marginnote shownote">
This is called the multiplicity. It’s the foundation of statistical inference. It gets big really fast when the Ns get equal.
</p>
</div>
<div class="figure">
<img src="slides/L11/09.png" alt="Only one way to get all the pebbles in bucket 3. " width="80%" />
<p class="caption marginnote shownote">
Only one way to get all the pebbles in bucket 3.
</p>
</div>
<div class="figure">
<img src="slides/L11/10.png" alt="How many ways to get the second distribution?" width="80%" />
<p class="caption marginnote shownote">
How many ways to get the second distribution?
</p>
</div>
<div class="figure">
<img src="slides/L11/11.png" alt="It's massively bigger. This will accelerate. People have really bad intuitions regarding combinatorics." width="80%" />
<p class="caption marginnote shownote">
It’s massively bigger. This will accelerate. People have really bad intuitions regarding combinatorics.
</p>
</div>
<div class="figure">
<img src="slides/L11/12.png" alt="Now we've got two in bucket 2. Now we're getting an order of magnitude increase." width="80%" />
<p class="caption marginnote shownote">
Now we’ve got two in bucket 2. Now we’re getting an order of magnitude increase.
</p>
</div>
<p><img src="slides/L11/13.png" width="80%" /></p>
<p><img src="slides/L11/14.png" width="80%" /></p>
<p><img src="slides/L11/15.png" width="80%" /></p>
<div class="figure">
<img src="slides/L11/16.png" alt="General principle: Distributions that are flat can happen in many many more ways. And this is why we bet on them. They have high entropy. Flat distributions are closer, less surprised when the distribution turns out to be different. Then become really good foundations for statistical inference, because they distribute the possibilities as widely as possible." width="80%" />
<p class="caption marginnote shownote">
General principle: Distributions that are flat can happen in many many more ways. And this is why we bet on them. They have high entropy. Flat distributions are closer, less surprised when the distribution turns out to be different. Then become really good foundations for statistical inference, because they distribute the possibilities as widely as possible.
</p>
</div>
<div class="figure">
<img src="slides/L11/17.png" alt="This is a unique way to derive the formula. It's nothing more than the multiplicity. W is the multiplicity (number of ways to get the N). Then we've normalised it across the number of the pebbles. And that turns out to be a good approximation. Information entropy is just the logarithm of the number of ways to realise a distribution. And it's maximised when the distribution is flat. And flatter distributions have higher entropy." width="80%" />
<p class="caption marginnote shownote">
This is a unique way to derive the formula. It’s nothing more than the multiplicity. W is the multiplicity (number of ways to get the N). Then we’ve normalised it across the number of the pebbles. And that turns out to be a good approximation. Information entropy is just the logarithm of the number of ways to realise a distribution. And it’s maximised when the distribution is flat. And flatter distributions have higher entropy.
</p>
</div>
<p><img src="slides/L11/18.png" width="80%" /></p>
<div class="figure">
<img src="slides/L11/19.png" alt="Most centrally associated with Jaynes. If you choose any other distribution to characterise your state of knowledge, you will be implicitly adding other information into your distribution. So if you lay out all the constraints, then solve for the distribution that's as flat as possible under those constraints, you do the best you possibly can. You're honestly characterising your ignorance." width="80%" />
<p class="caption marginnote shownote">
Most centrally associated with Jaynes. If you choose any other distribution to characterise your state of knowledge, you will be implicitly adding other information into your distribution. So if you lay out all the constraints, then solve for the distribution that’s as flat as possible under those constraints, you do the best you possibly can. You’re honestly characterising your ignorance.
</p>
</div>
<div class="figure">
<img src="slides/L11/20.png" alt="Lots of conceptual examples for. What is the information content of a prior distribution? It turns out that Bayesian updating is a special case of this principle. You can input the data as constraints, and you get the posterior distribution by solving the maximum entropy problem. High entropy is good because the distance from the truth is smaller. One way to thing about it is it's deflationary. No matter what happens, and even distrubtion is bound to arise. We put in a tiny sliver of scientific information in our model, and the rest we just bet on entropy." width="80%" />
<p class="caption marginnote shownote">
Lots of conceptual examples for. What is the information content of a prior distribution? It turns out that Bayesian updating is a special case of this principle. You can input the data as constraints, and you get the posterior distribution by solving the maximum entropy problem. High entropy is good because the distance from the truth is smaller. One way to thing about it is it’s deflationary. No matter what happens, and even distrubtion is bound to arise. We put in a tiny sliver of scientific information in our model, and the rest we just bet on entropy.
</p>
</div>
<div class="figure">
<img src="slides/L11/21.png" alt="Motivates forward to other distributions. If we're going to maximise this function, if all the $p$s are equal, they're highest. Sometimes there are constrants that prevent us from making the $p$s equal. What kind of constraints? Known mean or variance." width="80%" />
<p class="caption marginnote shownote">
Motivates forward to other distributions. If we’re going to maximise this function, if all the <span class="math inline">\(p\)</span>s are equal, they’re highest. Sometimes there are constrants that prevent us from making the <span class="math inline">\(p\)</span>s equal. What kind of constraints? Known mean or variance.
</p>
</div>
<div class="figure">
<img src="slides/L11/22.png" alt="This is actually what we did in Week 1. Shows that it's just counting. " width="80%" />
<p class="caption marginnote shownote">
This is actually what we did in Week 1. Shows that it’s just counting.
</p>
</div>
<div class="figure">
<img src="slides/L11/23.png" alt="Under some set of constraints, the distributions we use are maximum entropy distributions. Exponential distributions used for scale. They have a very clear maxent constraint. If a parameter is non-negative real, and has some mean value, then the exponential contains only that information." width="80%" />
<p class="caption marginnote shownote">
Under some set of constraints, the distributions we use are maximum entropy distributions. Exponential distributions used for scale. They have a very clear maxent constraint. If a parameter is non-negative real, and has some mean value, then the exponential contains only that information.
</p>
</div>
<div class="figure">
<img src="slides/L11/24.png" alt="Larger family of geocentric linear models. We want to connect a linear model to a mean to the distribution. Unreasonably effective given how geocentric it is. We pick an outcome distribution, then model the parameters using weird things called links, whcih link the distribution to some model. Can do all kinds of fancy things with the same basic strategy. Often if you don't want to play this game, when you write it down, it'll turn out to be a linear model anyway. In most cases, you probably want a GLM." width="80%" />
<p class="caption marginnote shownote">
Larger family of geocentric linear models. We want to connect a linear model to a mean to the distribution. Unreasonably effective given how geocentric it is. We pick an outcome distribution, then model the parameters using weird things called links, whcih link the distribution to some model. Can do all kinds of fancy things with the same basic strategy. Often if you don’t want to play this game, when you write it down, it’ll turn out to be a linear model anyway. In most cases, you probably want a GLM.
</p>
</div>
<div class="figure">
<img src="slides/L11/25.png" alt="Distributions arise from natural processes. And resist histomancy. This doesn't make sense under any framework. You want to use knowledge of your constraints to figure it out. There's no statistical framework where the aggregate outcomes is going to have any particular distribution." width="80%" />
<p class="caption marginnote shownote">
Distributions arise from natural processes. And resist histomancy. This doesn’t make sense under any framework. You want to use knowledge of your constraints to figure it out. There’s no statistical framework where the aggregate outcomes is going to have any particular distribution.
</p>
</div>
<div class="figure">
<img src="slides/L11/26.png" alt="Going to build GLMs with these different outcome distributions. Just an extension of what you've already been doing. Exponential is everyone's favourite because it only has 1 parameter. Lambda is a rate, and the mean is 1/lambda. Generatively it can arise from a machine with a number of parts. If one part breaks, the whole thing stops working. A fruit fly is the same. Bunch of parts inside the washing machine, and each part has a chance of breaking at a particular time, the waiting time until the washing machine stops is exponentially distributed. " width="80%" />
<p class="caption marginnote shownote">
Going to build GLMs with these different outcome distributions. Just an extension of what you’ve already been doing. Exponential is everyone’s favourite because it only has 1 parameter. Lambda is a rate, and the mean is 1/lambda. Generatively it can arise from a machine with a number of parts. If one part breaks, the whole thing stops working. A fruit fly is the same. Bunch of parts inside the washing machine, and each part has a chance of breaking at a particular time, the waiting time until the washing machine stops is exponentially distributed.
</p>
</div>
<div class="figure">
<img src="slides/L11/27.png" alt="If you count events arising from exponential distributions. Mortality rates of fruit flies is bionimal. Like coin flips. Each fly could or could not ascend. And the binomial is maxent. " width="80%" />
<p class="caption marginnote shownote">
If you count events arising from exponential distributions. Mortality rates of fruit flies is bionimal. Like coin flips. Each fly could or could not ascend. And the binomial is maxent.
</p>
</div>
<div class="figure">
<img src="slides/L11/28.png" alt="Poisson. Two ways of thinking about it. If you have a binomially distributed variable, but the probabiity of success is low and there are lots of flies oserved over a long time. " width="80%" />
<p class="caption marginnote shownote">
Poisson. Two ways of thinking about it. If you have a binomially distributed variable, but the probabiity of success is low and there are lots of flies oserved over a long time.
</p>
</div>
<div class="figure">
<img src="slides/L11/29.png" alt="If you think about the time to the event of the exponential - how long did you wait until the washing machine broke, if you start adding up that time, those waiting times are distributed like Gamma. Also maxent. e.g. age of onset of cancer, perhaps because there are a lot of cellular defence mechanisms, and all of them need to fail. " width="80%" />
<p class="caption marginnote shownote">
If you think about the time to the event of the exponential - how long did you wait until the washing machine broke, if you start adding up that time, those waiting times are distributed like Gamma. Also maxent. e.g. age of onset of cancer, perhaps because there are a lot of cellular defence mechanisms, and all of them need to fail.
</p>
</div>
<div class="figure">
<img src="slides/L11/30.png" alt="If you get a Gamma with a really large mean, it converges to a Normal. But not the only way - all roads lead to normal. And it's hard to leave. So these are generative processes, based on the constraints. Doesn't mean that they're correct, but it's the betting part." width="80%" />
<p class="caption marginnote shownote">
If you get a Gamma with a really large mean, it converges to a Normal. But not the only way - all roads lead to normal. And it’s hard to leave. So these are generative processes, based on the constraints. Doesn’t mean that they’re correct, but it’s the betting part.
</p>
</div>
<div class="figure">
<img src="slides/L11/31.png" alt="Tide prediction engine. When we get to GLMs, the metaphor is very potent. It's a mechinical computer, and a part of it is the prediction of times, and then there's messy stuff at the bottom that's calculating the output. You're absolutely wedded to the prediction perspective. Hard to have intuition about the parameters. You want to understand the prediction space, and you understand the parameters by observing their effects on prediction." width="80%" />
<p class="caption marginnote shownote">
Tide prediction engine. When we get to GLMs, the metaphor is very potent. It’s a mechinical computer, and a part of it is the prediction of times, and then there’s messy stuff at the bottom that’s calculating the output. You’re absolutely wedded to the prediction perspective. Hard to have intuition about the parameters. You want to understand the prediction space, and you understand the parameters by observing their effects on prediction.
</p>
</div>
<div class="figure">
<img src="slides/L11/32.png" alt="Just need to think about before the data have arrived, you know things about the outcome variable. e.g. count variables are integers starting at 0, so there are no negative counts. So from the beginning you know things about them. That constrains the distributions before they arrive. Next week we'll move onto monsters because we glue together different models using links. Likhert scales are ordinal scales, but they're not numeric. What it takes to get from 1 to 2 might be different from what it takes to go from 2 to 3. Fight monsters by making monsters. Mixture models are super useful. Bear a lot of resemblance to multi-level models." width="80%" />
<p class="caption marginnote shownote">
Just need to think about before the data have arrived, you know things about the outcome variable. e.g. count variables are integers starting at 0, so there are no negative counts. So from the beginning you know things about them. That constrains the distributions before they arrive. Next week we’ll move onto monsters because we glue together different models using links. Likhert scales are ordinal scales, but they’re not numeric. What it takes to get from 1 to 2 might be different from what it takes to go from 2 to 3. Fight monsters by making monsters. Mixture models are super useful. Bear a lot of resemblance to multi-level models.
</p>
</div>
<div class="figure">
<img src="slides/L11/33.png" alt="Consider the Gaussian linear regression. It's super benign, and that's because it has a special property: the scientific measurement units and the parameter for the mean are the same. " width="80%" />
<p class="caption marginnote shownote">
Consider the Gaussian linear regression. It’s super benign, and that’s because it has a special property: the scientific measurement units and the parameter for the mean are the same.
</p>
</div>
<div class="figure">
<img src="slides/L11/34.png" alt="The much more typical case is the binomial model. If you want to connect a linear model to $p$, it's a probability. Probability is unitless. They're divided out. But the outcome has counts. So now the units aren't the same, and we need something that connects the parameter to the outcome scale. We need some function to put in wehre the question mark is so that it obeys physics." width="80%" />
<p class="caption marginnote shownote">
The much more typical case is the binomial model. If you want to connect a linear model to <span class="math inline">\(p\)</span>, it’s a probability. Probability is unitless. They’re divided out. But the outcome has counts. So now the units aren’t the same, and we need something that connects the parameter to the outcome scale. We need some function to put in wehre the question mark is so that it obeys physics.
</p>
</div>
<div class="figure">
<img src="slides/L11/35.png" alt="We're going to wrap $p$ in some function which constraitns it. say there's some function we can apply to the probability so that it's linear in the outcome scale." width="80%" />
<p class="caption marginnote shownote">
We’re going to wrap <span class="math inline">\(p\)</span> in some function which constraitns it. say there’s some function we can apply to the probability so that it’s linear in the outcome scale.
</p>
</div>
<div class="figure">
<img src="slides/L11/36.png" alt="Searching is hearder. OLS can be used, but can be fragile. We're just going to use MCMC because we don't want to worry about it." width="80%" />
<p class="caption marginnote shownote">
Searching is hearder. OLS can be used, but can be fragile. We’re just going to use MCMC because we don’t want to worry about it.
</p>
</div>
<div class="figure">
<img src="slides/L11/37.png" alt="One of the fun things is that suddenly all the varibles automatically interact with each others. Imagine you're trying to understand the habitat preferences of a reptile. If it gets really cold, probability of surivival is low, but hot they're fine. On the porobability scale, evenutally things get cold enough that you're dead no matter what. If any one varible will kill the lizxard, it doesn't matter what the other variables are doing. That's an interaction. No matter how much food you give it, it's going to die if it's really cold. You want your model to do this." width="80%" />
<p class="caption marginnote shownote">
One of the fun things is that suddenly all the varibles automatically interact with each others. Imagine you’re trying to understand the habitat preferences of a reptile. If it gets really cold, probability of surivival is low, but hot they’re fine. On the porobability scale, evenutally things get cold enough that you’re dead no matter what. If any one varible will kill the lizxard, it doesn’t matter what the other variables are doing. That’s an interaction. No matter how much food you give it, it’s going to die if it’s really cold. You want your model to do this.
</p>
</div>
<div class="figure">
<img src="slides/L11/38.png" alt="If you like to think about the rate of change in a linear regression, you take a partial slope. Do this with any GLM, and the chain rule kicks in. And you get a much less nice expression. In a logistic regression, that's the equation. If you take the partial derivative, you get this thing in teh right That's the rate of change. " width="80%" />
<p class="caption marginnote shownote">
If you like to think about the rate of change in a linear regression, you take a partial slope. Do this with any GLM, and the chain rule kicks in. And you get a much less nice expression. In a logistic regression, that’s the equation. If you take the partial derivative, you get this thing in teh right That’s the rate of change.
</p>
</div>
<div class="figure">
<img src="slides/L11/39.png" alt="Let's move into doing some good work. We'll model some counts of events. What the Bionimal distriibution for? Counts of success out of trials. There's some constant expected value condtioinal on a set of predictor variables. Under those conditions the maxent distribution is binomial. " width="80%" />
<p class="caption marginnote shownote">
Let’s move into doing some good work. We’ll model some counts of events. What the Bionimal distriibution for? Counts of success out of trials. There’s some constant expected value condtioinal on a set of predictor variables. Under those conditions the maxent distribution is binomial.
</p>
</div>
<div class="figure">
<img src="slides/L11/40.png" alt="The expected value is $np$. Note the variance is related to the expected value. In general, the Guassian is the only distrubiton where the mean and the variance are independent. With all others, if the mean gets big, so does the variance. " width="80%" />
<p class="caption marginnote shownote">
The expected value is <span class="math inline">\(np\)</span>. Note the variance is related to the expected value. In general, the Guassian is the only distrubiton where the mean and the variance are independent. With all others, if the mean gets big, so does the variance.
</p>
</div>
<div class="figure">
<img src="slides/L11/41.png" alt="So we're going to plug a linear model and attach it to $p$." width="80%" />
<p class="caption marginnote shownote">
So we’re going to plug a linear model and attach it to <span class="math inline">\(p\)</span>.
</p>
</div>
<div class="figure">
<img src="slides/L11/42.png" alt="On the horizontal I have some predictor $x$. What are the log odds? The log of $p$. " width="80%" />
<p class="caption marginnote shownote">
On the horizontal I have some predictor <span class="math inline">\(x\)</span>. What are the log odds? The log of <span class="math inline">\(p\)</span>.
</p>
</div>
<div class="figure">
<img src="slides/L11/43.png" alt="If you do this, there's a really nice mapping onto the probability scale, where x is linear on the log odds scale, and constrained to the (0,1) internval on teh probability scale. This arises from the maxent derivation of the binomial distribution. In machine learnign they call it the maxent classifier." width="80%" />
<p class="caption marginnote shownote">
If you do this, there’s a really nice mapping onto the probability scale, where x is linear on the log odds scale, and constrained to the (0,1) internval on teh probability scale. This arises from the maxent derivation of the binomial distribution. In machine learnign they call it the maxent classifier.
</p>
</div>
<div class="figure">
<img src="slides/L11/44.png" alt="Logit means 'log odds'. $p$ is the probaility scale.  " width="80%" />
<p class="caption marginnote shownote">
Logit means ‘log odds’. <span class="math inline">\(p\)</span> is the probaility scale.
</p>
</div>
<p><img src="slides/L11/45.png" width="80%" /></p>
<p><img src="slides/L11/46.png" width="80%" /></p>
<p><img src="slides/L11/47.png" width="80%" /></p>
<div class="figure">
<img src="slides/L11/48.png" alt="It really is just log odds. If you measure stuff in odds, you can measure things really well. Log odds are just the log of the odds. That's linear. How do you get back to the linear scale? Solve for $p$. " width="80%" />
<p class="caption marginnote shownote">
It really is just log odds. If you measure stuff in odds, you can measure things really well. Log odds are just the log of the odds. That’s linear. How do you get back to the linear scale? Solve for <span class="math inline">\(p\)</span>.
</p>
</div>
<div class="figure">
<img src="slides/L11/49.png" alt="This is the conventional way to link, because it has lots of good mathematical properties." width="80%" />
<p class="caption marginnote shownote">
This is the conventional way to link, because it has lots of good mathematical properties.
</p>
</div>
<div class="figure">
<img src="slides/L11/50.png" alt="For intuition, you want to relate the two scales. Horizontal is probability. Vertical is log-odds. Log odds 0 is equal chance. There's this compression effect, so you need some scale. Log odds of -1 is 1/4. This is really important for defining priors. " width="80%" />
<p class="caption marginnote shownote">
For intuition, you want to relate the two scales. Horizontal is probability. Vertical is log-odds. Log odds 0 is equal chance. There’s this compression effect, so you need some scale. Log odds of -1 is 1/4. This is really important for defining priors.
</p>
</div>
<div class="figure">
<img src="slides/L11/51.png" alt="We use this thing because its the natural link within the probability formula. It arises naturally in the derivation of the distribution. Big and legitimate links. If you have a scientific model, you can derive the link automatically. " width="80%" />
<p class="caption marginnote shownote">
We use this thing because its the natural link within the probability formula. It arises naturally in the derivation of the distribution. Big and legitimate links. If you have a scientific model, you can derive the link automatically.
</p>
</div>
<div class="figure">
<img src="slides/L11/52.png" alt="Example dataset. Imagien you're a chimp on the close side. If you pull a lever, it's expand out on both sides. There may or may not be food in both trays. If you pull the right, they other chimp will get the snack too. Interested in whetehr chimps care about this distinction. It's not enough to do the experiment. They might pull the right because there's more food there. One of the treatments is to remove the partner from the other end. Also chimpanzees are handed, so you have to adjust for that. BUt you want to know the differnce - do they pulll the prosocial option more if there's a chimp on the other end. " width="80%" />
<p class="caption marginnote shownote">
Example dataset. Imagien you’re a chimp on the close side. If you pull a lever, it’s expand out on both sides. There may or may not be food in both trays. If you pull the right, they other chimp will get the snack too. Interested in whetehr chimps care about this distinction. It’s not enough to do the experiment. They might pull the right because there’s more food there. One of the treatments is to remove the partner from the other end. Also chimpanzees are handed, so you have to adjust for that. BUt you want to know the differnce - do they pulll the prosocial option more if there’s a chimp on the other end.
</p>
</div>
<div class="figure">
<img src="slides/L11/53.png" alt="Alone with no other chimp. Prosocial and asocial option is balanced across left and right. We want to predict the outcome as a function of the condition -the total treatment. " width="80%" />
<p class="caption marginnote shownote">
Alone with no other chimp. Prosocial and asocial option is balanced across left and right. We want to predict the outcome as a function of the condition -the total treatment.
</p>
</div>
<div class="figure">
<img src="slides/L11/54.png" alt="Four possible distinct unordered treatments. Wnat to estimate the tendency to pull the lever. The linear model on teh left is the Binomial. $lpha$ measures handedness. Then we have a vector of four $\beta$ parameters$, one for each treatment. Note that the Bernoullli is just the Binomial with one trial." width="80%" />
<p class="caption marginnote shownote">
Four possible distinct unordered treatments. Wnat to estimate the tendency to pull the lever. The linear model on teh left is the Binomial. <span class="math inline">\(lpha\)</span> measures handedness. Then we have a vector of four <span class="math inline">\(\beta\)</span> parameters$, one for each treatment. Note that the Bernoullli is just the Binomial with one trial.
</p>
</div>
<div class="figure">
<img src="slides/L11/55.png" alt="How to do priors? They behave in GLMs in very unpredictable ways. So need to do prior simulation. Let's consider a skeletal verison of Bionmal regression where the linear model is some alpha, some intercept, the average log odds across all trials. What kind of prior to set on that. Let's set a Gaussian. Centered on a half. But what about the scale? What happens when you pick $\omega$." width="80%" />
<p class="caption marginnote shownote">
How to do priors? They behave in GLMs in very unpredictable ways. So need to do prior simulation. Let’s consider a skeletal verison of Bionmal regression where the linear model is some alpha, some intercept, the average log odds across all trials. What kind of prior to set on that. Let’s set a Gaussian. Centered on a half. But what about the scale? What happens when you pick <span class="math inline">\(\omega\)</span>.
</p>
</div>
<div class="figure">
<img src="slides/L11/56.png" alt="Let's try with $''omega = 10$. " width="80%" />
<p class="caption marginnote shownote">
Let’s try with <span class="math inline">\(&#39;&#39;omega = 10\)</span>.
</p>
</div>
<div class="figure">
<img src="slides/L11/57.png" alt="What happens is we have the prior proabability scale. THe black density curve is the prior hwere you assign alpha the normal 0,10. Because a Gaussian distribution has huge amount of mass beyond absolute 3. Most of the mass is outside the extremes. Because the range of the log-odds scale is -4,4. So when you change it to the probabilty scale, it puts a lot of probability in the tails. We can adopt this heuritsitc postiion of having something flat, which is normal with omega of 1.5" width="80%" />
<p class="caption marginnote shownote">
What happens is we have the prior proabability scale. THe black density curve is the prior hwere you assign alpha the normal 0,10. Because a Gaussian distribution has huge amount of mass beyond absolute 3. Most of the mass is outside the extremes. Because the range of the log-odds scale is -4,4. So when you change it to the probabilty scale, it puts a lot of probability in the tails. We can adopt this heuritsitc postiion of having something flat, which is normal with omega of 1.5
</p>
</div>
<div class="figure">
<img src="slides/L11/58.png" alt="Next we'll talk about slopes." width="80%" />
<p class="caption marginnote shownote">
Next we’ll talk about slopes.
</p>
</div>
<p><img src="slides/L11/59.png" width="80%" /></p>
<p><img src="slides/L11/60.png" width="80%" /></p>
<p><img src="slides/L11/61.png" width="80%" /></p>
<p><img src="slides/L11/62.png" width="80%" /></p>
<p><img src="slides/L11/63.png" width="80%" /></p>
<p><img src="slides/L11/64.png" width="80%" /></p>
<p><img src="slides/L11/65.png" width="80%" /></p>
<p><img src="slides/L11/66.png" width="80%" /></p>
<p><img src="slides/L11/67.png" width="80%" /></p>
<p><img src="slides/L11/68.png" width="80%" /></p>
<p><img src="slides/L11/69.png" width="80%" /></p>

</div>
<p style="text-align: center;">
<a href="9-markov-chain-monte-carlo.html"><button class="btn btn-default">Previous</button></a>
<a href="11-god-spiked-the-integers.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
