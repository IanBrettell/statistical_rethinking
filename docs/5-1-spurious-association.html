<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="5.1 Spurious association | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-06-08" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="5.1 Spurious association | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>5.1 Spurious association | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index">Index</a></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2-1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2-2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2-3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li><a href="2-4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a></li>
</ul></li>
<li><a href="3-sampling-from-the-imaginary.html#sampling-from-the-imaginary"><span class="toc-section-number">3</span> Sampling from the Imaginary</a>
<ul>
<li><a href="3-1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> Sampling from a grid-approximate posterior</a></li>
<li><a href="3-2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> Sampling to summarize</a></li>
<li><a href="3-3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> Sampling to simulate prediction</a></li>
<li><a href="practice.html#practice">Practice</a></li>
<li><a href="homework-week-1.html#homework-week-1">Homework: week 1</a></li>
</ul></li>
<li><a href="4-geocentric-models.html#geocentric-models"><span class="toc-section-number">4</span> Geocentric Models</a>
<ul>
<li><a href="4-1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">4.1</span> Why normal distributions are normal</a></li>
<li><a href="4-2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">4.2</span> A language for describing models</a></li>
<li><a href="4-3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">4.3</span> Gaussian model of height</a></li>
<li><a href="4-4-linear-prediction.html#linear-prediction"><span class="toc-section-number">4.4</span> Linear prediction</a></li>
<li><a href="4-5-curves-from-lines.html#curves-from-lines"><span class="toc-section-number">4.5</span> Curves from lines</a></li>
<li><a href="4-6-practice-1.html#practice-1"><span class="toc-section-number">4.6</span> Practice</a></li>
</ul></li>
<li><a href="5-the-many-variables-the-spurious-waffles.html#the-many-variables-the-spurious-waffles"><span class="toc-section-number">5</span> The Many Variables &amp; The Spurious Waffles</a>
<ul>
<li><a href="5-1-spurious-association.html#spurious-association"><span class="toc-section-number">5.1</span> Spurious association</a></li>
</ul></li>
<li><a href="6-the-haunted-dag-the-causal-terror.html#the-haunted-dag-the-causal-terror"><span class="toc-section-number">6</span> The Haunted DAG &amp; The Causal Terror</a></li>
<li><a href="7-ulysses-compass.html#ulysses-compass"><span class="toc-section-number">7</span> Ulysses’ Compass</a></li>
<li><a href="8-conditional-manatees.html#conditional-manatees"><span class="toc-section-number">8</span> Conditional Manatees</a></li>
<li><a href="9-markov-chain-monte-carlo.html#markov-chain-monte-carlo"><span class="toc-section-number">9</span> Markov Chain Monte Carlo</a></li>
<li><a href="10-big-entropy-and-the-generalized-linear-model.html#big-entropy-and-the-generalized-linear-model"><span class="toc-section-number">10</span> Big Entropy and the Generalized Linear Model</a></li>
<li><a href="11-god-spiked-the-integers.html#god-spiked-the-integers"><span class="toc-section-number">11</span> God Spiked the Integers</a></li>
<li><a href="12-monsters-and-mixtures.html#monsters-and-mixtures"><span class="toc-section-number">12</span> Monsters and Mixtures</a></li>
<li><a href="13-models-with-memory.html#models-with-memory"><span class="toc-section-number">13</span> Models With Memory</a></li>
<li><a href="14-adventures-in-covariance.html#adventures-in-covariance"><span class="toc-section-number">14</span> Adventures in Covariance</a></li>
<li><a href="15-missing-data-and-other-opportunities.html#missing-data-and-other-opportunities"><span class="toc-section-number">15</span> Missing Data and Other Opportunities</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="spurious-association" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Spurious association</h2>
<div class="figure">
<img src="slides/L05/09.png" alt="Another variable - median age at marriage, could also be causal. Which could it be? We want to now put both in the same model, which reveals that one of these is an imposter." width="80%" />
<p class="caption marginnote shownote">
Another variable - median age at marriage, could also be causal. Which could it be? We want to now put both in the same model, which reveals that one of these is an imposter.
</p>
</div>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="5-1-spurious-association.html#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb202-2"><a href="5-1-spurious-association.html#cb202-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;WaffleDivorce&quot;</span>)</span>
<span id="cb202-3"><a href="5-1-spurious-association.html#cb202-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> WaffleDivorce</span>
<span id="cb202-4"><a href="5-1-spurious-association.html#cb202-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-5"><a href="5-1-spurious-association.html#cb202-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardise variables</span></span>
<span id="cb202-6"><a href="5-1-spurious-association.html#cb202-6" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>D <span class="ot">=</span> rethinking<span class="sc">::</span><span class="fu">standardize</span>( d<span class="sc">$</span>Divorce )</span>
<span id="cb202-7"><a href="5-1-spurious-association.html#cb202-7" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>M <span class="ot">=</span> rethinking<span class="sc">::</span><span class="fu">standardize</span>( d<span class="sc">$</span>Marriage)</span>
<span id="cb202-8"><a href="5-1-spurious-association.html#cb202-8" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>A <span class="ot">=</span> rethinking<span class="sc">::</span><span class="fu">standardize</span>( d<span class="sc">$</span>MedianAgeMarriage)</span></code></pre></div>
<div class="figure">
<img src="slides/L05/10.png" alt="This is what multiple regression is for. We've got two questions in a model that has both questions. Do you get any predictive information from the second variable? " width="80%" />
<p class="caption marginnote shownote">
This is what multiple regression is for. We’ve got two questions in a model that has both questions. Do you get any predictive information from the second variable?
</p>
</div>
<div class="figure">
<img src="slides/L05/11.png" alt="The arrows have directions to them; can be bidirectional. They're acyclic, so they don't loop. They're called graphs because they have nodes and edges. The associations are Bayesian networks, but they don't have interactions." width="80%" />
<p class="caption marginnote shownote">
The arrows have directions to them; can be bidirectional. They’re acyclic, so they don’t loop. They’re called graphs because they have nodes and edges. The associations are Bayesian networks, but they don’t have interactions.
</p>
</div>
<div class="figure">
<img src="slides/L05/12.png" alt="We have here a plausible graph. How does A affect M? If the young are getting married too, then more people are getting married. Median age of marriage influences divorce rate because possibly young people make worse decisions. Is the arrow from M to D there? We want to tell the difference between A and D, and M and D. **NOTE**: when you're walking along the path, you can walk backwards along a path. " width="80%" />
<p class="caption marginnote shownote">
We have here a plausible graph. How does A affect M? If the young are getting married too, then more people are getting married. Median age of marriage influences divorce rate because possibly young people make worse decisions. Is the arrow from M to D there? We want to tell the difference between A and D, and M and D. <strong>NOTE</strong>: when you’re walking along the path, you can walk backwards along a path.
</p>
</div>
<div class="figure">
<img src="slides/L05/13.png" alt="We want to tell the difference between these two things. Something causes waffle house, and something causes divorce. That thing is the South, and so they end up being correlated even though there's no causal relationship." width="80%" />
<p class="caption marginnote shownote">
We want to tell the difference between these two things. Something causes waffle house, and something causes divorce. That thing is the South, and so they end up being correlated even though there’s no causal relationship.
</p>
</div>
<p><code>|</code> means “conditional on”.</p>
<div class="figure">
<img src="slides/L05/14.png" alt="Already know how to do these, just need to do extra stuff. Linear regression is a special type of Bayesian network where there's an outcome variable, which is assigned Gaussian probability with some mean that is conditional based on some variable, and a standard deviation. The `i`s are states. Have some intercepts. " width="80%" />
<p class="caption marginnote shownote">
Already know how to do these, just need to do extra stuff. Linear regression is a special type of Bayesian network where there’s an outcome variable, which is assigned Gaussian probability with some mean that is conditional based on some variable, and a standard deviation. The <code>i</code>s are states. Have some intercepts.
</p>
</div>
<div class="figure">
<img src="slides/L05/15.png" alt="Have to think harder about priors now. It help a lot by standardising the priors - converting them into Z-scores. If you make all your variables z-scores, you make you life easier. (But not in all cases.) " width="80%" />
<p class="caption marginnote shownote">
Have to think harder about priors now. It help a lot by standardising the priors - converting them into Z-scores. If you make all your variables z-scores, you make you life easier. (But not in all cases.)
</p>
</div>
<pre class="marginfigure"><code>When you standardise your predictors, you&#39;re setting your mean as 0. The regression line has to go through 0, and so alpha should be 0, We&#39;ll give it a Gaussian prior with a tight SD. Maybe should even be tighter. 

Slopes are a little harder. You don&#39;t want to use flat priors because you don&#39;t want it to think wildly impossible slopes are possible to start. That&#39;s why we do some prior predictive simulation.</code></pre>
<div class="figure">
<img src="slides/L05/16.png" alt="You can fit your model. You can run that model. `extract.prior` samples from the prior to simulate. Then pass it to `link` to create predictions based on the prior. Then you can plot the regression lines. " width="80%" />
<p class="caption marginnote shownote">
You can fit your model. You can run that model. <code>extract.prior</code> samples from the prior to simulate. Then pass it to <code>link</code> to create predictions based on the prior. Then you can plot the regression lines.
</p>
</div>
<p>If <span class="math inline">\(\beta_A = 1\)</span>, that would imply that a change of one standard deviation in age at marriage is assocatied with a change of one standard deviation in divorce.</p>
<p>To know if that’s strong, how big is a standard deviation of age at marriage?</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="5-1-spurious-association.html#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>( d<span class="sc">$</span>MedianAgeMarriage )</span></code></pre></div>
<pre><code>## [1] 1.24363</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="5-1-spurious-association.html#cb206-1" aria-hidden="true" tabindex="-1"></a>m5<span class="fl">.1</span> <span class="ot">=</span> rethinking<span class="sc">::</span><span class="fu">quap</span>(</span>
<span id="cb206-2"><a href="5-1-spurious-association.html#cb206-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb206-3"><a href="5-1-spurious-association.html#cb206-3" aria-hidden="true" tabindex="-1"></a>    D <span class="sc">~</span> <span class="fu">dnorm</span>( mu , sigma ) ,</span>
<span id="cb206-4"><a href="5-1-spurious-association.html#cb206-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> bA <span class="sc">*</span> A ,</span>
<span id="cb206-5"><a href="5-1-spurious-association.html#cb206-5" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.2</span> ) ,</span>
<span id="cb206-6"><a href="5-1-spurious-association.html#cb206-6" aria-hidden="true" tabindex="-1"></a>    bA <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.5</span> ) ,</span>
<span id="cb206-7"><a href="5-1-spurious-association.html#cb206-7" aria-hidden="true" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dexp</span>( <span class="dv">1</span> )</span>
<span id="cb206-8"><a href="5-1-spurious-association.html#cb206-8" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data =</span> d )</span></code></pre></div>
<div class="figure">
<img src="slides/L05/17.png" alt="This is 50 regression lines from the prior. Standardised deviation of marriage. 2 SD is almost all. If your model thinks a possible divorce rate is outside the observable range of divorce rates, then they're bad. This prior allows really strong relationships. Allows it to govern nearly all the variation in divorce rate. But we'll move forward with this. This is the flattest prior you could justify scientifically. Priors by frequentists consider even crazier priors, just as vertical lines." width="80%" />
<p class="caption marginnote shownote">
This is 50 regression lines from the prior. Standardised deviation of marriage. 2 SD is almost all. If your model thinks a possible divorce rate is outside the observable range of divorce rates, then they’re bad. This prior allows really strong relationships. Allows it to govern nearly all the variation in divorce rate. But we’ll move forward with this. This is the flattest prior you could justify scientifically. Priors by frequentists consider even crazier priors, just as vertical lines.
</p>
</div>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="5-1-spurious-association.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10</span>)</span>
<span id="cb207-2"><a href="5-1-spurious-association.html#cb207-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> rethinking<span class="sc">::</span><span class="fu">extract.prior</span>( m5<span class="fl">.1</span> )</span>
<span id="cb207-3"><a href="5-1-spurious-association.html#cb207-3" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> rethinking<span class="sc">::</span><span class="fu">link</span>( m5<span class="fl">.1</span> , </span>
<span id="cb207-4"><a href="5-1-spurious-association.html#cb207-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">post=</span>prior , </span>
<span id="cb207-5"><a href="5-1-spurious-association.html#cb207-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">data=</span><span class="fu">list</span>( <span class="at">A=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>) ) )</span>
<span id="cb207-6"><a href="5-1-spurious-association.html#cb207-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-7"><a href="5-1-spurious-association.html#cb207-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( <span class="cn">NULL</span> , <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>) , <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>) )</span>
<span id="cb207-8"><a href="5-1-spurious-association.html#cb207-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span> ) <span class="fu">lines</span>( <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>) , mu[i,] , <span class="at">col=</span><span class="fu">col.alpha</span>(<span class="st">&quot;black&quot;</span>,<span class="fl">0.4</span>) )</span></code></pre></div>
<p><img src="05_the_many_variables_and_the_spurious_waffles_files/figure-html/5.4-1.svg" width="672" /></p>
<p>Now for the posterior predictions:</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="5-1-spurious-association.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute percentile interval of mean</span></span>
<span id="cb208-2"><a href="5-1-spurious-association.html#cb208-2" aria-hidden="true" tabindex="-1"></a>A_seq <span class="ot">=</span> <span class="fu">seq</span>( <span class="at">from=</span><span class="sc">-</span><span class="dv">3</span> , <span class="at">to=</span><span class="fl">3.2</span> , <span class="at">length.out=</span><span class="dv">30</span> )</span>
<span id="cb208-3"><a href="5-1-spurious-association.html#cb208-3" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="fu">link</span>( m5<span class="fl">.1</span> , <span class="at">data=</span><span class="fu">list</span>(<span class="at">A=</span>A_seq) )</span>
<span id="cb208-4"><a href="5-1-spurious-association.html#cb208-4" aria-hidden="true" tabindex="-1"></a>mu.mean <span class="ot">=</span> <span class="fu">apply</span>( mu , <span class="dv">2</span>, mean )</span>
<span id="cb208-5"><a href="5-1-spurious-association.html#cb208-5" aria-hidden="true" tabindex="-1"></a>mu.PI <span class="ot">=</span> <span class="fu">apply</span>( mu , <span class="dv">2</span> , PI )</span>
<span id="cb208-6"><a href="5-1-spurious-association.html#cb208-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-7"><a href="5-1-spurious-association.html#cb208-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot it all</span></span>
<span id="cb208-8"><a href="5-1-spurious-association.html#cb208-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( D <span class="sc">~</span> A , <span class="at">data=</span>d , <span class="at">col=</span>rangi2 )</span>
<span id="cb208-9"><a href="5-1-spurious-association.html#cb208-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>( A_seq , mu.mean , <span class="at">lwd=</span><span class="dv">2</span> )</span>
<span id="cb208-10"><a href="5-1-spurious-association.html#cb208-10" aria-hidden="true" tabindex="-1"></a><span class="fu">shade</span>( mu.PI , A_seq )</span></code></pre></div>
<p><img src="05_the_many_variables_and_the_spurious_waffles_files/figure-html/5.5-1.svg" width="672" />
<span class="math inline">\(\beta_A\)</span> is reliably negative. You can fit a similar regression for the relationship in the left-hand plot:</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="5-1-spurious-association.html#cb209-1" aria-hidden="true" tabindex="-1"></a>m5<span class="fl">.2</span> <span class="ot">&lt;-</span> rethinking<span class="sc">::</span><span class="fu">quap</span>(</span>
<span id="cb209-2"><a href="5-1-spurious-association.html#cb209-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb209-3"><a href="5-1-spurious-association.html#cb209-3" aria-hidden="true" tabindex="-1"></a>    D <span class="sc">~</span> <span class="fu">dnorm</span>( mu , sigma ) ,</span>
<span id="cb209-4"><a href="5-1-spurious-association.html#cb209-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> bM <span class="sc">*</span> M ,</span>
<span id="cb209-5"><a href="5-1-spurious-association.html#cb209-5" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.2</span> ) ,</span>
<span id="cb209-6"><a href="5-1-spurious-association.html#cb209-6" aria-hidden="true" tabindex="-1"></a>    bM <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.5</span> ) ,</span>
<span id="cb209-7"><a href="5-1-spurious-association.html#cb209-7" aria-hidden="true" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dexp</span>( <span class="dv">1</span> )</span>
<span id="cb209-8"><a href="5-1-spurious-association.html#cb209-8" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data =</span> d )</span></code></pre></div>
<p>Drawing a DAG</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="5-1-spurious-association.html#cb210-1" aria-hidden="true" tabindex="-1"></a>dag5<span class="fl">.1</span> <span class="ot">&lt;-</span> dagitty<span class="sc">::</span><span class="fu">dagitty</span>( <span class="st">&quot;dag{ A -&gt; D; A -&gt; M; M -&gt; D }&quot;</span> )</span>
<span id="cb210-2"><a href="5-1-spurious-association.html#cb210-2" aria-hidden="true" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>(dag5<span class="fl">.1</span>) <span class="ot">&lt;-</span> <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">A=</span><span class="dv">0</span>,<span class="at">D=</span><span class="dv">1</span>,<span class="at">M=</span><span class="dv">2</span>) , <span class="at">y=</span><span class="fu">c</span>(<span class="at">A=</span><span class="dv">0</span>,<span class="at">D=</span><span class="dv">1</span>,<span class="at">M=</span><span class="dv">0</span>) )</span>
<span id="cb210-3"><a href="5-1-spurious-association.html#cb210-3" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">drawdag</span>( dag5<span class="fl">.1</span> )</span></code></pre></div>
<p><img src="05_the_many_variables_and_the_spurious_waffles_files/figure-html/5.7-1.svg" width="672" /></p>
<p><strong><em>5.1.2 Testable implications</em></strong></p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="5-1-spurious-association.html#cb211-1" aria-hidden="true" tabindex="-1"></a>DMA_dag2 <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag{ D &lt;- A -&gt; M }&#39;</span>)</span>
<span id="cb211-2"><a href="5-1-spurious-association.html#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="fu">impliedConditionalIndependencies</span>( DMA_dag2 )</span></code></pre></div>
<pre><code>## D _||_ M | A</code></pre>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="5-1-spurious-association.html#cb213-1" aria-hidden="true" tabindex="-1"></a>DMA_dag1 <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag{ D &lt;- A -&gt; M -&gt; D }&#39;</span>)</span>
<span id="cb213-2"><a href="5-1-spurious-association.html#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="fu">impliedConditionalIndependencies</span>( DMA_dag1 )</span></code></pre></div>
<p>No conditional independencies, so no output.</p>
<p><strong><em>5.1.3 Multiple regression notation</em></strong></p>
<div class="figure">
<img src="slides/L05/18.png" alt="Linear means additive, so the model makes a plane. You keep adding them together. There are four parameters. $\alpha$, two slopes $\beta_1$ and $\beta_2$, and the standard deviation $\sigma$." width="80%" />
<p class="caption marginnote shownote">
Linear means additive, so the model makes a plane. You keep adding them together. There are four parameters. <span class="math inline">\(\alpha\)</span>, two slopes <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, and the standard deviation <span class="math inline">\(\sigma\)</span>.
</p>
</div>
<p><strong><em>5.1.4 Approximating the posterior</em></strong></p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="5-1-spurious-association.html#cb214-1" aria-hidden="true" tabindex="-1"></a>m5<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb214-2"><a href="5-1-spurious-association.html#cb214-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb214-3"><a href="5-1-spurious-association.html#cb214-3" aria-hidden="true" tabindex="-1"></a>    D <span class="sc">~</span> <span class="fu">dnorm</span>( mu , sigma ) ,</span>
<span id="cb214-4"><a href="5-1-spurious-association.html#cb214-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> bM<span class="sc">*</span>M <span class="sc">+</span> bA<span class="sc">*</span>A ,</span>
<span id="cb214-5"><a href="5-1-spurious-association.html#cb214-5" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.2</span> ) ,</span>
<span id="cb214-6"><a href="5-1-spurious-association.html#cb214-6" aria-hidden="true" tabindex="-1"></a>    bM <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.5</span> ) ,</span>
<span id="cb214-7"><a href="5-1-spurious-association.html#cb214-7" aria-hidden="true" tabindex="-1"></a>    bA <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.5</span> ) ,</span>
<span id="cb214-8"><a href="5-1-spurious-association.html#cb214-8" aria-hidden="true" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dexp</span>( <span class="dv">1</span> )</span>
<span id="cb214-9"><a href="5-1-spurious-association.html#cb214-9" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data =</span> d )</span>
<span id="cb214-10"><a href="5-1-spurious-association.html#cb214-10" aria-hidden="true" tabindex="-1"></a><span class="fu">precis</span>( m5<span class="fl">.3</span> )</span></code></pre></div>
<pre><code>##                mean         sd       5.5%      94.5%
## a     -2.828642e-05 0.09707123 -0.1551669  0.1551103
## bM    -6.553086e-02 0.15076312 -0.3064794  0.1754177
## bA    -6.136370e-01 0.15097351 -0.8549218 -0.3723521
## sigma  7.850672e-01 0.07783076  0.6606786  0.9094558</code></pre>
<p><img src="slides/L05/19.png" width="80%" />
Here’s the quap code, and we get a table of coefficients. Look for the mean, and as I promised, <span class="math inline">\(\alpha\)</span> is 0. `<code>bM</code> is about twice the size of the posterior value itself. No consistent relationship. Age of marriage however, is -.6, but now the posterior mass is entirely below 0. What’s the lesson here? There’s probably no causal relationship between marriage rate and divorce, and that’s because it was confounded by age of marriage.</p>
<div class="figure">
<img src="slides/L05/20.png" alt="This shows all three models. Bottom is age of marriage only. The one with marriage rates in the middle. Then marriage rate and and age of marriage in the bottom." width="80%" />
<p class="caption marginnote shownote">
This shows all three models. Bottom is age of marriage only. The one with marriage rates in the middle. Then marriage rate and and age of marriage in the bottom.
</p>
</div>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="5-1-spurious-association.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Faulty code</span></span>
<span id="cb216-2"><a href="5-1-spurious-association.html#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="co">#plot( rethinking::coeftab(m5.1,m5.2,m5.3), par=c(&quot;bA&quot;,&quot;bM&quot;) )</span></span></code></pre></div>
<div class="figure">
<img src="slides/L05/21.png" alt="This is the graph. Once you know the median age of marriage, you get little extra information in marriage. But when you ad A, it does give you information. If you just wanted to make a prediction, M is useful, but if you wanted to change D, you need to change other things like A." width="80%" />
<p class="caption marginnote shownote">
This is the graph. Once you know the median age of marriage, you get little extra information in marriage. But when you ad A, it does give you information. If you just wanted to make a prediction, M is useful, but if you wanted to change D, you need to change other things like A.
</p>
</div>
<p>You have to be clear about whether you’re interesting in predicting things, or understanding the true nature of things.</p>
<div class="figure">
<img src="slides/L05/22.png" alt="How do we visualise models like this? Lots of ways. Usually the most useful way to visualise depends on the model. You want to think about what you're trying to communicate. The first are predictor residual plots, not that you need to do them, but good for understanding how these linear regressions works." width="80%" />
<p class="caption marginnote shownote">
How do we visualise models like this? Lots of ways. Usually the most useful way to visualise depends on the model. You want to think about what you’re trying to communicate. The first are predictor residual plots, not that you need to do them, but good for understanding how these linear regressions works.
</p>
</div>
<div class="figure">
<img src="slides/L05/23.png" alt="Purpose is to show how the association looks, having controlled for the other predictors. We want to calculate the intermediate states. Great for intuition but terrible for analysis. Never any statistical justification for running regression over residuals. Why? Gives you the wrong answer, because it gives you bias estimates. What to do instead? Multiple regressions." width="80%" />
<p class="caption marginnote shownote">
Purpose is to show how the association looks, having controlled for the other predictors. We want to calculate the intermediate states. Great for intuition but terrible for analysis. Never any statistical justification for running regression over residuals. Why? Gives you the wrong answer, because it gives you bias estimates. What to do instead? Multiple regressions.
</p>
</div>
<p>Recipe:</p>
<ol style="list-style-type: decimal">
<li>Regress a predictor, and find the extra variance left over, and look at the pattern of the relationship between the residuals and the outcome.</li>
</ol>
<div class="figure">
<img src="slides/L05/24.png" alt="Here's our first residual plot" width="80%" />
<p class="caption marginnote shownote">
Here’s our first residual plot
</p>
</div>
<div class="figure">
<img src="slides/L05/25.png" alt="What are we looking at here? Marriage rate, standardised. THe distance from the regression line - the expected value of M conditional on A, is the residual. THe unexplained bit from the model. Highlighted some states with high residuals. Now we'll take the absolute distances for each point." width="80%" />
<p class="caption marginnote shownote">
What are we looking at here? Marriage rate, standardised. THe distance from the regression line - the expected value of M conditional on A, is the residual. THe unexplained bit from the model. Highlighted some states with high residuals. Now we’ll take the absolute distances for each point.
</p>
</div>
<div class="figure">
<img src="slides/L05/26.png" alt="Now take those residuals, and look at the correlation between the residuals of M and D. And you can see nothing. Shows you what the model is doing inside. If you do the multiple regresssion all at once, it handles all of that. If you do it this way you don't. But really good for intuition. Point of Maine with a really high divorce rate. " width="80%" />
<p class="caption marginnote shownote">
Now take those residuals, and look at the correlation between the residuals of M and D. And you can see nothing. Shows you what the model is doing inside. If you do the multiple regresssion all at once, it handles all of that. If you do it this way you don’t. But really good for intuition. Point of Maine with a really high divorce rate.
</p>
</div>
<div class="figure">
<img src="slides/L05/27.png" alt="Still no explanation about why ME is so high. Now you can pivot A on M. " width="80%" />
<p class="caption marginnote shownote">
Still no explanation about why ME is so high. Now you can pivot A on M.
</p>
</div>
<div class="figure">
<img src="slides/L05/28.png" alt="Then put those on the plot on the bottom. So now that you know M, there's considerable information in also knowing A. But the reverse is not true. " width="80%" />
<p class="caption marginnote shownote">
Then put those on the plot on the bottom. So now that you know M, there’s considerable information in also knowing A. But the reverse is not true.
</p>
</div>
<div class="figure">
<img src="slides/L05/29.png" alt="This is one of the things we mean. In observational studies like this one, there's no ethical intervention we can make. But we want to make causal information. Linear regression allows you to do that, but only if you have an idea of the causal relationship between the variables. To interpret, you need a causal framework. We'll have an example latter when controlling can *create* a confound." width="80%" />
<p class="caption marginnote shownote">
This is one of the things we mean. In observational studies like this one, there’s no ethical intervention we can make. But we want to make causal information. Linear regression allows you to do that, but only if you have an idea of the causal relationship between the variables. To interpret, you need a causal framework. We’ll have an example latter when controlling can <em>create</em> a confound.
</p>
</div>
<pre class="marginfigure"><code>These models are not magic. You shouldn&#39;t get cocky. This is the kind of study where you use average data. Quite a pathological dataset.</code></pre>
<div class="figure">
<img src="slides/L05/30.png" alt="Cases where you hold other predictor variables constant. All the code for generating them is in the text. In the real world we can't do that. If we're right in our DAG here, and we manipulate A, you'll also manipulate M as well. " width="80%" />
<p class="caption marginnote shownote">
Cases where you hold other predictor variables constant. All the code for generating them is in the text. In the real world we can’t do that. If we’re right in our DAG here, and we manipulate A, you’ll also manipulate M as well.
</p>
</div>
<p><img src="slides/L05/31.png" width="80%" /></p>
<p>Goals:
1. Figure out whether the approximation of the posterior works. Compare the predictions with the raw data. If they’re different, they fail.
2. It can inspire you to look at the cases that don’t fit well, and figure out what you need to make better causal inferences.</p>
<div class="figure">
<img src="slides/L05/32.png" alt="Diagonal is unity - perfect prediction. But there are states where they're making bad predictions, like ID. Has a very low D. That's why there's a mismatch. It's getting it really wrong. Why? The Mormons. They have a very low divorce rate. UT as well." width="80%" />
<p class="caption marginnote shownote">
Diagonal is unity - perfect prediction. But there are states where they’re making bad predictions, like ID. Has a very low D. That’s why there’s a mismatch. It’s getting it really wrong. Why? The Mormons. They have a very low divorce rate. UT as well.
</p>
</div>
<div class="figure">
<img src="slides/L05/33.png" alt="Another good thing regression can do is reveal spurious correlations. When there are two predictors that both influence the outcome in different directions, you can get the total causal effect between the two. This tends to arise where you have two predictors, and they act in different directions, and cancel each other out, so if you don't model both of them individually, it looks like they have no effect. " width="80%" />
<p class="caption marginnote shownote">
Another good thing regression can do is reveal spurious correlations. When there are two predictors that both influence the outcome in different directions, you can get the total causal effect between the two. This tends to arise where you have two predictors, and they act in different directions, and cancel each other out, so if you don’t model both of them individually, it looks like they have no effect.
</p>
</div>
<pre class="marginfigure"><code>Noise can also cause you not to see a relationship.</code></pre>
<div class="figure">
<img src="slides/L05/34.png" alt="Relationship between milk energy and how brainy they are? Intersted in things that are interested in what makes us unique. Primates are mammals, and some mammals have very highly energetic milk, like seals basically ooze butter. Primates in contrast carry their offspring on them. As a consequence, the energy density is lower. Human milk is not energetically rich. 75% of our brain mass is neocortex. Then the brainiest primate is *Cebus*. Can we see a signal of selection on milk energy from braininess?" width="80%" />
<p class="caption marginnote shownote">
Relationship between milk energy and how brainy they are? Intersted in things that are interested in what makes us unique. Primates are mammals, and some mammals have very highly energetic milk, like seals basically ooze butter. Primates in contrast carry their offspring on them. As a consequence, the energy density is lower. Human milk is not energetically rich. 75% of our brain mass is neocortex. Then the brainiest primate is <em>Cebus</em>. Can we see a signal of selection on milk energy from braininess?
</p>
</div>
<div class="figure">
<img src="slides/L05/35.png" alt="Sample of primate species. Pairs plot. Particularly strong correlation between the magnitude of body mass (log(mass)) is strongly correlated with neocortex. No particular strong relationship between `log(mass)` and `kcal.per.g`." width="80%" />
<p class="caption marginnote shownote">
Sample of primate species. Pairs plot. Particularly strong correlation between the magnitude of body mass (log(mass)) is strongly correlated with neocortex. No particular strong relationship between <code>log(mass)</code> and <code>kcal.per.g</code>.
</p>
</div>
<div class="figure">
<img src="slides/L05/36.png" alt="We need to do some prior predictive simulation. Left is not a good prior. All we need to do to get the regression lines to live in the outcome space, we can contract $lpha$ - should be about 0, and the slope should be about 0.5 to be tighter. If you standardise the predictor and outcome, Normal(0, 0.5) should keep you in the outcome space." width="80%" />
<p class="caption marginnote shownote">
We need to do some prior predictive simulation. Left is not a good prior. All we need to do to get the regression lines to live in the outcome space, we can contract <span class="math inline">\(lpha\)</span> - should be about 0, and the slope should be about 0.5 to be tighter. If you standardise the predictor and outcome, Normal(0, 0.5) should keep you in the outcome space.
</p>
</div>
<div class="figure">
<img src="slides/L05/37.png" alt="There's a slight relationship between brain and milk energy, and a slightly negative relationship with body mass. Look what happens when you include both in a model." width="80%" />
<p class="caption marginnote shownote">
There’s a slight relationship between brain and milk energy, and a slightly negative relationship with body mass. Look what happens when you include both in a model.
</p>
</div>
<div class="figure">
<img src="slides/L05/38.png" alt="Now very strong relationship between both." width="80%" />
<p class="caption marginnote shownote">
Now very strong relationship between both.
</p>
</div>
<div class="figure">
<img src="slides/L05/39.png" alt="You can see strong relationships, but only when they're both present in the model. This is the masking effect. One is positively related to the outcome, the other is negatively related to the outcome, and they're correlated with one another. Bigger primates need bigger brains, bigger brains need more energy, bigger bodies need less because they're are longer developmental times. THey're antagonistic effects, but correlated in same species. This sort of effect can happen a lot. " width="80%" />
<p class="caption marginnote shownote">
You can see strong relationships, but only when they’re both present in the model. This is the masking effect. One is positively related to the outcome, the other is negatively related to the outcome, and they’re correlated with one another. Bigger primates need bigger brains, bigger brains need more energy, bigger bodies need less because they’re are longer developmental times. THey’re antagonistic effects, but correlated in same species. This sort of effect can happen a lot.
</p>
</div>
<div class="figure">
<img src="slides/L05/40.png" alt="Fake data that causes this relationship. `U` is unobserved, some life history variable. Then the causal influence on each of those. This relationship is sufficient to create what we saw." width="80%" />
<p class="caption marginnote shownote">
Fake data that causes this relationship. <code>U</code> is unobserved, some life history variable. Then the causal influence on each of those. This relationship is sufficient to create what we saw.
</p>
</div>
<div class="figure">
<img src="slides/L05/41.png" alt="Often we have data that represents discrete categories. Want to include those, but they're not continuous. Useful variables because the mean varies, but can't add them as they are" width="80%" />
<p class="caption marginnote shownote">
Often we have data that represents discrete categories. Want to include those, but they’re not continuous. Useful variables because the mean varies, but can’t add them as they are
</p>
</div>
<pre class="marginfigure"><code>First is to create a dummy, and the next is almost always superior, and that&#39;s the index. </code></pre>
<div class="figure">
<img src="slides/L05/42.png" alt="Take a categorical variable and convert them to indicator variables. They stand in for something. e.g. Kalahari height data, `0` means not `male`. " width="80%" />
<p class="caption marginnote shownote">
Take a categorical variable and convert them to indicator variables. They stand in for something. e.g. Kalahari height data, <code>0</code> means not <code>male</code>.
</p>
</div>
<div class="figure">
<img src="slides/L05/43.png" alt="Height varies by sex, so you could include it in the model. The linear model looks like a continuous predictor, because you've coded it as 0 and 1, if effectively turns the parameter off and on, and adjusts the mean. Effectively makes two intercepts, one male, one female. `alpha` is the intercept for females, and `alpha + betaM` is the intercept for males. " width="80%" />
<p class="caption marginnote shownote">
Height varies by sex, so you could include it in the model. The linear model looks like a continuous predictor, because you’ve coded it as 0 and 1, if effectively turns the parameter off and on, and adjusts the mean. Effectively makes two intercepts, one male, one female. <code>alpha</code> is the intercept for females, and <code>alpha + betaM</code> is the intercept for males.
</p>
</div>
<div class="figure">
<img src="slides/L05/44.png" alt="Have to pick same number of priors for number of categories. Consequence is you end up assuming that one of the categories is less certain than all the others." width="80%" />
<p class="caption marginnote shownote">
Have to pick same number of priors for number of categories. Consequence is you end up assuming that one of the categories is less certain than all the others.
</p>
</div>
<div class="figure">
<img src="slides/L05/45.png" alt="This is the better option." width="80%" />
<p class="caption marginnote shownote">
This is the better option.
</p>
</div>
<hr />
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="5-1-spurious-association.html#cb220-1" aria-hidden="true" tabindex="-1"></a>slides_dir <span class="ot">=</span> here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;docs/slides/L06&quot;</span>)</span></code></pre></div>
<p><img src="slides/L06/01.png" width="80%" /></p>
<div class="figure">
<img src="slides/L06/02.png" alt="Pick up where we left off. How to include un-ordered categorical data in a regression? A lot of downsides for using dummy variables. Index variables have a lot of advantages, including that you can assign the same priors to each of the categories. Another reason is that when you get more and more categories, you don't have to change anything other than include more numbers in the index. Grows really nicely, and is the foundation of multi-level models." width="80%" />
<p class="caption marginnote shownote">
Pick up where we left off. How to include un-ordered categorical data in a regression? A lot of downsides for using dummy variables. Index variables have a lot of advantages, including that you can assign the same priors to each of the categories. Another reason is that when you get more and more categories, you don’t have to change anything other than include more numbers in the index. Grows really nicely, and is the foundation of multi-level models.
</p>
</div>
<div class="figure">
<img src="slides/L06/03.png" alt="Can code this in `quap`. The bracket notation means `a` for each `sex`. Now you see you get an alpha for each sex. The awkwardness is you need to make inferences." width="80%" />
<p class="caption marginnote shownote">
Can code this in <code>quap</code>. The bracket notation means <code>a</code> for each <code>sex</code>. Now you see you get an alpha for each sex. The awkwardness is you need to make inferences.
</p>
</div>
<div class="figure">
<img src="slides/L06/04.png" alt="If you want the posterior distribution for the sexes, you extract samples from the posterior. Then compute the difference by subtracting the difference from each sample. Still it in a column called `diff_fm`. Posterior mean is -8. All the comparisons are already in the posterior, you just have to extract samples to compute them. We'll be using this kind of coding in future examples." width="80%" />
<p class="caption marginnote shownote">
If you want the posterior distribution for the sexes, you extract samples from the posterior. Then compute the difference by subtracting the difference from each sample. Still it in a column called <code>diff_fm</code>. Posterior mean is -8. All the comparisons are already in the posterior, you just have to extract samples to compute them. We’ll be using this kind of coding in future examples.
</p>
</div>

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="5-the-many-variables-the-spurious-waffles.html"><button class="btn btn-default">Previous</button></a>
<a href="6-the-haunted-dag-the-causal-terror.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
