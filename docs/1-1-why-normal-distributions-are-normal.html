<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.1 Why normal distributions are normal | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-05-12" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="1.1 Why normal distributions are normal | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>1.1 Why normal distributions are normal | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-geocentric-models.html#geocentric-models"><span class="toc-section-number">1</span> Geocentric Models</a>
<ul>
<li><a href="1-1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">1.1</span> Why normal distributions are normal</a></li>
<li><a href="1-2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">1.2</span> A language for describing models</a></li>
<li><a href="1-3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">1.3</span> Gaussian model of height</a></li>
<li><a href="1-4-linear-prediction.html#linear-prediction"><span class="toc-section-number">1.4</span> Linear prediction</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="why-normal-distributions-are-normal" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Why normal distributions are normal</h2>
<div class="figure"><span id="fig:unnamed-chunk-9"></span>
<p class="caption marginnote shownote">
Figure 1.6: These appear all throughout nature. Why are they so normal? They arise from all over the place.
</p>
<img src="slides/L03/07.png" alt="These appear all throughout nature. Why are they so normal? They arise from all over the place." width="80%"  />
</div>
<p><strong><em>4.1.1 Normal by addition</em></strong></p>
<p>One of the things that are nice about them is that they are additive. So easy to work with.
Second is that they’re very common.</p>
<div class="figure"><span id="fig:unnamed-chunk-10"></span>
<p class="caption marginnote shownote">
Figure 1.7: Imagine a football pitch. We all line up on the midfield line. Take a coin out of your pocket and flip it. One step left for heads, right for tails. Do it a few hundred times.
</p>
<img src="slides/L03/08.png" alt="Imagine a football pitch. We all line up on the midfield line. Take a coin out of your pocket and flip it. One step left for heads, right for tails. Do it a few hundred times." width="80%"  />
</div>
<p><img src="slides/L03/09.png" width="80%"  /></p>
<p><img src="slides/L03/10.png" width="80%"  /></p>
<p><img src="slides/L03/11.png" width="80%"  /></p>
<div class="figure"><span id="fig:unnamed-chunk-14"></span>
<p class="caption marginnote shownote">
Figure 1.8: The frequency distribution will be Gaussian.
</p>
<img src="slides/L03/12.png" alt="The frequency distribution will be Gaussian." width="80%"  />
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="04_geocentric_models_files/figure-html/4.1-1.png" alt=" " width="672"  />
<!--
<p class="caption marginnote">--> <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="1-1-why-normal-distributions-are-normal.html#cb3-1" aria-hidden="true" tabindex="-1"></a>pos <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sum</span>(<span class="fu">runif</span>(<span class="dv">16</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb3-2"><a href="1-1-why-normal-distributions-are-normal.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(pos))</span></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-15"></span>
<p class="caption marginnote shownote">
Figure 1.9: This is a simulation of the soccer field experiment. After four flips (steps). Black follows one particular student. A pattern forms in the aggregation. This isn’t very Gaussian yet.
</p>
<img src="slides/L03/13.png" alt="This is a simulation of the soccer field experiment. After four flips (steps). Black follows one particular student. A pattern forms in the aggregation. This isn't very Gaussian yet. " width="80%"  />
</div>
<div class="figure"><span id="fig:unnamed-chunk-16"></span>
<p class="caption marginnote shownote">
Figure 1.10: After 8 it’s pretty Gaussian.
</p>
<img src="slides/L03/14.png" alt="After 8 it's pretty Gaussian." width="80%"  />
</div>
<div class="figure"><span id="fig:unnamed-chunk-17"></span>
<p class="caption marginnote shownote">
Figure 1.11: And after 16 it’s very Gaussian. It’ll get wider and wider over time. Why does this happen? Lot’s of mathematical theorems. But the intuition is that each coin flip is a fluctuation. And in the long run, fluctuations tend to cancel. If you get a string of lefts, eventually you’ll get a string of rights, so the average student will end up near the middle. A very large number of them exactly cancel each other. There are more paths that will give you 0 than any other path. Then there are a few less that give you +1 or -1. And so forth.
</p>
<img src="slides/L03/15.png" alt="And after 16 it's very Gaussian. It'll get wider and wider over time. Why does this happen? Lot's of mathematical theorems. But the intuition is that each coin flip is a fluctuation. And in the long run, fluctuations tend to cancel. If you get a string of lefts, eventually you'll get a string of rights, so the average student will end up near the middle. A very large number of them exactly cancel each other. There are more paths that will give you 0 than any other path. Then there are a few less that give you +1 or -1. And so forth." width="80%"  />
</div>
<div class="figure"><span id="fig:unnamed-chunk-18"></span>
<p class="caption marginnote shownote">
Figure 1.12: That’s why a bunch of natural systems are normally distributed. We don’t need to know anything except that they cancel out. A lot of common statistics follow this kind of process. What you’re left with are particular shapes, called <em>maximum entropy distributions</em>. For the Gaussian, <strong>addition</strong> is our friend. One of the things about it is that products of deviations are actually addition. So lots of multiplicative interactions also produce Gaussian distributions. You can measure things on logarithmic scales.
</p>
<img src="slides/L03/16.png" alt="That's why a bunch of natural systems are normally distributed. We don't need to know anything except that they cancel out. A lot of common statistics follow this kind of process. What you're left with are particular shapes, called *maximum entropy distributions*. For the Gaussian, **addition** is our friend. One of the things about it is that products of deviations are actually addition. So lots of multiplicative interactions also produce Gaussian distributions. You can measure things on logarithmic scales." width="80%"  />
</div>
<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<p class="caption marginnote shownote">
Figure 1.13: This is the ontological perspective on distributions. When fluctuations tend to dampen one another, you end up with a symmetric curve. What neat and also frustrating is that you lose a lot of information about the generative processes. When you see heights are normally distributed, you learn basically nothing about it. This is cool because all that’s preserved from the underlying process is the mean and the variance. What’s terrible is that you can’t figure out the process from the distribution. All the maximum entropy distributions have the same feature. Power laws arise through lots of processes, and it tells you nothing other than it has high variance. The other perspective is epistemological. If you’re building a model and you want to be as conservative as possible, you should use the Gaussian distribution. Because any other distribution will be narrower. So it’s a very good assumption to use when you don’t have additional information. The Gaussian is the one where all you’re willing to say is there’s a mean and a variance, you should use the Gaussian. It assumes the least.
</p>
<img src="slides/L03/18.png" alt="This is the ontological perspective on distributions. When fluctuations tend to dampen one another, you end up with a symmetric curve. What neat and also frustrating is that you lose a lot of information about the generative processes. When you see heights are normally distributed, you learn basically nothing about it. This is cool because all that's preserved from the underlying process is the mean and the variance. What's terrible is that you can't figure out the process from the distribution. All the maximum entropy distributions have the same feature. Power laws arise through lots of processes, and it tells you nothing other than it has high variance. The other perspective is epistemological. If you're building a model and you want to be as conservative as possible, you should use the Gaussian distribution. Because any other distribution will be narrower. So it's a very good assumption to use when you don't have additional information. The Gaussian is the one where all you're willing to say is there's a mean and a variance, you should use the Gaussian. It assumes the least." width="80%"  />
</div>
<p><strong><em>4.1.2 Normal by multiplication</em></strong></p>
<p>This code just samples 12 random numbers between 1.0 and 1.1, each representing a proportional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="1-1-why-normal-distributions-are-normal.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, .<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 1.907699</code></pre>
<p>Now what distribution do you think these random products will take? Let’s generate 10,000 of them and see:</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="04_geocentric_models_files/figure-html/4.3-1.png" alt=" " width="672"  />
<!--
<p class="caption marginnote">--> <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="1-1-why-normal-distributions-are-normal.html#cb6-1" aria-hidden="true" tabindex="-1"></a>growth <span class="ot">=</span> <span class="fu">replicate</span>(<span class="fl">1e4</span>, <span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, <span class="fl">0.1</span>)))</span>
<span id="cb6-2"><a href="1-1-why-normal-distributions-are-normal.html#cb6-2" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(growth, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p>Multiplying small numbers if approximately the same as addition.</p>
<p>The smaller the effect of each locus, the better this additive approximation will be. In this way, small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.</p>
<p>Verify by comparing:</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="04_geocentric_models_files/figure-html/4.4-1.png" alt=" " width="672"  />
<!--
<p class="caption marginnote">--> <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="1-1-why-normal-distributions-are-normal.html#cb7-1" aria-hidden="true" tabindex="-1"></a>big <span class="ot">=</span> <span class="fu">replicate</span>(<span class="fl">1e4</span>, <span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, .<span class="dv">5</span>)))</span>
<span id="cb7-2"><a href="1-1-why-normal-distributions-are-normal.html#cb7-2" aria-hidden="true" tabindex="-1"></a>small <span class="ot">=</span> <span class="fu">replicate</span>(<span class="fl">1e4</span>, <span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, <span class="fl">0.01</span>)))</span>
<span id="cb7-3"><a href="1-1-why-normal-distributions-are-normal.html#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="1-1-why-normal-distributions-are-normal.html#cb7-4" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(big, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="04_geocentric_models_files/figure-html/4.4-2.png" alt=" " width="672"  />
<!--
<p class="caption marginnote">--> <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="1-1-why-normal-distributions-are-normal.html#cb8-1" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(small, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p><strong><em>4.1.3 Normal by log-multiplication</em></strong></p>
<p>Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. e.g.:</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="04_geocentric_models_files/figure-html/4.5-1.png" alt=" " width="672"  />
<!--
<p class="caption marginnote">--> <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="1-1-why-normal-distributions-are-normal.html#cb9-1" aria-hidden="true" tabindex="-1"></a>log.big <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">log</span>(<span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, .<span class="dv">5</span>))))</span>
<span id="cb9-2"><a href="1-1-why-normal-distributions-are-normal.html#cb9-2" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(log.big, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p>Adding logs is equivalent to multiplying the original numbers.</p>
<p><strong><em>4.1.4 Using Gaussian distributions</em></strong></p>
<p><strong>Caution</strong>: Many natural (and unnatural) processes have much heavier tails - much higher probabilities of producing extreme events.</p>
</div>
<p style="text-align: center;">
<a href="1-geocentric-models.html"><button class="btn btn-default">Previous</button></a>
<a href="1-2-a-language-for-describing-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
