<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2021-04-23" />

<title>Chapter 11. God spiked the integers</title>

<script src="site_libs/header-attrs-2.5/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistical Rethinking</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="01_the_golem_of_prague.html">1. The Golem of Prague</a>
</li>
<li>
  <a href="02_small_worlds_and_large_worlds.html">2. Small Worlds and Large Worlds</a>
</li>
<li>
  <a href="03_sampling_from_the_imaginary.html">3. Sampling from the Imaginary</a>
</li>
<li>
  <a href="04_geocentric_models.html">4. Geocentric models</a>
</li>
<li>
  <a href="11_god_spiked_the_integers.html">11. God spiked the integers</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Chapter 11. God spiked the integers</h1>
<h4 class="date">2021-04-23</h4>

</div>


<div id="setup" class="section level1">
<h1>Setup</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;code/scripts/source.R&quot;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>slides_dir <span class="ot">=</span> here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;docs/slides/L12&quot;</span>)</span></code></pre></div>
<p><img src="slides/L12/L12_02_fireflies.png" width="800" /></p>
<p>Fireflies flash in sync. All that’s need to acheive this is to bring forward the internal clock every time they see a flash.</p>
<p><img src="slides/L12/L12_03_kirkwood_gaps.png" width="800" /></p>
<p><img src="slides/L12/L12_04_asteroids.png" width="800" /></p>
</div>
<div id="binomial-regression" class="section level1">
<h1>11.1 Binomial regression</h1>
<ul>
<li><strong>Generalized linear models</strong> (GLMs) are like early computers - the moving pieces inside interact to produce non-obvious predictions. But we can’t read the paramteres directly to understand the predictions.</li>
</ul>
<div id="logistic-regression-prosocial-chimpanzees" class="section level2">
<h2>11.1.1 Logistic regression: prosocial chimpanzees</h2>
<p>The data come from an experiment aimed at evaluating the procsocial tendencies of chimpanzees.</p>
<p>While both levers deliver food to the focal animal, only one of the levers delivers food to the other side of the table.</p>
<p>There are two experimental conditions: in the <em>partner</em> condition, antoher chimpanzee is seated at the opposite end of the table. Int eh control condition, the other side of the table is empty.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># trimmed data list</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;chimpanzees&quot;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> chimpanzees</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Add index variables</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>treatment <span class="ot">=</span> <span class="dv">1</span> <span class="sc">+</span> d<span class="sc">$</span>prosoc_left <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>d<span class="sc">$</span>condition</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify with crosstabs</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>stats<span class="sc">::</span><span class="fu">xtabs</span>( <span class="sc">~</span> treatment <span class="sc">+</span> prosoc_left <span class="sc">+</span> condition , d )</span></code></pre></div>
<pre><code>## , , condition = 0
## 
##          prosoc_left
## treatment   0   1
##         1 126   0
##         2   0 126
##         3   0   0
##         4   0   0
## 
## , , condition = 1
## 
##          prosoc_left
## treatment   0   1
##         1   0   0
##         2   0   0
##         3 126   0
##         4   0 126</code></pre>
<p>Let’s try with a flat prior:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>m11<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    pulled_left <span class="sc">~</span> <span class="fu">dbinom</span>( <span class="dv">1</span> , p ) ,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">logit</span>(p) <span class="ot">&lt;-</span> a ,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="dv">10</span> )</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data=</span>d )</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Now sample from the prior</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1999</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">extract.prior</span>(m11<span class="fl">.1</span>, <span class="at">n =</span> <span class="fl">1e4</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Now conver the parameter to the outcome scale using the Inverse-link function.</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">inv_logit</span>(prior<span class="sc">$</span>a)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="fu">dens</span>(p, <span class="at">adj =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<p><img src="11_god_spiked_the_integers_files/figure-html/unnamed-chunk-7-1.svg" width="672" /></p>
<p>Before it sees the data, the model thinks the chimps either never or always pull the left lever.</p>
<p>Now we need to determine a prior for the treatment effects, the <span class="math inline">\((\beta)\)</span> parameters. To emphasis the weirdness of flat priors, let’s see what Normal(0,10) looks like.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>m11<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    pulled_left <span class="sc">~</span> <span class="fu">dbinom</span>( <span class="dv">1</span> , p ) ,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">logit</span>(p) <span class="ot">&lt;-</span> a <span class="sc">+</span> b[treatment] ,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">1.5</span> ),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    b[treatment] <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="dv">10</span> )</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data=</span>d )</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1999</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">extract.prior</span>( m11<span class="fl">.2</span> , <span class="at">n=</span><span class="fl">1e4</span> )</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">sapply</span>( <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span> , <span class="cf">function</span>(k) <span class="fu">inv_logit</span>( prior<span class="sc">$</span>a <span class="sc">+</span> prior<span class="sc">$</span>b[,k] ) )</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot absolute prior difference between the first two treatments</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="fu">dens</span>( <span class="fu">abs</span>( p[,<span class="dv">1</span>] <span class="sc">-</span> p[,<span class="dv">2</span>] ) , <span class="at">adj=</span><span class="fl">0.1</span> )</span></code></pre></div>
<p><img src="11_god_spiked_the_integers_files/figure-html/unnamed-chunk-8-1.svg" width="672" /></p>
<p>This doesn’t make sense. Typical behavioural treatments have modest effects.</p>
<p>Average prior difference:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>m11<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    pulled_left <span class="sc">~</span> <span class="fu">dbinom</span>( <span class="dv">1</span> , p ) ,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">logit</span>(p) <span class="ot">&lt;-</span> a <span class="sc">+</span> b[treatment] ,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">1.5</span> ),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    b[treatment] <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.5</span> )</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data=</span>d )</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1999</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">extract.prior</span>( m11<span class="fl">.3</span> , <span class="at">n=</span><span class="fl">1e4</span> )</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">sapply</span>( <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span> , <span class="cf">function</span>(k) <span class="fu">inv_logit</span>( prior<span class="sc">$</span>a <span class="sc">+</span> prior<span class="sc">$</span>b[,k] ) )</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>( <span class="fu">abs</span>( p[,<span class="dv">1</span>] <span class="sc">-</span> p[,<span class="dv">2</span>] ) )</span></code></pre></div>
<pre><code>## [1] 0.09838663</code></pre>
<p>The average prior difference is about 10%. Extremely large differences are less plausible. However this is not a strong prior, so if the data contain evidence of large differences, they will shine through.</p>
<p>Let’s turn to Hamiltonian Mone Carlo to approximate the posterior. <code>quap</code> will doa fine job, but only because the priors are sufficiently regularizing.</p>
<p><img src="slides/L12/L12_05_logit.png" width="800" /></p>
<p><img src="slides/L12/L12_06.png" width="800" /></p>
<p>The right is the probability difference between treatments. We just subtract one from the other. The biggest difference is 1; smallest is 0. Absolute difference. The black density is the 0,10 Norm. Treatments are all identical or completely different. That’s not the correct thing to assume. We want a prior that says the differences aren’t that great. Something like SD = 0.5. So you need a tigther parameter scale on the prior.</p>
<p>Now we run the model…</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># trimmed data list</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>dat_list <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">pulled_left =</span> d<span class="sc">$</span>pulled_left,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">actor =</span> d<span class="sc">$</span>actor,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">treatment =</span> <span class="fu">as.integer</span>(d<span class="sc">$</span>treatment) )</span></code></pre></div>
<p>Now we start the Markov chain.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>m11<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">ulam</span>(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    pulled_left <span class="sc">~</span> <span class="fu">dbinom</span>( <span class="dv">1</span> , p ) ,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">logit</span>(p) <span class="ot">&lt;-</span> a[actor] <span class="sc">+</span> b[treatment] ,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    a[actor] <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">1.5</span> ),</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    b[treatment] <span class="sc">~</span> <span class="fu">dnorm</span>( <span class="dv">0</span> , <span class="fl">0.5</span> )</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  ) , <span class="at">data=</span>dat_list , <span class="at">chains=</span><span class="dv">4</span> , <span class="at">log_lik=</span><span class="cn">TRUE</span> )</span></code></pre></div>
<pre><code>## This is cmdstanr version 0.4.0.9000</code></pre>
<pre><code>## - Online documentation and vignettes at mc-stan.org/cmdstanr</code></pre>
<pre><code>## - CmdStan path set to: /Users/brettell/.cmdstanr/cmdstan-2.26.1</code></pre>
<pre><code>## - Use set_cmdstan_path() to change the path</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">precis</span>( m11<span class="fl">.4</span> , <span class="at">depth=</span><span class="dv">2</span> )</span></code></pre></div>
<p><img src="slides/L12/L12_07_chimps.png" width="800" /></p>
<p>We ran four chains here and here’s the summary. 7 chimp parameters and 4 treatment parameters. Each posterior mean is on the logit scale for handedness preference. Above 0 = left lever pulled more. Below is pulled left.</p>
<p><img src="slides/L12/L12_08.png" width="800" /></p>
<p>We extract the samples and transform them to the porbability scale using inverse logit. Them dump into precis and plot it.</p>
<p>Chimp 2 knows what chimp 2 wants.</p>
<p>Handedness is not technically a confound, but it makes measurement harder.</p>
<p>Now look at the treatment effects. <img src="slides/L12/L12_09.png" width="800" /></p>
<p>Even reading parameters in something simple is hard. You need them to choose the prosocial option more when there’s a partner on the other end. How do you figure that out? Let’s look at the two treatments with L on the front. Now we expect higher estimates. But now there’s not a diffference between when there’s a partner or not.</p>
<p>Now for 1 and 3 when the food is on the other side, there’s not a big effect. They’re not as big as the handedness effects.</p>
<p>It’s much easier to plot it on the outcome scale.</p>
<blockquote>
<p>If you want to understand the behaviour of a model, you should make it behave on the outcome scale.</p>
</blockquote>
<p><img src="slides/L12/L12_10.png" width="800" /></p>
<p>Each group is an actor, and they’re anchored to a single point. Open points = no partner present. Then we have the two groups connected by lines depending on whether the prosocial option was on the left or the right.</p>
<p>Forget actor 2. For the others, you can see that there’s one group that consistently deviates, and that’s that they pull the left lever more when there’s more food on that side on the table. But the lines are pretty horizontal. If they tilted more there’d be an interaction, but we don’t see that.</p>
<p>After you’ve trained the model on the data, it thinks there are more pulls left when prosocial is on the left. Adding a partner if anything reduced the tendency to pull to the left.</p>
<p>Look at actor 2’s prediction. It’s not absolutely sure that it would always pull left.</p>
<p>Very distinct to 5-year-old humans. The prosocial option changes everything.</p>
<p><img src="slides/L12/L12_11.png" width="800" /></p>
<p>If you want to do model comparison between these <code>ulam</code> models, you have to add the <code>log_lik</code> option in the <code>ulam</code>s. This computes the log probabilty of each observation.</p>
<p><img src="slides/L12/L12_12.png" width="800" /></p>
<p>We can compare this model’s accuracy to the previous one to confirm that the interaction is not doing any predictive work.</p>
<p><img src="slides/L12/L12_13.png" width="800" /></p>
<p>One of the tensions in interpretation is you can talk about effect sizes in two important ways. I call these the <em>relative</em> and <em>absolute</em> scales. When you’re looking at differences between parameters on the log odds scale, it’s relative. But if you wnat to predict the rate of the event in the real world, it’s absolute. And the base rate (how often things happen) matters. The relative effect can seem really big even when the absolute difference is small.</p>
<p>0.9 means a 10% reduction in odds on the relative scale. The risk with this is that unimportant things can seem really impnortant on the relative scale.</p>
<p><img src="slides/L12/L12_14.png" width="800" /></p>
<p>Shark and penguin parable. If you tally up mortality from different animals, sharks don’t kill many. That’s the base rate effect. The exposure to deer is large, because humans are on the land. But what if you’re a penguin? Then the absolute effect is large.</p>
<p><img src="slides/L12/L12_15.png" width="800" /></p>
<p>Relative effects are useful, and you see them (mis)used in epidemiological studies often, which can make rare things seem significant because of large relative effects. e.g. lung cancer - it’s still rare, even for smokers. But you really need them to do causal inference, because you have to transport results to contexts where there are difference base rates.</p>
<p><img src="slides/L12/L12_16.png" width="800" /></p>
<p>Other than Australians, we don’t worry about sharks, but we do worry about other things.</p>
<p>Increased rates of blood clots for women on birth control by 200%, so a lot of British women stopped taking birth control, even though the risk was still small and they have a much higher risk of death from giving birth.</p>
<p>Wearing your helmet in the car is a good idea.</p>
<p><img src="slides/L12/L12_17.png" width="800" /></p>
<p>Binomial regression comes in two major flavours: 1. Logistic regression. The outcomes are decomposed into 0,1 (Bernouli) trials. Mathematically it’s the same as.. 2. Aggregated binomial.</p>
<p>Same kind of model, just differ in how the data is coded. Very interesting because a high profile publication came out if this, which illustrates some important statistical principles.</p>
<p><img src="slides/L12/L12_18.png" width="800" /></p>
<p>Each application is a coin flip, with the result as their admission. 6 anonymised departments. Gender column. Admit column is total number admitted. Reject is the complement. Last column is the total. You could disaggregate this table to a row of each column.</p>
<p><img src="slides/L12/L12_19.png" width="800" /></p>
<p>Male is 1, female is 2. Now there’s a variable number of trials on each row. Males have a higher average rate of admission.</p>
<p><img src="slides/L12/L12_20.png" width="800" /></p>
<p>Think about the penguin. Calculate the difference in log odds (relative), then the difference in the absolute measure (absolute).</p>
<p>Between 0.12 and 0.15 on the probability scale, that’s a 10% advantage. That’s a huge effect.</p>
<p>Now do the posterior validation check.</p>
<p><img src="slides/L12/L12_21.png" width="800" /></p>
<p>Always looks ugly. Blue is raw data, black is model-based predictions. Open circle is the posteriro mean probability of admission for each case in the data. Posterior predictions go down from male to female, but at the department level, females are almost always admitted more than males.</p>
<p><img src="slides/L12/L12_22.png" width="800" /></p>
<p>There’s a backdoor path into gender, and one of them is department. Some get a small number of applicants and take half (physics). Social psychology has many applicants and only takes 10%. The applicants’ gender differ across departments.</p>
<p>We add another vector of parameters - department - and we estimate the offsets with deltas.</p>
<p>Regression models are list hostile oracles. They understand your questions extremely literally. Conditioning on the department’s overall admission rate, what’s the average difference between males and females.</p>
<p><img src="slides/L12/L12_24.png" width="800" /></p>
<p>Now we get some estimates. Just notice that some deltas are high, meaning that the majority are accepted. Department 6 is social psychology.</p>
<p><img src="slides/L12/L12_25.png" width="800" /></p>
<p>How you intepret this reversal depends on the model. Simpson’s paradox.</p>
<p><img src="slides/L12/26.png" width="1666" /></p>
<p><img src="slides/L12/27.png" width="1666" /></p>
<p>The systems is discriminatory, but the application assessors are not.</p>
<p><img src="slides/L12/28.png" width="1666" /></p>
<p>Homework problem using a different dataset.</p>
<p><img src="slides/L12/29.png" width="1666" /></p>
<p>These are like Binomials but the number of trials are unknown and very large. But the probability that it happens is very small. In that case you can describe it with one number: <span class="math inline">\((\lambda)\)</span></p>
<p>Nature is full of Poisson distributions. <img src="slides/L12/30.png" width="1666" /></p>
<p><img src="slides/L12/31.png" width="1666" /></p>
<p>Example dataset from cultural and technological evolution research. Research in Fiji. Interested in social evolution. Paper where she finds toolkit complexity. Underlying theoretical model that predicts that cultural complexity is proportional to the log population size. Outcome variable is total number of unique tool types.</p>
<p>Another variable is the contact rate, interacting with bigger societies.</p>
<p><img src="slides/L12/32.png" width="1666" /></p>
<p>Total tools at the top. Link is a log link which ensures a parameter is positive. Here it’s an expected count. If you exponentiate any number you get a real number.</p>
<p>Contact rates and an average on the log scale number of tools. Then a change (slope) for each contact rate times the log population size. Then priors to determine.</p>
<p><img src="slides/L12/33.png" width="1666" /></p>
<p>Goal is to ensure your parameter is always positive. Maps all real numbers to the (0,1) interval. Scaling is explosive.. literally exponential. Massive compression of small numbers into a tiny range. This is a really difficult scale to think about. So you should simulate to understand the implications of the link functions.</p>
<p><img src="slides/L12/34.png" width="1666" /></p>
<p>Because of this explosive scaling, priors need to be simulated. Let’s think about some rudimentary Poisson. We sample a bunch of <code>rnorms</code>, exponentiate them, then plot in black. In the text we’ve calculated the mean. This is the explosive scaling. But with Normal(3,0.5) we have a much nicer curve. It’s going to move with the sample, but at least now it’s in the sample space.</p>
<p><img src="slides/L12/35.png" width="1666" /></p>
<p>Slopes are equally hard to intuit. Calculate log population then centre it. If we put a Normal(0,10) prior, you get explosive scaling up or down. A billion tools again really rapidly. On the right something much tighter and flatter. If it’s flat on the linear scale, it’s not going to be flat on the outcome scale.</p>
<p><img src="slides/L12/36.png" width="1666" /></p>
<p>Top model is the intercept-only. The model of interest at the bottom. Compare the two</p>
<p><img src="slides/L12/37.png" width="1666" /></p>
<p>This shows you the effective number of parameters have little to do with the parameter count.</p>
<p>The more complicated model actually has less overfitting risk than the model with one parameter - actually common with Poisson models. Consequence of the way the log link generates predictions.</p>
<p><img src="slides/L12/38.png" width="1666" /></p>
<p>Poisson only needs one parameter - expected counts.</p>
<p>Here the posterior is on two difference scales. On the left is the log. Two trendlines - high and low (dashed) contact with neighbours. Very strong relationship with log population. Some difference in contact rates. Massive uncertainty for high populations. The crossover seems wrong.</p>
<p>On the right, it’s on the natural scale. Same curves but squished and transformed. This comes from the Pareto smooth LOO cross-validation. It gives you the Pareto-k diagnostic, which is a measure of leverage - how much force is each point exerting on the posterior fit. Really useful. Hawaii is the biggest, and you can see why. Has an order of magnitude larger population, so only on informing what happens on that end. Don’t think you want to drop Hawaii, but might want to for fun. It’s doing a lot of work at the high end.</p>
<p><img src="slides/L12/39.png" width="1666" /></p>
<p>Could of criticisms with this model. I like GLMs because they’re unreasonably effective. But they generate a bunch of anomalies. Often they produce ridiculous effects. &gt; Generalized linear madness</p>
<p>Caused by missing scientific knowledge. The first thing is that the intercepts don’t pass through the origin. It has to be true that the 0 and 0 have to go together (0 monkeys, 0 tools). This model doesn’t assert that. Doesn’t have the physics anchoring it where it has to cross. Not a total disaster. Also a weird thing where they cross at the top. <img src="slides/L12/42.png" width="1666" /></p>
<p>What we want is a dynamical systems model that says from one time point to the next, they’re inventing and losing tools, and we get an expected number of tools. Simplest we could do is change in number of tools, with innovation inputs per person <span class="math inline">\(/alpha\)</span>, then <span class="math inline">\(\beta\)</span> for diminishing returns for inventions. Then there’s the loss rate <span class="math inline">\(\gamma\)</span>. World’s simplest cultural evolution model. Now we’ll fit it to data.</p>
<p><img src="slides/L12/43.png" width="1666" /></p>
<p>In this model, the intercepts pass through 0. You can pass this through a Markov chain - just draw some circles.</p>
<p><img src="slides/L12/44.png" width="1666" /></p>
<p>This is just the Poisson regression, but we have this expected lambda now. The only trick is that the parameters must be positive. One is that you can exponentiate the number, which I did with <span class="math inline">\(\alpha\)</span>.</p>
<p><img src="slides/L12/45.png" width="1666" /></p>
<p>Now we can compare the two models. The scientific model still has flaws, but it passes through 0, and you get a solid separation between the solid and dashed lines.</p>
<p><img src="slides/L12/46.png" width="1666" /></p>
<p>If the different counts you have in the dataset, you may have different exposure windows. But say you’re counting fish, and they come out in the Poisson rate. But if someone spends twice as much time fishing, you can’t compare the counts - you need to adjust for the exposure difference. In Poisson, you just add an offset to the model, which here is the log of the amount of time spent fishing.</p>
<p><img src="slides/L12/47.png" width="1666" /></p>
<p>There are a number of other count distributions. Multinomial/categorical is like a dice rather than a coin.</p>
<p><img src="slides/L12/48.png" width="1666" /></p>
<p>Very important class of models. Two things to understand: 1. Discrete (count) events are happening. Cat adoption dataset. But to do it properly, you need to estimate the rate of events. The paramters are about rates, and survival is a count. Because of the exposure window, the cat escapes. We don’t know if it would have been adopted or not, so what do you do with that data? Don’t throw it away, because it’s information. So you count the waiting time until adoption, but also the amount of time you <em>weren’t</em> adopted before you escaped. Phenomenon called censoring.</p>
<p><img src="slides/L12/49.png" width="1666" /></p>
<p>This is 20k cats. Data on the website. They all have chips in them. We’re interested in adoption rates. Black cats to non-black cats. Estimate rates of adoption. Given some assumption about the rate, what’s the probability of waiting for a week. Some cats die, some escape, and peer-censoring is the cat is still there, and the cat’s life goes on.</p>
<p><img src="slides/L12/50.png" width="1666" /></p>
<p>The simplest kind of distribution for survival analysis is exponential. Constant, then fast decay. Half-life.</p>
<p><img src="slides/L12/51.png" width="1666" /></p>
<p>With censored cats, you need to figure out what it means to not have been adopted after a certain time. The complement of the adopted cats up to a day is the cats not adopted up to that day.</p>
<p><img src="slides/L12/52.png" width="1666" /></p>
<p>You’re really just coding the log posterior here. If adopted = 1, here’s the probability.</p>
<p><img src="slides/L12/53.png" width="1666" /></p>
<p>Hypothesis was correct - black cats are discriminated against.</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
