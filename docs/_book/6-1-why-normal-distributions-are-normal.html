<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="6.1 4.1. Why normal distributions are normal | Chapter 5. The Many Variables &amp; The Spurious Waffles" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-05-12" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="6.1 4.1. Why normal distributions are normal | Chapter 5. The Many Variables &amp; The Spurious Waffles">

<title>6.1 4.1. Why normal distributions are normal | Chapter 5. The Many Variables &amp; The Spurious Waffles</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="preface.html#preface">Preface</a>
<ul>
<li><a href="0-1-install-packages.html#install-packages"><span class="toc-section-number">0.1</span> Install packages</a></li>
<li><a href="0-2-split-and-convert-slides.html#split-and-convert-slides"><span class="toc-section-number">0.2</span> Split and convert slides</a></li>
<li><a href="0-3-create-markdown-code-to-incorporate-slides.html#create-markdown-code-to-incorporate-slides"><span class="toc-section-number">0.3</span> Create markdown code to incorporate slides</a></li>
<li><a href="0-4-test-slide.html#test-slide"><span class="toc-section-number">0.4</span> Test slide</a></li>
<li><a href="0-5-test-some-more.html#test-some-more"><span class="toc-section-number">0.5</span> Test some more</a></li>
<li><a href="0-6-and-some-more.html#and-some-more"><span class="toc-section-number">0.6</span> And some more</a>
<ul>
<li><a href="0-6-and-some-more.html#and-what-happens-here"><span class="toc-section-number">0.6.1</span> And what happens here?</a></li>
</ul></li>
</ul></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2-1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2-2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2-3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li><a href="2-4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a></li>
</ul></li>
<li><a href="3-setup.html#setup"><span class="toc-section-number">3</span> Setup</a>
<ul>
<li><a href="3-1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> 3.1. Sampling from a grid-approximate posterior</a></li>
<li><a href="3-2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> 3.2. Sampling to summarize</a>
<ul>
<li><a href="3-2-sampling-to-summarize.html#intervals-of-defined-boundaries"><span class="toc-section-number">3.2.1</span> 3.2.1. Intervals of defined boundaries</a></li>
<li><a href="3-2-sampling-to-summarize.html#intervals-of-defined-mass"><span class="toc-section-number">3.2.2</span> 3.2.2. Intervals of defined mass</a></li>
<li><a href="3-2-sampling-to-summarize.html#point-estimates"><span class="toc-section-number">3.2.3</span> 3.2.3. Point estimates</a></li>
</ul></li>
<li><a href="3-3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> 3.3. Sampling to simulate prediction</a>
<ul>
<li><a href="3-3-sampling-to-simulate-prediction.html#dummy-data"><span class="toc-section-number">3.3.1</span> 3.3.1. Dummy data</a></li>
<li><a href="3-3-sampling-to-simulate-prediction.html#model-checking"><span class="toc-section-number">3.3.2</span> 3.3.2. Model checking</a></li>
</ul></li>
<li><a href="3-4-practice.html#practice"><span class="toc-section-number">3.4</span> 3.5. Practice</a></li>
</ul></li>
<li><a href="4-homework-week-1.html#homework-week-1"><span class="toc-section-number">4</span> Homework: week 1</a>
<ul>
<li><a href="4-1-section.html#section"><span class="toc-section-number">4.1</span> 1.</a></li>
<li><a href="4-2-section-1.html#section-1"><span class="toc-section-number">4.2</span> 2.</a></li>
<li><a href="4-3-section-2.html#section-2"><span class="toc-section-number">4.3</span> 3.</a></li>
</ul></li>
<li><a href="5-setup-1.html#setup-1"><span class="toc-section-number">5</span> Setup</a></li>
<li><a href="6-text.html#text"><span class="toc-section-number">6</span> Text</a>
<ul>
<li><a href="6-1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">6.1</span> 4.1. Why normal distributions are normal</a>
<ul>
<li><a href="6-1-why-normal-distributions-are-normal.html#normal-by-addition"><span class="toc-section-number">6.1.1</span> 4.1.1. Normal by addition</a></li>
<li><a href="6-1-why-normal-distributions-are-normal.html#normal-by-multiplication"><span class="toc-section-number">6.1.2</span> 4.1.2 Normal by multiplication</a></li>
<li><a href="6-1-why-normal-distributions-are-normal.html#using-gaussian-distributions"><span class="toc-section-number">6.1.3</span> 4.1.4. Using Gaussian distributions</a></li>
</ul></li>
<li><a href="6-2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">6.2</span> 4.2. A language for describing models</a>
<ul>
<li><a href="6-2-a-language-for-describing-models.html#re-describing-the-glob-tossing-model"><span class="toc-section-number">6.2.1</span> 4.2.1. Re-describing the glob tossing model</a></li>
<li><a href="6-2-a-language-for-describing-models.html#the-data"><span class="toc-section-number">6.2.2</span> 4.3.1. The data</a></li>
<li><a href="6-2-a-language-for-describing-models.html#the-model"><span class="toc-section-number">6.2.3</span> 4.3.2. The model</a></li>
<li><a href="6-2-a-language-for-describing-models.html#grid-approximation-of-the-posterior-distribution"><span class="toc-section-number">6.2.4</span> 4.3.3. Grid approximation of the posterior distribution</a></li>
<li><a href="6-2-a-language-for-describing-models.html#finding-the-posterior-distribution-with-quap"><span class="toc-section-number">6.2.5</span> 4.3.5. Finding the posterior distribution with <code>quap</code></a></li>
<li><a href="6-2-a-language-for-describing-models.html#sampling-from-a-quap"><span class="toc-section-number">6.2.6</span> 4.3.6. Sampling from a <code>quap</code></a></li>
</ul></li>
<li><a href="6-3-linear-prediction.html#linear-prediction"><span class="toc-section-number">6.3</span> 4.4. Linear prediction</a>
<ul>
<li><a href="6-3-linear-prediction.html#the-linear-model-strategy"><span class="toc-section-number">6.3.1</span> 4.14.1. The linear model strategy</a></li>
<li><a href="6-3-linear-prediction.html#finding-the-posterior-distribution"><span class="toc-section-number">6.3.2</span> 4.4.2. Finding the posterior distribution</a></li>
<li><a href="6-3-linear-prediction.html#interpreting-the-posterior-distribution"><span class="toc-section-number">6.3.3</span> 4.4.3. Interpreting the posterior distribution</a></li>
</ul></li>
</ul></li>
<li><a href="7-setup-2.html#setup-2"><span class="toc-section-number">7</span> Setup</a></li>
<li><a href="8-text-1.html#text-1"><span class="toc-section-number">8</span> Text</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="why-normal-distributions-are-normal" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> 4.1. Why normal distributions are normal</h2>
<p><img src="slides/L03/07.png" width="60%"  /></p>
<p>These appear all throughout nature. Why are they so normal? They arise from all over the place.</p>
<div id="normal-by-addition" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> 4.1.1. Normal by addition</h3>
<p>One of the things that are nice about them is that they are additive. So easy to work with.
Second is that they’re very common.</p>
<p><img src="slides/L03/08.png" width="60%"  /></p>
<p>Imagine a football pitch. We all line up on the midfield line. Take a coin out of your pocket and flip it. One step left for heads, right for tails. Do it a few hundred times.</p>
<p><img src="slides/L03/09.png" width="60%"  /></p>
<p><img src="slides/L03/10.png" width="60%"  /></p>
<p><img src="slides/L03/11.png" width="60%"  /></p>
<p><img src="slides/L03/12.png" width="60%"  /></p>
<p>The frequency distribution will be Gaussian.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="6-1-why-normal-distributions-are-normal.html#cb85-1" aria-hidden="true" tabindex="-1"></a>pos <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sum</span>(<span class="fu">runif</span>(<span class="dv">16</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb85-2"><a href="6-1-why-normal-distributions-are-normal.html#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(pos))</span></code></pre></div>
<p><img src="Statistical-Rethinking-notes_files/figure-html/4.1-1.png" width="672"  /></p>
<p><img src="slides/L03/13.png" width="60%"  /></p>
<p>This is a simulation of the soccer field experiment. After four flips (steps). Black follows one particular student. A pattern forms in the aggregation. This isn’t very Gaussian yet.</p>
<p><img src="slides/L03/14.png" width="60%"  /></p>
<p>After 8 it’s pretty Gaussian.
<img src="slides/L03/15.png" width="60%"  /></p>
<p>And after 16 it’s very Gaussian. It’ll get wider and wider over time. Why does this happen? Lot’s of mathematical theorems. But the intuition is that each coin flip is a fluctuation. And in the long run, fluctuations tend to cancel. If you get a string of lefts, eventually you’ll get a string of rights, so the average student will end up near the middle. A very large number of them exactly cancel each other. There are more paths that will give you 0 than any other path. Then there are a few less that give you +1 or -1. And so forth.</p>
<p><img src="slides/L03/16.png" width="60%"  /></p>
<p>That’s why a bunch of natural systems are normally distributed. We don’t need to know anything except that they cancel out. A lot of common statistics follow this kind of process. What you’re left with are particular shapes, called <em>maximum entropy distributions</em>. For the Gaussian, <strong>addition</strong> is our friend.</p>
<p>One of the things about it is that products of deviations are actually addition. So lots of multiplicative interactions also produce Gaussian distributions.</p>
<p>You can measure things on logarithmic scales.</p>
<p><img src="slides/L03/18.png" width="60%"  /></p>
<p>This is the ontological perspective on distributions. When fluctuations tend to dampen one another, you end up with a symmetric curve. What neat and also frustrating is that you lose a lot of information about the generative processes. When you see heights are normally distributed, you learn basically nothing about it. This is cool because all that’s preserved from the underlying process is the mean and the variance. What’s terrible is that you can’t figure out the process from the distribution.</p>
<p>All the maximum entropy distributions have the same feature. Power laws arise through lots of processes, and it tells you nothing other than it has high variance.</p>
<p>The other perspective is epistemological. If you’re building a model and you want to be as conservative as possible, you should use the Gaussian distribution. Because any other distribution will be narrower. So it’s a very good assumption to use when you don’t have additional information.</p>
<p>The Gaussian is the one where all you’re willing to say is there’s a mean and a variance, you should use the Gaussian. It assumes the least.</p>
</div>
<div id="normal-by-multiplication" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> 4.1.2 Normal by multiplication</h3>
<p>This code just samples 12 random numbers between 1.0 and 1.1, each representing a proportional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="6-1-why-normal-distributions-are-normal.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, .<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 1.744779</code></pre>
<p>Now what distribution do you think these random products will take? Let’s generate 10,000 of them and see:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="6-1-why-normal-distributions-are-normal.html#cb88-1" aria-hidden="true" tabindex="-1"></a>growth <span class="ot">=</span> <span class="fu">replicate</span>(<span class="fl">1e4</span>, <span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, <span class="fl">0.1</span>)))</span>
<span id="cb88-2"><a href="6-1-why-normal-distributions-are-normal.html#cb88-2" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(growth, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p><img src="Statistical-Rethinking-notes_files/figure-html/4.3-1.png" width="672"  />
Multiplying small numbers if approximately the same as addition.</p>
<p>The smaller the effect of each locus, the better this additive approximation will be. In this way, small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.</p>
<p>Verify by comparing:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="6-1-why-normal-distributions-are-normal.html#cb89-1" aria-hidden="true" tabindex="-1"></a>big <span class="ot">=</span> <span class="fu">replicate</span>(<span class="fl">1e4</span>, <span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, .<span class="dv">5</span>)))</span>
<span id="cb89-2"><a href="6-1-why-normal-distributions-are-normal.html#cb89-2" aria-hidden="true" tabindex="-1"></a>small <span class="ot">=</span> <span class="fu">replicate</span>(<span class="fl">1e4</span>, <span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, <span class="fl">0.01</span>)))</span>
<span id="cb89-3"><a href="6-1-why-normal-distributions-are-normal.html#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="6-1-why-normal-distributions-are-normal.html#cb89-4" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(big, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p><img src="Statistical-Rethinking-notes_files/figure-html/4.4-1.png" width="672"  /></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="6-1-why-normal-distributions-are-normal.html#cb90-1" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(small, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p><img src="Statistical-Rethinking-notes_files/figure-html/4.4-2.png" width="672"  />
### 4.1.3. Normal by log-multiplication</p>
<p>Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. e.g.:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="6-1-why-normal-distributions-are-normal.html#cb91-1" aria-hidden="true" tabindex="-1"></a>log.big <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">log</span>(<span class="fu">prod</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">12</span>, <span class="dv">0</span>, .<span class="dv">5</span>))))</span>
<span id="cb91-2"><a href="6-1-why-normal-distributions-are-normal.html#cb91-2" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">dens</span>(log.big, <span class="at">norm.comp =</span> T)</span></code></pre></div>
<p><img src="Statistical-Rethinking-notes_files/figure-html/4.5-1.png" width="672"  />
Adding logs is equivalent to multiplying the original numbers.</p>
</div>
<div id="using-gaussian-distributions" class="section level3" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> 4.1.4. Using Gaussian distributions</h3>
<p><strong>Caution</strong>: Many natural (and unnatural) processes have much heavier tails - much higher probabilities of producing extreme events.</p>
</div>
</div>
<p style="text-align: center;">
<a href="6-text.html"><button class="btn btn-default">Previous</button></a>
<a href="6-2-a-language-for-describing-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
