<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.2 Entropy and accuracy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-07-02" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="7.2 Entropy and accuracy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>7.2 Entropy and accuracy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index">Index</a></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2-1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2-2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2-3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li><a href="2-4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a></li>
</ul></li>
<li><a href="3-sampling-from-the-imaginary.html#sampling-from-the-imaginary"><span class="toc-section-number">3</span> Sampling from the Imaginary</a>
<ul>
<li><a href="3-1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> Sampling from a grid-approximate posterior</a></li>
<li><a href="3-2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> Sampling to summarize</a></li>
<li><a href="3-3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> Sampling to simulate prediction</a></li>
<li><a href="practice.html#practice">Practice</a></li>
<li><a href="homework-week-1.html#homework-week-1">Homework: week 1</a></li>
</ul></li>
<li><a href="4-geocentric-models.html#geocentric-models"><span class="toc-section-number">4</span> Geocentric Models</a>
<ul>
<li><a href="4-1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">4.1</span> Why normal distributions are normal</a></li>
<li><a href="4-2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">4.2</span> A language for describing models</a></li>
<li><a href="4-3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">4.3</span> Gaussian model of height</a></li>
<li><a href="4-4-linear-prediction.html#linear-prediction"><span class="toc-section-number">4.4</span> Linear prediction</a></li>
<li><a href="4-5-curves-from-lines.html#curves-from-lines"><span class="toc-section-number">4.5</span> Curves from lines</a></li>
<li><a href="4-6-practice-1.html#practice-1"><span class="toc-section-number">4.6</span> Practice</a></li>
</ul></li>
<li><a href="5-the-many-variables-the-spurious-waffles.html#the-many-variables-the-spurious-waffles"><span class="toc-section-number">5</span> The Many Variables &amp; The Spurious Waffles</a>
<ul>
<li><a href="5-1-spurious-association.html#spurious-association"><span class="toc-section-number">5.1</span> Spurious association</a></li>
<li><a href="5-2-masked-relationship.html#masked-relationship"><span class="toc-section-number">5.2</span> Masked relationship</a></li>
<li><a href="5-3-categorical-variables.html#categorical-variables"><span class="toc-section-number">5.3</span> Categorical variables</a></li>
<li><a href="5-4-practice-2.html#practice-2"><span class="toc-section-number">5.4</span> Practice</a></li>
</ul></li>
<li><a href="6-the-haunted-dag-the-causal-terror.html#the-haunted-dag-the-causal-terror"><span class="toc-section-number">6</span> The Haunted DAG &amp; The Causal Terror</a>
<ul>
<li><a href="6-1-multicollinearity.html#multicollinearity"><span class="toc-section-number">6.1</span> Multicollinearity</a></li>
<li><a href="6-2-post-treatment-bias.html#post-treatment-bias"><span class="toc-section-number">6.2</span> Post-treatment bias</a></li>
<li><a href="6-3-collider-bias.html#collider-bias"><span class="toc-section-number">6.3</span> Collider bias</a></li>
<li><a href="6-4-confronting-confounding.html#confronting-confounding"><span class="toc-section-number">6.4</span> Confronting confounding</a></li>
<li><a href="6-5-summary.html#summary"><span class="toc-section-number">6.5</span> Summary</a></li>
<li><a href="6-6-practice-3.html#practice-3"><span class="toc-section-number">6.6</span> Practice</a></li>
</ul></li>
<li><a href="7-ulysses-compass.html#ulysses-compass"><span class="toc-section-number">7</span> Ulysses’ Compass</a>
<ul>
<li><a href="7-1-the-problem-with-parameters.html#the-problem-with-parameters"><span class="toc-section-number">7.1</span> The problem with parameters</a></li>
<li><a href="7-2-entropy-and-accuracy.html#entropy-and-accuracy"><span class="toc-section-number">7.2</span> Entropy and accuracy</a></li>
<li><a href="7-3-golem-taming-regularization.html#golem-taming-regularization"><span class="toc-section-number">7.3</span> Golem taming: regularization</a></li>
<li><a href="7-4-predicting-predictive-accuracy.html#predicting-predictive-accuracy"><span class="toc-section-number">7.4</span> Predicting predictive accuracy</a></li>
<li><a href="7-5-model-comparison.html#model-comparison"><span class="toc-section-number">7.5</span> Model comparison</a></li>
<li><a href="7-6-practice-4.html#practice-4"><span class="toc-section-number">7.6</span> Practice</a></li>
</ul></li>
<li><a href="8-conditional-manatees.html#conditional-manatees"><span class="toc-section-number">8</span> Conditional Manatees</a>
<ul>
<li><a href="8-1-building-an-interaction.html#building-an-interaction"><span class="toc-section-number">8.1</span> Building an interaction</a></li>
<li><a href="8-2-symmetry-of-interactions.html#symmetry-of-interactions"><span class="toc-section-number">8.2</span> Symmetry of interactions</a></li>
<li><a href="8-3-continuous-interactions.html#continuous-interactions"><span class="toc-section-number">8.3</span> Continuous interactions</a></li>
</ul></li>
<li><a href="9-markov-chain-monte-carlo.html#markov-chain-monte-carlo"><span class="toc-section-number">9</span> Markov Chain Monte Carlo</a>
<ul>
<li><a href="9-1-good-king-markov-and-his-island-kingdom.html#good-king-markov-and-his-island-kingdom"><span class="toc-section-number">9.1</span> Good King Markov and his island kingdom</a></li>
<li><a href="9-2-metropolis-algorithm.html#metropolis-algorithm"><span class="toc-section-number">9.2</span> Metropolis algorithm</a></li>
<li><a href="9-3-hamiltonian-monte-carlo.html#hamiltonian-monte-carlo"><span class="toc-section-number">9.3</span> Hamiltonian Monte Carlo</a></li>
<li><a href="9-4-easy-hmc-ulam.html#easy-hmc-ulam"><span class="toc-section-number">9.4</span> Easy HMC: <code>ulam</code></a></li>
<li><a href="9-5-care-and-feeding-of-your-markov-chain.html#care-and-feeding-of-your-markov-chain"><span class="toc-section-number">9.5</span> Care and feeding of your Markov chain</a></li>
</ul></li>
<li><a href="10-big-entropy-and-the-generalized-linear-model.html#big-entropy-and-the-generalized-linear-model"><span class="toc-section-number">10</span> Big Entropy and the Generalized Linear Model</a>
<ul>
<li><a href="10-1-maximum-entropy.html#maximum-entropy"><span class="toc-section-number">10.1</span> Maximum entropy</a></li>
</ul></li>
<li><a href="11-god-spiked-the-integers.html#god-spiked-the-integers"><span class="toc-section-number">11</span> God Spiked the Integers</a></li>
<li><a href="12-monsters-and-mixtures.html#monsters-and-mixtures"><span class="toc-section-number">12</span> Monsters and Mixtures</a></li>
<li><a href="13-models-with-memory.html#models-with-memory"><span class="toc-section-number">13</span> Models With Memory</a></li>
<li><a href="14-adventures-in-covariance.html#adventures-in-covariance"><span class="toc-section-number">14</span> Adventures in Covariance</a></li>
<li><a href="15-missing-data-and-other-opportunities.html#missing-data-and-other-opportunities"><span class="toc-section-number">15</span> Missing Data and Other Opportunities</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="entropy-and-accuracy" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Entropy and accuracy</h2>
<div class="figure">
<img src="slides/L07/35.png" alt="Multiple strategies. In Bayesian statistics, we regularise. Can be even omre aggressive. In non-Bayesian, it's mathematically identical to using a prior. Why do machine leanring people regularise? Because it makes better predictions. " width="80%" />
<p class="caption marginnote shownote">
Multiple strategies. In Bayesian statistics, we regularise. Can be even omre aggressive. In non-Bayesian, it’s mathematically identical to using a prior. Why do machine leanring people regularise? Because it makes better predictions.
</p>
</div>
<div class="figure">
<img src="slides/L07/36.png" alt="We want to get to CV and WAIC, which replaced AIC. The jounrey to these appraoches requires some setups. First thing to answer is how to measure accuracy. Many bad ways to measure it. There's an actual gold standard. And once we've got it, we want to measure distance from the target. How do we decide how close the models are getting to it? Then we learn how to develop these instruments." width="80%" />
<p class="caption marginnote shownote">
We want to get to CV and WAIC, which replaced AIC. The jounrey to these appraoches requires some setups. First thing to answer is how to measure accuracy. Many bad ways to measure it. There’s an actual gold standard. And once we’ve got it, we want to measure distance from the target. How do we decide how close the models are getting to it? Then we learn how to develop these instruments.
</p>
</div>
<p><strong><em>7.2.1. Firing the weatherperson</em></strong></p>
<p>In defining a target, there are two major dimensions to worry about:</p>
<ol style="list-style-type: decimal">
<li><em>Cost-benefit analysis</em>. How much does it cost when we’re wrong? How much do we win when we’re right? Most scientists never ask these questions in any formal way, but applied scientists must routinely answer them.</li>
<li><em>Accuracy in context</em>. Some prediction tasks are inherently easier than others. So even if we ignore costs and benefits, we still need a way to judge “accuracy” that accounts for how much a model could possibly improve prediction.</li>
</ol>
<p><img src="slides/L07/37.png" width="80%" /></p>
<hr />
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="7-2-entropy-and-accuracy.html#cb349-1" aria-hidden="true" tabindex="-1"></a>slides_dir <span class="ot">=</span> here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;docs/slides/L08&quot;</span>)</span></code></pre></div>
<p><img src="slides/L08/01.png" width="60%" /></p>
<div class="figure">
<img src="slides/L08/02.png" alt="We need to appeal to information theory because machine prediction works by following the laws of information theory. We'll drive the single gold-standard way to score a model's accuracy. Here's the basic problem information theory sets out to address. When we have some unknown event, there is uncertainty. When we know more, we become less uncertain. IC is a principle for saying when something is more uncertain than something else. There's uncertainty about the weather tomorrow. We may use cues from today to predict tomorrow." width="60%" />
<p class="caption marginnote shownote">
We need to appeal to information theory because machine prediction works by following the laws of information theory. We’ll drive the single gold-standard way to score a model’s accuracy. Here’s the basic problem information theory sets out to address. When we have some unknown event, there is uncertainty. When we know more, we become less uncertain. IC is a principle for saying when something is more uncertain than something else. There’s uncertainty about the weather tomorrow. We may use cues from today to predict tomorrow.
</p>
</div>
<div class="figure">
<img src="slides/L08/03.png" alt="Presume you know that LA has no weather. Always sunny. 15-20 degrees. Little uncertainty. If it does rain, you'll be shocked. Contrast this with Glasgow, where it rains a lot. More rain than not. NY has highly-variable weather. There's great uncertainty about what the weather would be like, unlike the other two. This uncertainty arises from the frequency distributions of these microclimates." width="60%" />
<p class="caption marginnote shownote">
Presume you know that LA has no weather. Always sunny. 15-20 degrees. Little uncertainty. If it does rain, you’ll be shocked. Contrast this with Glasgow, where it rains a lot. More rain than not. NY has highly-variable weather. There’s great uncertainty about what the weather would be like, unlike the other two. This uncertainty arises from the frequency distributions of these microclimates.
</p>
</div>
<p><strong><em>7.2.2. Information and uncertainty</em></strong></p>
<p>The basic insight is to ask: How much is our uncertainty reduced by learning an outcome?</p>
<blockquote>
<p>Information: The reduction in uncertainty when we learn an outcome.</p>
</blockquote>
<p>There are many possible ways to measure uncertainty. The most common way begins by naming some properties a measure of uncertainty should possess. These are the three intuitive desiderata:</p>
<ol style="list-style-type: decimal">
<li><p>The measure of uncertainty should be continuous.</p></li>
<li><p>The measure of uncertainty should increase as the number of possible events increases.</p></li>
<li><p>The measure of uncertainty should be additive.</p></li>
</ol>
<p>There is only one function that satisfies these desiderata. This function is usually known as INFORMATION ENTROPY, and has a surprisingly simple definition.</p>
<div class="figure">
<img src="slides/L08/04.png" alt="Uncertainty $H$ of $p$, which is a vector of probability, is just the average log-probability of the event. This is a unique criterion. If you want a reasonable measure of surprise, you have to adopt something that is this or something proportional to this. Your mobile phones (3G and above) work because of this." width="60%" />
<p class="caption marginnote shownote">
Uncertainty <span class="math inline">\(H\)</span> of <span class="math inline">\(p\)</span>, which is a vector of probability, is just the average log-probability of the event. This is a unique criterion. If you want a reasonable measure of surprise, you have to adopt something that is this or something proportional to this. Your mobile phones (3G and above) work because of this.
</p>
</div>
<p><strong><em>7.2.3. From entropy to accuracy</em></strong></p>
<p>How can we use information entropy to say how far a model is from the target?</p>
<blockquote>
<p><strong>Divergence</strong>: The additional uncertainty induced by using probabilities from one distribution to describe another distribution.</p>
</blockquote>
<div class="figure">
<img src="slides/L08/05.png" alt="What's the **potential for surprise**?. We are interested in this. Want to calculate the entropy of our model, and then there's the entropy of the true distribution, of nature. And we want to minimise the difference between them. This is called the $D_{KL}$ divergence. Two probabilities $p$ and $q$. $p$ is nature, say the frequencies of weather events, and $q$ is our forecast. If we want to score $q$, we look at the divergence. K is for Kulbak. The distance from $p$ to $q$ is the sum (averaging) between $p$ and $q$. It's a distance, but it's not symmetric. " width="60%" />
<p class="caption marginnote shownote">
What’s the <strong>potential for surprise</strong>?. We are interested in this. Want to calculate the entropy of our model, and then there’s the entropy of the true distribution, of nature. And we want to minimise the difference between them. This is called the <span class="math inline">\(D_{KL}\)</span> divergence. Two probabilities <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. <span class="math inline">\(p\)</span> is nature, say the frequencies of weather events, and <span class="math inline">\(q\)</span> is our forecast. If we want to score <span class="math inline">\(q\)</span>, we look at the divergence. K is for Kulbak. The distance from <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span> is the sum (averaging) between <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. It’s a distance, but it’s not symmetric.
</p>
</div>
<div class="figure">
<img src="slides/L08/06.png" alt="Easy to code. Take the vector `p`. Sum `p` time the difference between `log(p)` and `log(q)`. It's only 0 where `q = p`. " width="60%" />
<p class="caption marginnote shownote">
Easy to code. Take the vector <code>p</code>. Sum <code>p</code> time the difference between <code>log(p)</code> and <code>log(q)</code>. It’s only 0 where <code>q = p</code>.
</p>
</div>
<p>Compute the information entropy for the weather:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="7-2-entropy-and-accuracy.html#cb350-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="fl">0.3</span> , <span class="fl">0.7</span> )</span>
<span id="cb350-2"><a href="7-2-entropy-and-accuracy.html#cb350-2" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">sum</span>( p<span class="sc">*</span><span class="fu">log</span>(p) )</span></code></pre></div>
<pre><code>## [1] 0.6108643</code></pre>
<p>Suppose instead we live in Abu Dhabi. Then the probabilities of rain and shine might be more like
$p_1 = 0.01 $ and <span class="math inline">\(p_2 = 0.99\)</span>. Now the entropy would be approximately 0.06. Why has the uncertainty decreased? Because in Abu Dhabi it hardly ever rains. Therefore there’s much less uncertainty about any given day, compared to a place in which it rains 30% of the time. It’s in this way that information entropy measures the uncertainty inherent in a distribution of events. Similarly, if we add another kind of event to the distribution—forecasting into winter, so also predicting snow—entropy tends to increase, due to the added dimensionality of the prediction problem.</p>
<div class="figure">
<img src="slides/L08/07.png" alt="Here's a cartoon version. You're heading to Mars, or a Mars-like planet, but you don't know much about it. You can't control your rocket and you want to predict whether you'll land on water or land. You'll use Earth as your only model. Earth is a high-entropy planet because it has a lot of water and land. So you won't be surprised whether you get land or water. " width="60%" />
<p class="caption marginnote shownote">
Here’s a cartoon version. You’re heading to Mars, or a Mars-like planet, but you don’t know much about it. You can’t control your rocket and you want to predict whether you’ll land on water or land. You’ll use Earth as your only model. Earth is a high-entropy planet because it has a lot of water and land. So you won’t be surprised whether you get land or water.
</p>
</div>
<div class="figure">
<img src="slides/L08/08.png" alt="But say you're going in the other direction. Your potential for surprise is now very high. When you get to Earth and discover all this blue liquid, you'll be surprised. Mars is the LA of planets. And as a consequence, the information distance from Earth to Mars is smaller than the information distance from Mars to Earth. Because if your model is the Earth, it expects all sorts of events, which means that it's less surprised, which means that its prediction error is lower, on average, across a huge number of potential planets across the universe, than if you came from Mars, where you'll be surprised by water all the time. **This is why simpler models work better - because they have higher entropy.** The distance between a simpler model and other things are on average lower, because it expects many things. Gneeralized linear models have higher entropy. All machine learning works this way." width="60%" />
<p class="caption marginnote shownote">
But say you’re going in the other direction. Your potential for surprise is now very high. When you get to Earth and discover all this blue liquid, you’ll be surprised. Mars is the LA of planets. And as a consequence, the information distance from Earth to Mars is smaller than the information distance from Mars to Earth. Because if your model is the Earth, it expects all sorts of events, which means that it’s less surprised, which means that its prediction error is lower, on average, across a huge number of potential planets across the universe, than if you came from Mars, where you’ll be surprised by water all the time. <strong>This is why simpler models work better - because they have higher entropy.</strong> The distance between a simpler model and other things are on average lower, because it expects many things. Gneeralized linear models have higher entropy. All machine learning works this way.
</p>
</div>
<p><strong><em>7.2.4. Estimating divergence</em></strong></p>
<div class="figure">
<img src="slides/L08/09.png" alt="How to estimate this in practice: we want the gold standard way to score, but the problem is we can't score the truth. Turns out we don't need the truth part because it's just an additive term, so you can get the relative scores of the models without knowning the truth. THe log score is the gold standard, whether you're Bayesian or not. In practice, there's not a single log score, but a distribution of log scores. So we want the average log score, which unfortunately is called the *log-pointwise-predictive-density*. For each point `i`, we're taking the average probability of that observation conditional on the samples, and we average over the samples, and find the average probabiltiy that the model expects, then we take the log and sum across all observations in the model." width="60%" />
<p class="caption marginnote shownote">
How to estimate this in practice: we want the gold standard way to score, but the problem is we can’t score the truth. Turns out we don’t need the truth part because it’s just an additive term, so you can get the relative scores of the models without knowning the truth. THe log score is the gold standard, whether you’re Bayesian or not. In practice, there’s not a single log score, but a distribution of log scores. So we want the average log score, which unfortunately is called the <em>log-pointwise-predictive-density</em>. For each point <code>i</code>, we’re taking the average probability of that observation conditional on the samples, and we average over the samples, and find the average probabiltiy that the model expects, then we take the log and sum across all observations in the model.
</p>
</div>
<blockquote>
<p>This kind of score is a log-probability score, and it is the gold standard way to compare the predictive accuracy of different models. It is an estimate of <span class="math inline">\(E\ log(q_i)\)</span>, just without the final step of dividing by the number of observations.</p>
</blockquote>
<p>Compute lppd for the first model we fit in this chapter:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="7-2-entropy-and-accuracy.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb352-2"><a href="7-2-entropy-and-accuracy.html#cb352-2" aria-hidden="true" tabindex="-1"></a>rethinking<span class="sc">::</span><span class="fu">lppd</span>( m7<span class="fl">.1</span> , <span class="at">n=</span><span class="fl">1e4</span> ) </span></code></pre></div>
<pre><code>## [1]  0.6098669  0.6483439  0.5496093  0.6234934  0.4648143  0.4347605 -0.8444632</code></pre>
<p>Larger values are better, because that indicates larger average accuracy.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="7-2-entropy-and-accuracy.html#cb354-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb354-2"><a href="7-2-entropy-and-accuracy.html#cb354-2" aria-hidden="true" tabindex="-1"></a>logprob <span class="ot">&lt;-</span> <span class="fu">sim</span>( m7<span class="fl">.1</span> , <span class="at">ll=</span><span class="cn">TRUE</span> , <span class="at">n=</span><span class="fl">1e4</span> )</span>
<span id="cb354-3"><a href="7-2-entropy-and-accuracy.html#cb354-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">ncol</span>(logprob)</span>
<span id="cb354-4"><a href="7-2-entropy-and-accuracy.html#cb354-4" aria-hidden="true" tabindex="-1"></a>ns <span class="ot">&lt;-</span> <span class="fu">nrow</span>(logprob)</span>
<span id="cb354-5"><a href="7-2-entropy-and-accuracy.html#cb354-5" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>( i ) <span class="fu">log_sum_exp</span>( logprob[,i] ) <span class="sc">-</span> <span class="fu">log</span>(ns)</span>
<span id="cb354-6"><a href="7-2-entropy-and-accuracy.html#cb354-6" aria-hidden="true" tabindex="-1"></a>( lppd <span class="ot">&lt;-</span> <span class="fu">sapply</span>( <span class="dv">1</span><span class="sc">:</span>n , f ) )</span></code></pre></div>
<pre><code>## [1]  0.6098669  0.6483439  0.5496093  0.6234934  0.4648143  0.4347605 -0.8444632</code></pre>
<div class="figure">
<img src="slides/L08/10.png" alt="Why does this all matter in a practical sense? We can measure overfitting. Look at the difference between in- and out-of-sample. Smaller is better. The more negative it is, the better it is. Two samples from the same generative process. Training and testing set. Fit our model to the training sample, and get the deviance of train. Then we force it to predict the out-of-sample. The difference between them are our measure of overfitting." width="60%" />
<p class="caption marginnote shownote">
Why does this all matter in a practical sense? We can measure overfitting. Look at the difference between in- and out-of-sample. Smaller is better. The more negative it is, the better it is. Two samples from the same generative process. Training and testing set. Fit our model to the training sample, and get the deviance of train. Then we force it to predict the out-of-sample. The difference between them are our measure of overfitting.
</p>
</div>
<div class="figure">
<img src="slides/L08/11.png" alt="We'll generate some samples based on a known &quot;truth&quot;. The first is our intercept model. " width="60%" />
<p class="caption marginnote shownote">
We’ll generate some samples based on a known “truth”. The first is our intercept model.
</p>
</div>
<p><strong><em>7.2.5. Scoring the right data</em></strong></p>
<p>Let’s compute the log-score for each of the models from earlier in this chapter:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="7-2-entropy-and-accuracy.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) </span>
<span id="cb356-2"><a href="7-2-entropy-and-accuracy.html#cb356-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sapply</span>( <span class="fu">list</span>(m7<span class="fl">.1</span>,m7<span class="fl">.2</span>,m7<span class="fl">.3</span>,m7<span class="fl">.4</span>,m7<span class="fl">.5</span>,m7<span class="fl">.6</span>) , <span class="cf">function</span>(m) <span class="fu">sum</span>(<span class="fu">lppd</span>(m)) )</span></code></pre></div>
<pre><code>## [1]  2.490390  2.566165  3.707343  5.333750 14.090061 39.445390</code></pre>
<p>The more complex models have larger scores, but it is really the score on new data that interests us.</p>
<div class="figure">
<img src="slides/L08/12.png" alt="This is what happens in-sample. Lower deviance is better. The point is the average across all simulations, with one standard deviation on either side. Note the more complicated models do better. They're always going to fit in-sample better. Note there's a big jump at 3, then very little after 3. " width="60%" />
<p class="caption marginnote shownote">
This is what happens in-sample. Lower deviance is better. The point is the average across all simulations, with one standard deviation on either side. Note the more complicated models do better. They’re always going to fit in-sample better. Note there’s a big jump at 3, then very little after 3.
</p>
</div>
<div class="figure">
<img src="slides/L08/13.png" alt="Here's out-of-sample. Unsurprisingly, everything does worse out-of-sample. There's a pattern to the amount of overfitting. You can see that model 3 is best on average. Models 4 and 5 get progressively worse, because they're fitting noise. " width="60%" />
<p class="caption marginnote shownote">
Here’s out-of-sample. Unsurprisingly, everything does worse out-of-sample. There’s a pattern to the amount of overfitting. You can see that model 3 is best on average. Models 4 and 5 get progressively worse, because they’re fitting noise.
</p>
</div>
<div class="figure">
<img src="slides/L08/14.png" alt="In anthropology we're happy with 20. But with N = 100, you can more precisely estimate when a data point doesn't matter. So 4 and 5 are only slightly worse. Because you can get a really good posterior distribution. But they pattern is the same. There's a very special pattern in the distances between these points. On the left, you can see the distances are growing, and approximately twice the number of parameters in each case. Hold that in your mind." width="60%" />
<p class="caption marginnote shownote">
In anthropology we’re happy with 20. But with N = 100, you can more precisely estimate when a data point doesn’t matter. So 4 and 5 are only slightly worse. Because you can get a really good posterior distribution. But they pattern is the same. There’s a very special pattern in the distances between these points. On the left, you can see the distances are growing, and approximately twice the number of parameters in each case. Hold that in your mind.
</p>
</div>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="7-2-entropy-and-accuracy.html#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: this takes ages to run, so I reduced the number of samples from 1e4 to 1e3 </span></span>
<span id="cb358-2"><a href="7-2-entropy-and-accuracy.html#cb358-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb358-3"><a href="7-2-entropy-and-accuracy.html#cb358-3" aria-hidden="true" tabindex="-1"></a>kseq <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span></span>
<span id="cb358-4"><a href="7-2-entropy-and-accuracy.html#cb358-4" aria-hidden="true" tabindex="-1"></a>dev <span class="ot">&lt;-</span> <span class="fu">sapply</span>( kseq , <span class="cf">function</span>(k) {</span>
<span id="cb358-5"><a href="7-2-entropy-and-accuracy.html#cb358-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(k);</span>
<span id="cb358-6"><a href="7-2-entropy-and-accuracy.html#cb358-6" aria-hidden="true" tabindex="-1"></a>    r <span class="ot">&lt;-</span> <span class="fu">replicate</span>( <span class="dv">1000</span> , rethinking<span class="sc">::</span><span class="fu">sim_train_test</span>( <span class="at">N=</span>N, <span class="at">k=</span>k ) );</span>
<span id="cb358-7"><a href="7-2-entropy-and-accuracy.html#cb358-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>( <span class="fu">mean</span>(r[<span class="dv">1</span>,]) , <span class="fu">mean</span>(r[<span class="dv">2</span>,]) , <span class="fu">sd</span>(r[<span class="dv">1</span>,]) , <span class="fu">sd</span>(r[<span class="dv">2</span>,]) )</span>
<span id="cb358-8"><a href="7-2-entropy-and-accuracy.html#cb358-8" aria-hidden="true" tabindex="-1"></a>  } )</span></code></pre></div>
<p>Parallelize the simulations by replacing the replicate line with:</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="7-2-entropy-and-accuracy.html#cb359-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: doesn&#39;t seem to work</span></span>
<span id="cb359-2"><a href="7-2-entropy-and-accuracy.html#cb359-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">mcreplicate</span>( <span class="fl">1e4</span> , <span class="fu">sim_train_test</span>( <span class="at">N=</span>N, <span class="at">k=</span>k ) , <span class="at">mc.cores=</span><span class="dv">2</span> )</span></code></pre></div>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="7-2-entropy-and-accuracy.html#cb360-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add variables again</span></span>
<span id="cb360-2"><a href="7-2-entropy-and-accuracy.html#cb360-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb360-3"><a href="7-2-entropy-and-accuracy.html#cb360-3" aria-hidden="true" tabindex="-1"></a>kseq <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span></span>
<span id="cb360-4"><a href="7-2-entropy-and-accuracy.html#cb360-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-5"><a href="7-2-entropy-and-accuracy.html#cb360-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span> , dev[<span class="dv">1</span>,] , <span class="at">ylim=</span><span class="fu">c</span>( <span class="fu">min</span>(dev[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,])<span class="sc">-</span><span class="dv">5</span> , <span class="fu">max</span>(dev[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,])<span class="sc">+</span><span class="dv">10</span> ) ,</span>
<span id="cb360-6"><a href="7-2-entropy-and-accuracy.html#cb360-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">5.1</span>) , <span class="at">xlab=</span><span class="st">&quot;number of parameters&quot;</span> , <span class="at">ylab=</span><span class="st">&quot;deviance&quot;</span> ,</span>
<span id="cb360-7"><a href="7-2-entropy-and-accuracy.html#cb360-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">pch=</span><span class="dv">16</span> , <span class="at">col=</span>rangi2 )</span>
<span id="cb360-8"><a href="7-2-entropy-and-accuracy.html#cb360-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>( <span class="fu">concat</span>( <span class="st">&quot;N = &quot;</span>,N ) )</span>
<span id="cb360-9"><a href="7-2-entropy-and-accuracy.html#cb360-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>( (<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">+</span><span class="fl">0.1</span> , dev[<span class="dv">2</span>,] )</span>
<span id="cb360-10"><a href="7-2-entropy-and-accuracy.html#cb360-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( i <span class="cf">in</span> kseq ) {</span>
<span id="cb360-11"><a href="7-2-entropy-and-accuracy.html#cb360-11" aria-hidden="true" tabindex="-1"></a>  pts_in <span class="ot">&lt;-</span> dev[<span class="dv">1</span>,i] <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="sc">+</span><span class="dv">1</span>)<span class="sc">*</span>dev[<span class="dv">3</span>,i]</span>
<span id="cb360-12"><a href="7-2-entropy-and-accuracy.html#cb360-12" aria-hidden="true" tabindex="-1"></a>  pts_out <span class="ot">&lt;-</span> dev[<span class="dv">2</span>,i] <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="sc">+</span><span class="dv">1</span>)<span class="sc">*</span>dev[<span class="dv">4</span>,i]</span>
<span id="cb360-13"><a href="7-2-entropy-and-accuracy.html#cb360-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>( <span class="fu">c</span>(i,i) , pts_in , <span class="at">col=</span>rangi2 )</span>
<span id="cb360-14"><a href="7-2-entropy-and-accuracy.html#cb360-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>( <span class="fu">c</span>(i,i)<span class="sc">+</span><span class="fl">0.1</span> , pts_out )</span>
<span id="cb360-15"><a href="7-2-entropy-and-accuracy.html#cb360-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="07_ulysses_compass_files/figure-html/7.18-1.png" width="672" /></p>
</div>
<p style="text-align: center;">
<a href="7-1-the-problem-with-parameters.html"><button class="btn btn-default">Previous</button></a>
<a href="7-3-golem-taming-regularization.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
