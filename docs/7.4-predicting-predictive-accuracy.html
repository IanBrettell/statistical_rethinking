<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.4 Predicting predictive accuracy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath" />
<meta property="og:type" content="book" />






<meta name="date" content="2021-12-04" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="7.4 Predicting predictive accuracy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath">

<title>7.4 Predicting predictive accuracy | Notes for Statistical Rethinking 2nd ed. by Richard McElreath</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index">Index</a></li>
<li><a href="1-the-golem-of-prague.html#the-golem-of-prague"><span class="toc-section-number">1</span> The Golem of Prague</a></li>
<li class="has-sub"><a href="2-small-worlds-and-large-worlds.html#small-worlds-and-large-worlds"><span class="toc-section-number">2</span> Small Worlds and Large Worlds</a>
<ul>
<li><a href="2.1-the-garden-of-forking-data.html#the-garden-of-forking-data"><span class="toc-section-number">2.1</span> The garden of forking data</a></li>
<li><a href="2.2-building-a-model.html#building-a-model"><span class="toc-section-number">2.2</span> Building a model</a></li>
<li><a href="2.3-components-of-the-model.html#components-of-the-model"><span class="toc-section-number">2.3</span> Components of the model</a></li>
<li class="has-sub"><a href="2.4-making-the-model-go.html#making-the-model-go"><span class="toc-section-number">2.4</span> Making the model go</a>
<ul>
<li><a href="2.4-making-the-model-go.html#markov-chain-monte-carlo"><span class="toc-section-number">2.4.1</span> Markov chain Monte Carlo</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="3-sampling-from-the-imaginary.html#sampling-from-the-imaginary"><span class="toc-section-number">3</span> Sampling from the Imaginary</a>
<ul>
<li><a href="3.1-sampling-from-a-grid-approximate-posterior.html#sampling-from-a-grid-approximate-posterior"><span class="toc-section-number">3.1</span> Sampling from a grid-approximate posterior</a></li>
<li><a href="3.2-sampling-to-summarize.html#sampling-to-summarize"><span class="toc-section-number">3.2</span> Sampling to summarize</a></li>
<li><a href="3.3-sampling-to-simulate-prediction.html#sampling-to-simulate-prediction"><span class="toc-section-number">3.3</span> Sampling to simulate prediction</a></li>
<li><a href="3.4-lets-practice-with-brms.html#lets-practice-with-brms"><span class="toc-section-number">3.4</span> Let’s practice with brms</a></li>
<li><a href="practice.html#practice">Practice</a></li>
<li><a href="homework-week-1.html#homework-week-1">Homework: week 1</a></li>
</ul></li>
<li class="has-sub"><a href="4-geocentric-models.html#geocentric-models"><span class="toc-section-number">4</span> Geocentric Models</a>
<ul>
<li><a href="4.1-why-normal-distributions-are-normal.html#why-normal-distributions-are-normal"><span class="toc-section-number">4.1</span> Why normal distributions are normal</a></li>
<li><a href="4.2-a-language-for-describing-models.html#a-language-for-describing-models"><span class="toc-section-number">4.2</span> A language for describing models</a></li>
<li><a href="4.3-gaussian-model-of-height.html#gaussian-model-of-height"><span class="toc-section-number">4.3</span> Gaussian model of height</a></li>
<li><a href="4.4-linear-prediction.html#linear-prediction"><span class="toc-section-number">4.4</span> Linear prediction</a></li>
<li><a href="4.5-curves-from-lines.html#curves-from-lines"><span class="toc-section-number">4.5</span> Curves from lines</a></li>
<li><a href="4.6-practice-1.html#practice-1"><span class="toc-section-number">4.6</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="5-the-many-variables-the-spurious-waffles.html#the-many-variables-the-spurious-waffles"><span class="toc-section-number">5</span> The Many Variables &amp; The Spurious Waffles</a>
<ul>
<li><a href="5.1-spurious-association.html#spurious-association"><span class="toc-section-number">5.1</span> Spurious association</a></li>
<li><a href="5.2-masked-relationship.html#masked-relationship"><span class="toc-section-number">5.2</span> Masked relationship</a></li>
<li><a href="5.3-categorical-variables.html#categorical-variables"><span class="toc-section-number">5.3</span> Categorical variables</a></li>
<li><a href="5.4-practice-2.html#practice-2"><span class="toc-section-number">5.4</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="6-the-haunted-dag-the-causal-terror.html#the-haunted-dag-the-causal-terror"><span class="toc-section-number">6</span> The Haunted DAG &amp; The Causal Terror</a>
<ul>
<li><a href="6.1-multicollinearity.html#multicollinearity"><span class="toc-section-number">6.1</span> Multicollinearity</a></li>
<li><a href="6.2-post-treatment-bias.html#post-treatment-bias"><span class="toc-section-number">6.2</span> Post-treatment bias</a></li>
<li><a href="6.3-collider-bias.html#collider-bias"><span class="toc-section-number">6.3</span> Collider bias</a></li>
<li><a href="6.4-confronting-confounding.html#confronting-confounding"><span class="toc-section-number">6.4</span> Confronting confounding</a></li>
<li><a href="6.5-summary.html#summary"><span class="toc-section-number">6.5</span> Summary</a></li>
<li><a href="6.6-practice-3.html#practice-3"><span class="toc-section-number">6.6</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="7-ulysses-compass.html#ulysses-compass"><span class="toc-section-number">7</span> Ulysses’ Compass</a>
<ul>
<li><a href="7.1-the-problem-with-parameters.html#the-problem-with-parameters"><span class="toc-section-number">7.1</span> The problem with parameters</a></li>
<li><a href="7.2-entropy-and-accuracy.html#entropy-and-accuracy"><span class="toc-section-number">7.2</span> Entropy and accuracy</a></li>
<li><a href="7.3-golem-taming-regularization.html#golem-taming-regularization"><span class="toc-section-number">7.3</span> Golem taming: regularization</a></li>
<li><a href="7.4-predicting-predictive-accuracy.html#predicting-predictive-accuracy"><span class="toc-section-number">7.4</span> Predicting predictive accuracy</a></li>
<li><a href="7.5-model-comparison.html#model-comparison"><span class="toc-section-number">7.5</span> Model comparison</a></li>
<li><a href="7.6-practice-4.html#practice-4"><span class="toc-section-number">7.6</span> Practice</a></li>
</ul></li>
<li class="has-sub"><a href="8-conditional-manatees.html#conditional-manatees"><span class="toc-section-number">8</span> Conditional Manatees</a>
<ul>
<li><a href="8.1-building-an-interaction.html#building-an-interaction"><span class="toc-section-number">8.1</span> Building an interaction</a></li>
<li><a href="8.2-symmetry-of-interactions.html#symmetry-of-interactions"><span class="toc-section-number">8.2</span> Symmetry of interactions</a></li>
<li><a href="8.3-continuous-interactions.html#continuous-interactions"><span class="toc-section-number">8.3</span> Continuous interactions</a></li>
</ul></li>
<li class="has-sub"><a href="9-markov-chain-monte-carlo-1.html#markov-chain-monte-carlo-1"><span class="toc-section-number">9</span> Markov Chain Monte Carlo</a>
<ul>
<li><a href="9.1-good-king-markov-and-his-island-kingdom.html#good-king-markov-and-his-island-kingdom"><span class="toc-section-number">9.1</span> Good King Markov and his island kingdom</a></li>
<li><a href="9.2-metropolis-algorithm.html#metropolis-algorithm"><span class="toc-section-number">9.2</span> Metropolis algorithm</a></li>
<li><a href="9.3-hamiltonian-monte-carlo.html#hamiltonian-monte-carlo"><span class="toc-section-number">9.3</span> Hamiltonian Monte Carlo</a></li>
<li><a href="9.4-easy-hmc-ulam.html#easy-hmc-ulam"><span class="toc-section-number">9.4</span> Easy HMC: <code>ulam</code></a></li>
<li><a href="9.5-care-and-feeding-of-your-markov-chain.html#care-and-feeding-of-your-markov-chain"><span class="toc-section-number">9.5</span> Care and feeding of your Markov chain</a></li>
</ul></li>
<li class="has-sub"><a href="10-big-entropy-and-the-generalized-linear-model.html#big-entropy-and-the-generalized-linear-model"><span class="toc-section-number">10</span> Big Entropy and the Generalized Linear Model</a>
<ul>
<li><a href="10.1-maximum-entropy.html#maximum-entropy"><span class="toc-section-number">10.1</span> Maximum entropy</a></li>
<li><a href="10.2-generalized-linear-models.html#generalized-linear-models"><span class="toc-section-number">10.2</span> Generalized linear models</a></li>
</ul></li>
<li class="has-sub"><a href="11-god-spiked-the-integers.html#god-spiked-the-integers"><span class="toc-section-number">11</span> God Spiked the Integers</a>
<ul>
<li><a href="11.1-binomial-regression.html#binomial-regression"><span class="toc-section-number">11.1</span> Binomial regression</a></li>
<li><a href="11.2-poisson-regression.html#poisson-regression"><span class="toc-section-number">11.2</span> Poisson regression</a></li>
<li><a href="11.3-multinomial-and-categorical-models.html#multinomial-and-categorical-models"><span class="toc-section-number">11.3</span> Multinomial and categorical models</a></li>
</ul></li>
<li class="has-sub"><a href="12-models-with-memory.html#models-with-memory"><span class="toc-section-number">12</span> Models With Memory</a>
<ul>
<li><a href="12.1-example-multilevel-tadpoles.html#example-multilevel-tadpoles"><span class="toc-section-number">12.1</span> Example: Multilevel tadpoles</a></li>
<li><a href="12.2-varying-effects-and-the-underfittingoverfitting-trade-off.html#varying-effects-and-the-underfittingoverfitting-trade-off"><span class="toc-section-number">12.2</span> Varying effects and the underfitting/overfitting trade-off</a></li>
<li><a href="12.3-more-than-one-type-of-cluster.html#more-than-one-type-of-cluster"><span class="toc-section-number">12.3</span> More than one type of cluster</a></li>
<li><a href="12.4-divergent-transitions-and-non-centered-priors.html#divergent-transitions-and-non-centered-priors"><span class="toc-section-number">12.4</span> Divergent transitions and non-centered priors</a></li>
<li><a href="12.5-multilevel-posterior-predictions.html#multilevel-posterior-predictions"><span class="toc-section-number">12.5</span> Multilevel posterior predictions</a></li>
</ul></li>
<li class="has-sub"><a href="13-adventures-in-covariance.html#adventures-in-covariance"><span class="toc-section-number">13</span> Adventures in Covariance</a>
<ul>
<li><a href="13.1-varying-slopes-by-construction.html#varying-slopes-by-construction"><span class="toc-section-number">13.1</span> Varying slopes by construction</a></li>
<li><a href="13.2-advanced-varying-slopes.html#advanced-varying-slopes"><span class="toc-section-number">13.2</span> Advanced varying slopes</a></li>
<li><a href="13.3-instruments-and-causal-designs.html#instruments-and-causal-designs"><span class="toc-section-number">13.3</span> Instruments and causal designs</a></li>
<li><a href="13.4-social-relations-as-correlated-varying-effects.html#social-relations-as-correlated-varying-effects"><span class="toc-section-number">13.4</span> Social relations as correlated varying effects</a></li>
<li><a href="13.5-continuous-categories-and-the-gaussian-process.html#continuous-categories-and-the-gaussian-process"><span class="toc-section-number">13.5</span> Continuous categories and the Gaussian process</a></li>
<li><a href="13.6-bonus-multilevel-growth-models-and-the-melsm.html#bonus-multilevel-growth-models-and-the-melsm"><span class="toc-section-number">13.6</span> Bonus: Multilevel growth models and the MELSM</a></li>
</ul></li>
<li class="has-sub"><a href="14-missing-data-and-other-opportunities.html#missing-data-and-other-opportunities"><span class="toc-section-number">14</span> Missing Data and Other Opportunities</a>
<ul>
<li><a href="14.1-measurement-error.html#measurement-error"><span class="toc-section-number">14.1</span> Measurement error</a></li>
<li><a href="14.2-missing-data.html#missing-data"><span class="toc-section-number">14.2</span> Missing data</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="predicting-predictive-accuracy" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Predicting predictive accuracy</h2>
<div class="figure">
<img src="slides/L08/21.png" alt="If we regularize correctly, we'll do better out-of-sample. We can actually predict the amount of overfitting, even when you don't have the out-of-sample. This is all small-world stuff, so be sceptical, but it gives us a principled way of talking about a model in terms of its overfitting risk. " width="60%" />
<p class="caption marginnote shownote">
If we regularize correctly, we’ll do better out-of-sample. We can actually predict the amount of overfitting, even when you don’t have the out-of-sample. This is all small-world stuff, so be sceptical, but it gives us a principled way of talking about a model in terms of its overfitting risk.
</p>
</div>
<p><strong><em>7.4.1. Cross-validation</em></strong></p>
<div class="figure">
<img src="slides/L08/22.png" alt="If you do this across lots of left-out bits from your sample, that turns out to be a really good approximation of your model. These prediction contests in industry. Motivated this on Monday talking about under- vs over-fitted model. There's a LOOCV function for `quap`. Huge literature about how many to leave out. But the general idea is to use this. " width="60%" />
<p class="caption marginnote shownote">
If you do this across lots of left-out bits from your sample, that turns out to be a really good approximation of your model. These prediction contests in industry. Motivated this on Monday talking about under- vs over-fitted model. There’s a LOOCV function for <code>quap</code>. Huge literature about how many to leave out. But the general idea is to use this.
</p>
</div>
<p>The key trouble with leave-one-out cross-validation is that, if we have 1000 observations, that means computing 1000 posterior distributions. That can be time consuming. Luckily, there are clever ways to approximate the cross-validation score without actually running the model over and over again. One approach is to use the “importance” of each observation to the posterior distribution. What “importance” means here is that some observations have a larger impact on the posterior distribution—if we remove an important observation, the posterior changes more. Other observations have less impact. It is a benign aspect of the universe that this importance can be estimated without refitting the model. <em>The key intuition is that an observation that is relatively unlikely is more important than one that is relatively expected.</em></p>
<div class="figure">
<img src="slides/L08/23.png" alt="These days you have too many data points. Really good analytical approximations, like Pareto-smoothed. Incredibly accurate. Pareto-smoothed is useful because you get a lot of diagnostic information." width="60%" />
<p class="caption marginnote shownote">
These days you have too many data points. Really good analytical approximations, like Pareto-smoothed. Incredibly accurate. Pareto-smoothed is useful because you get a lot of diagnostic information.
</p>
</div>
<p><strong><em>7.4.2. Information criteria</em></strong></p>
<div class="figure">
<img src="slides/L08/24.png" alt="Other approach, stemming from Akaike. To get an analytical approximation, a lot of assumptions are made, including that you need a Gaussian distribution. If that's true, you can get a really nice approxiatmion of the performance of the log score out-of-sample. Just the training deviance time twice th enumber of parameters. Incredible acheivement." width="60%" />
<p class="caption marginnote shownote">
Other approach, stemming from Akaike. To get an analytical approximation, a lot of assumptions are made, including that you need a Gaussian distribution. If that’s true, you can get a really nice approxiatmion of the performance of the log score out-of-sample. Just the training deviance time twice th enumber of parameters. Incredible acheivement.
</p>
</div>
<div class="figure">
<img src="slides/L08/25.png" alt="It has since been eclipsed. Another theoretical statistician has developed this new, more capable version. This thing looks complicated, but lppd is the Bayesian distance, and the penalty term on the right is the point-wise variance of the log probability of each observation. That's the generalised parameter count you want. This works for anything. Turns out in general the parameter count isn't what matters, rather the variance in the posterior distribution. And for models with flat priors and Gaussian distributions, it gives you the same value as AIC. But in general we won't use flat priors, and it often has interesting information." width="60%" />
<p class="caption marginnote shownote">
It has since been eclipsed. Another theoretical statistician has developed this new, more capable version. This thing looks complicated, but lppd is the Bayesian distance, and the penalty term on the right is the point-wise variance of the log probability of each observation. That’s the generalised parameter count you want. This works for anything. Turns out in general the parameter count isn’t what matters, rather the variance in the posterior distribution. And for models with flat priors and Gaussian distributions, it gives you the same value as AIC. But in general we won’t use flat priors, and it often has interesting information.
</p>
</div>
<p>To see how WAIC works:</p>
<p>Consider a simple regression fit with <code>quap</code>:</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="7.4-predicting-predictive-accuracy.html#cb533-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(cars)</span>
<span id="cb533-2"><a href="7.4-predicting-predictive-accuracy.html#cb533-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb533-3"><a href="7.4-predicting-predictive-accuracy.html#cb533-3" aria-hidden="true" tabindex="-1"></a>b7.m <span class="ot">&lt;-</span> </span>
<span id="cb533-4"><a href="7.4-predicting-predictive-accuracy.html#cb533-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(<span class="at">data =</span> cars, </span>
<span id="cb533-5"><a href="7.4-predicting-predictive-accuracy.html#cb533-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">family =</span> gaussian,</span>
<span id="cb533-6"><a href="7.4-predicting-predictive-accuracy.html#cb533-6" aria-hidden="true" tabindex="-1"></a>      dist <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> speed,</span>
<span id="cb533-7"><a href="7.4-predicting-predictive-accuracy.html#cb533-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="at">class =</span> Intercept),</span>
<span id="cb533-8"><a href="7.4-predicting-predictive-accuracy.html#cb533-8" aria-hidden="true" tabindex="-1"></a>                <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="at">class =</span> b),</span>
<span id="cb533-9"><a href="7.4-predicting-predictive-accuracy.html#cb533-9" aria-hidden="true" tabindex="-1"></a>                <span class="fu">prior</span>(<span class="fu">exponential</span>(<span class="dv">1</span>), <span class="at">class =</span> sigma)),</span>
<span id="cb533-10"><a href="7.4-predicting-predictive-accuracy.html#cb533-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">iter =</span> <span class="dv">2000</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb533-11"><a href="7.4-predicting-predictive-accuracy.html#cb533-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">seed =</span> <span class="dv">7</span>,</span>
<span id="cb533-12"><a href="7.4-predicting-predictive-accuracy.html#cb533-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">file =</span> <span class="st">&quot;fits/b07.0m&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="7.4-predicting-predictive-accuracy.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(b7.m)</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: dist ~ 1 + speed 
##    Data: cars (Number of observations: 50) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   -17.54      6.14   -29.45    -5.13 1.00     3878     2503
## speed         3.93      0.38     3.16     4.67 1.00     3872     2895
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    13.82      1.22    11.66    16.40 1.00     3662     2650
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Get the log-likelihood of each observation <span class="math inline">\(i\)</span> at each sample <span class="math inline">\(s\)</span> from the posterior:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="7.4-predicting-predictive-accuracy.html#cb536-1" aria-hidden="true" tabindex="-1"></a>n_cases <span class="ot">&lt;-</span> <span class="fu">nrow</span>(cars)</span>
<span id="cb536-2"><a href="7.4-predicting-predictive-accuracy.html#cb536-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb536-3"><a href="7.4-predicting-predictive-accuracy.html#cb536-3" aria-hidden="true" tabindex="-1"></a>ll <span class="ot">&lt;-</span></span>
<span id="cb536-4"><a href="7.4-predicting-predictive-accuracy.html#cb536-4" aria-hidden="true" tabindex="-1"></a>  brms<span class="sc">::</span><span class="fu">log_lik</span>(b7.m) <span class="sc">%&gt;%</span></span>
<span id="cb536-5"><a href="7.4-predicting-predictive-accuracy.html#cb536-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb536-6"><a href="7.4-predicting-predictive-accuracy.html#cb536-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="fu">c</span>(<span class="fu">str_c</span>(<span class="dv">0</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>), <span class="dv">10</span><span class="sc">:</span>n_cases))</span>
<span id="cb536-7"><a href="7.4-predicting-predictive-accuracy.html#cb536-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb536-8"><a href="7.4-predicting-predictive-accuracy.html#cb536-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(ll)</span></code></pre></div>
<pre><code>## [1] 4000   50</code></pre>
<p>You end up with a 50-by-4000 matrix of log-likelihoods, with observations in rows and samples in columns. Now to compute lppd, the Bayesian deviance, we average the samples in each row, take the log, and add all of the logs together. However, to do this with precision, we need to do all of the averaging on the log scale. This is made easy with a function <code>log_sum_exp</code>, which computes the log of a sum of exponentiated terms. Then we can just subtract the log of the number of samples. This computes the log of the average.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="7.4-predicting-predictive-accuracy.html#cb538-1" aria-hidden="true" tabindex="-1"></a>log_mu_l <span class="ot">&lt;-</span></span>
<span id="cb538-2"><a href="7.4-predicting-predictive-accuracy.html#cb538-2" aria-hidden="true" tabindex="-1"></a>  ll <span class="sc">%&gt;%</span> </span>
<span id="cb538-3"><a href="7.4-predicting-predictive-accuracy.html#cb538-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>(),</span>
<span id="cb538-4"><a href="7.4-predicting-predictive-accuracy.html#cb538-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;i&quot;</span>,</span>
<span id="cb538-5"><a href="7.4-predicting-predictive-accuracy.html#cb538-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&quot;loglikelihood&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb538-6"><a href="7.4-predicting-predictive-accuracy.html#cb538-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">likelihood =</span> <span class="fu">exp</span>(loglikelihood)) <span class="sc">%&gt;%</span> </span>
<span id="cb538-7"><a href="7.4-predicting-predictive-accuracy.html#cb538-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(i) <span class="sc">%&gt;%</span> </span>
<span id="cb538-8"><a href="7.4-predicting-predictive-accuracy.html#cb538-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">log_mean_likelihood =</span> <span class="fu">mean</span>(likelihood) <span class="sc">%&gt;%</span> <span class="fu">log</span>())</span>
<span id="cb538-9"><a href="7.4-predicting-predictive-accuracy.html#cb538-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb538-10"><a href="7.4-predicting-predictive-accuracy.html#cb538-10" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb538-11"><a href="7.4-predicting-predictive-accuracy.html#cb538-11" aria-hidden="true" tabindex="-1"></a>  lppd <span class="ot">&lt;-</span></span>
<span id="cb538-12"><a href="7.4-predicting-predictive-accuracy.html#cb538-12" aria-hidden="true" tabindex="-1"></a>  log_mu_l <span class="sc">%&gt;%</span> </span>
<span id="cb538-13"><a href="7.4-predicting-predictive-accuracy.html#cb538-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">lppd =</span> <span class="fu">sum</span>(log_mean_likelihood)) <span class="sc">%&gt;%</span> </span>
<span id="cb538-14"><a href="7.4-predicting-predictive-accuracy.html#cb538-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(lppd) </span>
<span id="cb538-15"><a href="7.4-predicting-predictive-accuracy.html#cb538-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## [1] -206.6265</code></pre>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="7.4-predicting-predictive-accuracy.html#cb540-1" aria-hidden="true" tabindex="-1"></a>v_i <span class="ot">&lt;-</span></span>
<span id="cb540-2"><a href="7.4-predicting-predictive-accuracy.html#cb540-2" aria-hidden="true" tabindex="-1"></a>  ll <span class="sc">%&gt;%</span> </span>
<span id="cb540-3"><a href="7.4-predicting-predictive-accuracy.html#cb540-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>(),</span>
<span id="cb540-4"><a href="7.4-predicting-predictive-accuracy.html#cb540-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;i&quot;</span>,</span>
<span id="cb540-5"><a href="7.4-predicting-predictive-accuracy.html#cb540-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&quot;loglikelihood&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb540-6"><a href="7.4-predicting-predictive-accuracy.html#cb540-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(i) <span class="sc">%&gt;%</span> </span>
<span id="cb540-7"><a href="7.4-predicting-predictive-accuracy.html#cb540-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">var_loglikelihood =</span> <span class="fu">var</span>(loglikelihood))</span>
<span id="cb540-8"><a href="7.4-predicting-predictive-accuracy.html#cb540-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb540-9"><a href="7.4-predicting-predictive-accuracy.html#cb540-9" aria-hidden="true" tabindex="-1"></a>pwaic <span class="ot">&lt;-</span></span>
<span id="cb540-10"><a href="7.4-predicting-predictive-accuracy.html#cb540-10" aria-hidden="true" tabindex="-1"></a>  v_i <span class="sc">%&gt;%</span></span>
<span id="cb540-11"><a href="7.4-predicting-predictive-accuracy.html#cb540-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">pwaic =</span> <span class="fu">sum</span>(var_loglikelihood)) <span class="sc">%&gt;%</span> </span>
<span id="cb540-12"><a href="7.4-predicting-predictive-accuracy.html#cb540-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>()</span>
<span id="cb540-13"><a href="7.4-predicting-predictive-accuracy.html#cb540-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb540-14"><a href="7.4-predicting-predictive-accuracy.html#cb540-14" aria-hidden="true" tabindex="-1"></a>pwaic</span></code></pre></div>
<pre><code>## [1] 4.111924</code></pre>
<p>To compute WAIC:</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="7.4-predicting-predictive-accuracy.html#cb542-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> (lppd <span class="sc">-</span> pwaic)</span></code></pre></div>
<pre><code>## [1] 421.4769</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="7.4-predicting-predictive-accuracy.html#cb544-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">waic</span>(b7.m)</span></code></pre></div>
<pre><code>## Warning: 
## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## 
## Computed from 4000 by 50 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -210.7  8.2
## p_waic         4.1  1.6
## waic         421.5 16.4
## 
## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<p>Compute the WAIC standard error:</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="7.4-predicting-predictive-accuracy.html#cb547-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">lppd   =</span> <span class="fu">pull</span>(log_mu_l, log_mean_likelihood),</span>
<span id="cb547-2"><a href="7.4-predicting-predictive-accuracy.html#cb547-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">p_waic =</span> <span class="fu">pull</span>(v_i, var_loglikelihood)) <span class="sc">%&gt;%</span> </span>
<span id="cb547-3"><a href="7.4-predicting-predictive-accuracy.html#cb547-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">waic_vec =</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> (lppd <span class="sc">-</span> p_waic)) <span class="sc">%&gt;%</span> </span>
<span id="cb547-4"><a href="7.4-predicting-predictive-accuracy.html#cb547-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">waic_se =</span> <span class="fu">sqrt</span>(n_cases <span class="sc">*</span> <span class="fu">var</span>(waic_vec)))</span></code></pre></div>
<pre><code>## # A tibble: 1 × 1
##   waic_se
##     &lt;dbl&gt;
## 1    16.4</code></pre>
<p>If you want the pointwise values:</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="7.4-predicting-predictive-accuracy.html#cb549-1" aria-hidden="true" tabindex="-1"></a>brms<span class="sc">::</span><span class="fu">waic</span>(b7.m)<span class="sc">$</span>pointwise <span class="sc">%&gt;%</span> </span>
<span id="cb549-2"><a href="7.4-predicting-predictive-accuracy.html#cb549-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>()</span></code></pre></div>
<pre><code>## Warning: 
## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>##      elpd_waic     p_waic     waic
## [1,] -3.649870 0.02182091 7.299741
## [2,] -4.023347 0.09214764 8.046694
## [3,] -3.684137 0.02150916 7.368275
## [4,] -3.995993 0.05840990 7.991986
## [5,] -3.588907 0.01056900 7.177814
## [6,] -3.741526 0.02125830 7.483051</code></pre>
<p><strong><em>7.4.3. Comparing CV, PSIS, and WAIC</em></strong></p>
<div class="figure">
<img src="slides/L08/26.png" alt="Now we'll score them on their error. All are trying to estimate the prediction error. So how close do they get? Top are flat priors. Open circles and the actual generalisation errors. Each trend line is a different metric for calculating it. WAIC is getting closer, but the differences are really small. LOOIC is a really good approximation. At the bottom we have regularising priors. Everything does better, but the differences are about the same. Unit difference on the vertical is tiny." width="60%" />
<p class="caption marginnote shownote">
Now we’ll score them on their error. All are trying to estimate the prediction error. So how close do they get? Top are flat priors. Open circles and the actual generalisation errors. Each trend line is a different metric for calculating it. WAIC is getting closer, but the differences are really small. LOOIC is a really good approximation. At the bottom we have regularising priors. Everything does better, but the differences are about the same. Unit difference on the vertical is tiny.
</p>
</div>
<div class="figure">
<img src="slides/L08/27.png" alt="Target we're trying to get is the out-of-sample error. These differences are tiny. All of these things work amazingly well." width="60%" />
<p class="caption marginnote shownote">
Target we’re trying to get is the out-of-sample error. These differences are tiny. All of these things work amazingly well.
</p>
</div>
<div class="figure">
<img src="slides/L08/28.png" alt="When samples are large, they all work identically." width="60%" />
<p class="caption marginnote shownote">
When samples are large, they all work identically.
</p>
</div>
</div>
<p style="text-align: center;">
<a href="7.3-golem-taming-regularization.html"><button class="btn btn-default">Previous</button></a>
<a href="7.5-model-comparison.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
